\documentclass[problem]{mcs}

\begin{pcomments}
  \pcomment{FP_chebyshev_pq_F17}
  \pcomment{same as FP_chebyshev_pq w/o last part}
  \pcomment{generalizes TP_Flipping_coins}
  \pcomment{ARM 5/11/16, edits 12/14/17}
  \pcomment{F17.final}
\end{pcomments}

\begin{problem}
You have a biased coin which flips Heads with probability $p$.  You
flip the coin $n$ times.  The coin flips are all mutually independent.
Let $H$ be the number of Heads.

\bparts

\ppart%\label{expected}

Write a closed-form (no summations) expression in terms of $p$ and $n$
for $\expect{H}$, the expected number of Heads.

\exambox{0.7in}{0.5in}{0in}

\begin{solution}
$\mathbf{np}$.

\begin{staffnotes}
Not needed for full credit:

Let $H_{i}$ be the indicator variable that is 1 if and only if the
$i$th coin flip comes out Heads (and 0 otherwise).  Then
\begin{equation*}
    H = H_{1} + H_{2} + \dots + H_{n}.
\end{equation*}

Hence, by linearity of expectation, 
\[
\expect{H}
    = \expect{H_{1} + H_{2} + \cdots + H_{n}}
    = \expect{H_{1}} + \cdots + \expect{H_{n}}.
\]
The expectation of an indicator variable is the probability it
equals~1\inbook{ by Lemma~\bref{expindic}}.  Hence, $\expect{H_{i}} =
p$.  We conclude that $\expect{H} = n \cdot p$.
\end{staffnotes}
\end{solution}

\ppart Write a closed-form expression in terms of $p$ and $n$ for
$\variance{H}$, the variance of the number of Heads.

\exambox{0.7in}{0.5in}{0in}

\begin{solution}
$\mathbf{np(1-p)}$.

\begin{staffnotes}
Not needed for full credit:

By the independence of the $H_{i}$, we know
\[
\Var[H] = 
\Var[H_{1} + \cdots  + H_{n}] = 
\Var[H_{1}]+ \cdots  + \Var[H_{n}].
\]
Finally, we know \inbook{by Corollary~\bref{bernoulli-variance}} the
variance of an indicator with expectation $p$ is $p(1-p)$.
\end{staffnotes}
\end{solution}

\ppart Write a closed-form expression in terms of $p$ for the upper bound
that Markov's Theorem gives for the probability that the number of
Heads is larger than the expected number by at least 1\% of the number
of flips, that is, by $n/100$.  \iffalse
\[
\Prob{H \geq\ \expect{H} + \frac{n}{100}}.
\]
\fi

\exambox{0.7in}{0.5in}{0in}

\examspace[1.0in]

\begin{solution}
\[
\frac{100p}{100p+1}.
\]

We want the Markov bound on
\[
\pr{H \geq\ \expect{H} + \frac{n}{100}}.
\]

The Markov bound on the probability of being at least $a\mu$ is $1/a$.
So we want
\[
a\mu = \mu + \frac{n}{100}.
\]
Solving for $a$ gives
\[
a = 1+ \frac{n}{100\mu} = 1 + \frac{n}{100pn} = 1+ \frac{1}{100p} =
\frac{100p+1}{100p}.
\]
\end{solution}

\ppart\label{C25bnd} Show that the upper bound given by
Chebyshev's Theorem for the probability that $H$ differs from
$\expect{H}$ by at least $n/100$ is
\[
100^2\frac{p(1-p)}{n}.
\]

\examspace[1.0in]

\begin{solution}
\begin{align*}
\Prob{\abs{H-\expect{H}} \geq \frac{n}{100}}
  & \leq \frac{\variance {H}}{(n/100)^2} & \text{(by Chebyshev's Theorem)}\\
  & = \frac{np(1-p)}{(n/100)^2}.
\end{align*}

Simplifying the right-hand expression gives the stated bound.
\end{solution}

\iffalse
\ppart The bound in part~\eqref{C25bnd} implies that if you flip at
least $n$ times for a certain number $n$, then there is a 95\% chance
that the proportion of Heads among these $n$ flips will be within 0.01
of $p$.  Use the result from part~\eqref{C25bnd} to write a simple
expression for $n$ in terms of $p$.

\exambox{0.7in}{0.5in}{0in}

\begin{solution}
\[
\mathbf{20\cdot 100^2p(1-p)}.
\]

The average number Heads is within 0.01 of $p$ iff the total number,
$H$, of heads is with $n/100$ of the expectation $pn$.  So letting $u
\eqdef$ the answer to part~\eqref{C25bnd}.  We need
\[
u \leq \frac{1}{20}.
\]
Solving for $n$, we get the answer.
\end{solution}
\fi


\eparts

\end{problem}

\endinput
