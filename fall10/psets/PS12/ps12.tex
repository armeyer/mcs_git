\documentclass[12pt,twoside]{article}   
\usepackage{../../light}

\newcommand{\hint}[1]{({\it Hint: #1})}
\newcommand{\card}[1]{\left|#1\right|}
\newcommand{\union}{\cup}
\newcommand{\lgunion}{\bigcup}
\newcommand{\intersect}{\cap}
\newcommand{\lgintersect}{\bigcap}
\newcommand{\cross}{\times}

%\hidesolutions
\showsolutions

\newlength{\strutheight}
\newcommand{\prob}[1]{\mathop{\textup{Pr}} \nolimits \left( #1 \right)}
\newcommand{\prsub}[2]{\mathop{\textup{Pr}_{#1}}\nolimits\left(#2\right)}
\newcommand{\prcond}[2]{
  \ifinner \settoheight{\strutheight}{$#1 #2$}
  \else    \settoheight{\strutheight}{$\displaystyle#1 #2$} \fi%
  \mathop{\textup{Pr}}\nolimits\left(
    #1\,\left|\protect\rule{0cm}{\strutheight}\right.\,#2
  \right)}
\newcommand{\comment}[1]{}
\newcommand{\cE}{\mathcal{E}}
\renewcommand{\setminus}{-}
\renewcommand{\complement}[1]{\overline{#1}}


\begin{document}
\problemset{12}{November 23, 2010}{Friday, December 3, 7pm}


\begin{problem}{15}

In this problem, we will (hopefully) be making tons of money!  Use your knowledge of probability and statistics to keep from going broke!

		Suppose the stock market contains $N$ types of stocks, which can be modelled by independent random variables.  Suppose furthermore that the behavior of these stocks is modelled by a 
		double-or-nothing coin flip.  That is, stock $S_i$ has half probability of doubling its value and half probability of going to $0$.  The stocks all cost a dollar, and you have
		$N$ dollars.  Say you only keep these stocks for one time-step (that is, at the end of this timestep, all stocks would have doubled in value or gone to $0$). 

\bparts
	\ppart{3} 
		What is your expected amount of money if you spend all your money on one stock?  Your variance?
		
		\solution{
			The stock doubles on a coin flip, so your expected final amount is $.5(2N) + .5(0) = N$. Your variance is calculated as
			$E[(X - \mu)^2] = $.  This, when we take into account the probability distribution of the stock, is 
			
			$$1/2(2N - N)^2 + 1/2(0 - N)^2 = N^2$$
		}
		\ppart{3}
		Suppose instead you diversified your purchases and bought $N$ shares of all different stocks. What is your expected amount of money then?  Your variance?
		
		\solution{
			The amount of money you have in stocks is $X_1 + X_2 + \ldots + X_N$, where $X_i$ is a random variable describing how much money you have in stock $i$.  The amount of
			money you expect to have is 
			$$E[\sum_{i=1}^N X_i] = \sum_{i=1}^N E[X_i]$$
			But $E[X_i] = 2 * 1/2 + 0 * 1/2 = 1$, so this sum turns out to be $N$ again.
			
			As for variance, recall that $Var(\sum X_i) = \sum Var(X_i)$ if the random variables $X_i$ are independent (which they are in this case).  The variance of a single $X_i$ is
			$$1/2 * (2 - 1)^2 + 1/2*(0 - 1)^2 = 1$$
			so the variance of your entire portfolio is $N$. 
		}
		\ppart{3}
		The money that you have invested came from your financially conservative mother.  As a result, your goals are much aligned with hers.  Given this,
		which investment strategy should you take?
		\solution{
			Your mother prefers to have less risk, and so would like the stock with less variance.  This is the strategy associated with (b). 
		}
		\ppart{3}
		Now instead say that you make money on rolls of dice.  Specifically, you play a game where you roll a standard six-sided dice, and get paid an amount (in dollars) equal to
		 the number that comes up.  What is	your expected payoff?  What is the variance?
		\solution{
		The expected payoff is $1/6(1 + 2 + 3 + 4 + 5 + 6) = 3.5$.
		The variance is 
		$$1/6(2.5^2 + 1.5^2 + .5^2 + .5^2 + 1.5^2 + 2.5^2) = 35/12$$
		}
		\ppart{3}
		We change the rules of the game so that your payoff is the cube of the number that comes up.  In that case, what is your expected payoff?  What is its variance?
		\solution{
		Your expected earnings is $1/6(1 + 8 + 27 + 64 + 125 + 216) = 441/6$.
		To calculate variance, we can simplify by noting that $Var(X) = E[X^2] - E^2[X]$.  So the variance is
		$$1/6(1 + 64 + 27^2 + 64^2 + 125^2 + 216^2) - (441//6)^2 = 67171/6 - 194481/36 = 208545/36 \approx 5792$$
		}
\eparts


\end{problem}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%spring 04 cp14w

\begin{problem}{10}
Here are seven propositions:
%
\[
\begin{array}{rcrcr}
x_1 & \vee & x_3 & \vee & \neg x_7 \\
\neg x_5 & \vee & x_6 & \vee & x_7 \\
x_2 & \vee & \neg x_4 & \vee & x_6 \\
\neg x_4 & \vee & x_5 & \vee & \neg x_7 \\
x_3 & \vee & \neg x_5 & \vee & \neg x_8 \\
x_9 & \vee & \neg x_8 & \vee & x_2 \\
\neg x_3 & \vee & x_9 & \vee & x_4
\end{array}
\]
%
Note that:

\begin{enumerate}

\item Each proposition is the OR of three terms of the form $x_i$ or
the form $\neg x_i$.

\item The variables in the three terms in each proposition are all
different.

\end{enumerate}

\noindent Suppose that we assign true/false values to the variables
$x_1, \ldots, x_9$ independently and with equal probability.

\bparts

\ppart{5} What is the expected number of true propositions?

\solution{Each proposition is true unless all three of its terms are
false.  Thus, each proposition is true with probability:
%
\[
1 - \paren{\frac{1}{2}}^3 = \frac{7}{8}
\]

Let $T_i$ be an indicator for the event that the $i$-th proposition is
true.  Then the number of true propositions is $T_1 +
\ldots + T_7$ and the expected number is:
%
\begin{align*}
\expect{T_1 + \ldots + T_7}
    & = \expect{T_1} + \ldots + \expect{T_7} \\
    & = 7/8 + \ldots + 7/8 \\
    & = 49/8 = 6 \frac{1}{8}
\end{align*}}

\ppart{5} Use your answer to prove that there exists an assignment to the
variables that makes \textit{all} of the propositions true.

\solution{A random variable can not always be less than its
expectation, so there must be some assignment such that:
%
\[
T_1 + \ldots T_7 \geq 6 \frac{1}{8}
\]
%
This implies that $T_1 + \ldots + T_7 = 7$ for at least one outcome.
This outcome is an assignment to the variables such that all of the
propositions are true.}

\eparts

\end{problem}


\begin{problem}{20}
MIT students sometimes delay laundry for a few days (to the chagrin of their roommates).  Assume all
random variables described below are mutually independent.

\bparts

\ppart{5} A \term{busy} student must complete 3 problem sets before doing
laundry.  Each problem set requires 1 day with probability $2/3$ and 2
days with probability $1/3$.  Let $B$ be the number of days a busy
student delays laundry.  What is $\expect{B}$?

Example: If the first problem set requires 1 day and the second and
third problem sets each require 2 days, then the student delays for $B
= 5$ days.

\solution{ The expected time to complete a problem
set is:
%
\[
1 \cdot \frac{2}{3} + 2 \cdot \frac{1}{3} = \frac{4}{3}
\]
%
Therefore, the expected time to complete all three problem sets is:
%
\begin{align*}
\expect{B}
    & = \expect{\text{pset1}} + \expect{\text{pset2}} + \expect{\text{pset3}} \\
    & = \frac{4}{3} + \frac{4}{3} + \frac{4}{3} \\
    & = 4
\end{align*}
}

\ppart{5} A \term{relaxed} student rolls a fair, 6-sided die in the
morning.  If he rolls a 1, then he does his laundry immediately (with
zero days of delay).  Otherwise, he delays for one day and repeats the
experiment the following morning.  Let $R$ be the number of days a
relaxed student delays laundry.  What is $\expect{R}$?

Example: If the student rolls a 2 the first morning, a 5 the second
morning, and a 1 the third morning, then he delays for $R = 2$ days.

\solution{If we regard doing laundry as a failure, then the mean time
to failure is $1 / (1/6) = 6$.  However, this counts the day laundry
is done, so the number of days delay is $6 - 1 = 5$.  Alternatively,
we could derive the answer as follows:
%
\begin{align*}
\expect{R}
    & = \sum_{k=0}^{\infty} \pr{R > k} \\
    & = \frac{5}{6} + \paren{\frac{5}{6}}^2 + \paren{\frac{5}{6}}^3 + \ldots \\
    & = \frac{5}{6} \cdot
            \paren{1 + \frac{5}{6} + \paren{\frac{5}{6}}^2 + \ldots} \\
    & = \frac{5}{6} \cdot \frac{1}{1 - 5/6} \\
    & = 5
\end{align*}
}

\ppart{5} Before doing laundry, an \term{unlucky} student must recover
from illness for a number of days equal to the product of the numbers
rolled on two fair, 6-sided dice.  Let $U$ be the expected number of
days an unlucky student delays laundry.  What is $\expect{U}$?

Example: If the rolls are 5 and 3, then the student delays for $U =
15$ days.

\solution{Let $D_1$ and $D_2$ be the two die rolls.  Recall that a die
roll has expectation $7/2$.  Thus:
%
\begin{align*}
\expect{U}
    & = \expect{D_1 \cdot D_2} \\
    & = \expect{D_1} \cdot \expect{D_2} \\
    & = \frac{7}{2} \cdot \frac{7}{2} \\
    & = \frac{49}{4}
\end{align*}
}

\ppart{5} A student is \term{busy} with probability $1/2$, \term{relaxed}
with probability $1/3$, and \term{unlucky} with probability $1/6$.
Let $D$ be the number of days the student delays laundry.  What is
$\expect{D}$?

\solution{
\[
\expect{D} = \frac{1}{2} \expect{B} + \frac{1}{3} \expect{R} + \frac{1}{6} \expect{U}
\]
}

\eparts

\end{problem}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%
%%S02 cp14m
%
%\begin{problem}{10}\label{noAi}
%
%In this problem you will check a proof of:
%\begin{theorem*}
%Let $E_1, E_2, \dots, E_n$ be a sequence of mutually independent events,
%and let $K$ be the random variable equal to the number of these events
%that occur.  The probability that none of the events occur is at most
%$e^{-\expect{K}}$.
%\end{theorem*}
%To prove the Theorem, let $K_i$ be the indicator variable for the event
%$E_i$.  Justify each line in the following derivation:
%
%\begin{proof}
%\begin{align*}
%\pr{K = 0}
%  & = \pr{\bar{\bigcup_{i=1}^n E_i}}\\
%  & = \pr{\bigcap_{i=1}^n \bar{E_i}}\\
%  & = \prod_{i=1}^n (1 - \pr{E_i})\\
%  & \leq  \prod_{i=1}^n e^{-\pr{E_i}}\\
%  & =  e^{\displaystyle -\sum_{i=1}^n \pr{E_i}}\\
%  & =  e^{\displaystyle -\sum_{i=1}^n \expect{K_i}}\\
%  & =  e^{-\expect{K}}.
%\end{align*}
%\end{proof}
%
%\solution{
%Note that
%\begin{equation}\label{Tsum}
%K = \sum_i K_i.
%\end{equation}
%Also, remember that
%\begin{equation} \label{1+xeleq}
%1 + x \leq e^x.
%\end{equation}
%
%\begin{proof}
%\begin{align*}
%\pr{K = 0}
%  & = \pr{\bar{\lgunion_{i=1}^n E_i}}
%               & \text{(def. of $K$)}\\
%  & =  \pr{\lgintersect_{i=1}^n \bar{E_i}}
%          & \text{(De Morgan's law)}\\
%  & =  \prod_{i=1}^n \pr{\bar{E_i}} & \text{(mutual independence of $E_i$'s)}\\
%  & =  \prod_{i=1}^n 1 - \pr{E_i} & \text{(complement rule)}\\
%  & \leq  \prod_{i=1}^n e^{-\pr{E_i}} & \text{(by~(\ref{1+xeleq}))}\\
%  & =  e^{-\sum_{i=1}^n \pr{E_i}} & \text{(exponent algebra)}\\
%  & =  e^{-\sum_{i=1}^n \expect{K_i}} & \text{(expectation of indicator variable)}\\
%  & =  e^{-\expect{K}}. & \text{((\ref{Tsum}) \& linearity of expectation)}
%\end{align*}
%\end{proof}
%}
%\end{problem}
%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%



%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%spring 04 cp14w
\begin{problem}{10}
We have two coins: one is a fair coin and the other is a coin that
produces heads with probability 3/4.  One of the two coins is picked, and
this coin is tossed $n$ times.  Explain how to calculate the number of
tosses to make us 95\% confident which coin was chosen.  You do not have
to calculate the minimum value of $n$, though we'd be pleased if you
did.

\solution{To guess which coin was picked, set a threshold $t$ between
$1/2$ and$3/4$.  If the proportion of heads is less than the threshold,
guess it was the fair coin; otherwise, guess the biased coin.  Let the
random variable $J$ be the number of heads in the first $n$ flips.  We
need to flip the coin enough times so that $\pr{J/n \geq t} \leq 0.05$ if
the fair coin was picked, and $\pr{J/n \leq t} \leq 0.05$ if the biased
coin was picked.  A natural threshold to choose is $5/8$, exactly in the
middle of $1/2$ and $3/4$.

For the fair coin, $J$ has an $(n,1/2)$-binomial distribution, so
we need to choose $n$ so that
\[
\pr{J > \paren{\frac{5}{8}}n} \leq 0.05
\]
which is equivalent to 
\begin{equation}\label{cdffair}
CDF_J\paren{\frac{5}{8}n} \geq 0.95  %changed \cdf to CDF
\end{equation}

For the biased coin, $J$ has an $(n,3/4)$-binomial distribution, so
we need to choose $n$ so that
\[
\pr{J \leq \paren{\frac{5}{8}}n} \leq 0.05
\]
which is equivalent to 
\begin{equation}\label{cdfunfair}
CDf_J\paren{\frac{5}{8}n} \leq 0.95 %changed \cdf to CDF
\end{equation}

We can now search for the minimum $n$ that satisfies both~\eqref{cdffair}
and~\eqref{cdfunfair}, using one of the several ways we know to calculate
or approximate the binomial cumulative distribution function.

\iffalse
For example, we know the variance of $J$ is either $n/4$ for the fair coin
or $3n/16$ for the biased coin.  Using Chebyshev's inequality for the fair
coin,
\begin{align*}
\pr{\frac{J}{n} > \frac{5}{8}} & =
  \pr{\frac{J}{n} - \frac{1}{2} > \frac{5}{8} - \frac{1}{2}}
  = \pr{J - \frac{n}{2} > \frac{n}{8} } \\
& = \pr{J - \expect{J} > \frac{n}{8} } \leq
  \pr{ \abs{J - \expect{J}} > \frac{n}{8} } \\
& \leq \frac{\variance{J}}{(n/8)^2} = \frac{n/4}{n^2/64}
  = \frac{16}{n}
\end{align*}

For the biased coin, we have
\begin{align*}
\pr{\frac{J}{n} < \frac{5}{8} } & =
  \pr{\frac{3}{4} - \frac{J}{n} > \frac{3}{4} - \frac{5}{8}}
  = \pr{\frac{3n}{4} - J > \frac{n}{8} } \\ 
  & = \pr{\expect{J} - J > \frac{n}{8} } \leq
                   \pr{\abs{J - \expect{J}} > \frac{n}{8}} \\
& \leq \frac{\variance{J}}{(n/8)^2} = \frac{3n/16}{n^2/64}
  = \frac{12}{n}
\end{align*}

We are 95\% confident if these fractions are at most $0.05$.  Namely, we
need
\[
\frac{16}{n} \leq 0.05
\]
which is satisfied if $n \geq 320$.

(Because the variance of the biased coin is less that of the fair coin, we
can do slightly better if we make our threshold a bit bigger, to about
$0.634$, which gives 95\% confidence with 279 coin flips.)
\fi
}
\end{problem}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\begin{problem}{13}
Each 6.042 final exam (out of 100 points) will be graded according to a rigorous procedure:
\begin{itemize}

\item With probability $\frac{4}{7}$ the exam is graded by a {\em TA}; with
probability $\frac{2}{7}$ it is graded by a {\em lecturer}; and with
probability $\frac{1}{7}$, it is accidentally dropped behind the radiator
and arbitrarily given a score of 84.

\item {\em TAs} score an exam by scoring each problem individually and
then taking the sum.

\begin{itemize}
\item There are ten true/false questions worth 2 points each.  For each,
full credit is given with probability 3/4, and no credit is
given with probability 1/4.

\item There are four questions worth 15 points each.  For each, the score
is determined by rolling two fair dice, summing the results, and adding 3.

\item The single 20 point question is awarded either 12 or 18 points with
equal probability.
\end{itemize}

\item {\em Lecturers} score an exam by rolling a fair die twice,
multiplying the results, and then adding a ``general impression'' score.

\begin{itemize}
\item With probability $\frac{4}{10}$, the general impression score is 40.
\item With probability $\frac{3}{10}$, the general impression score is 50.
\item With probability $\frac{3}{10}$, the general impression score is 60.
\end{itemize}
\end{itemize}
Assume all random choices during the grading process are independent.

\bparts

\ppart{5} What is the expected score on an exam graded by a TA?

\solution{
Let the random variable $T$ denote the score a TA would give.  By
linearity of expectation, the expected sum of the problemscores is the sum
of the expected problem scores.  Therefore, we have:
\begin{eqnarray*}
\expect{T}
& = & 10 \cdot \expect{\text{T/F score}} +  4 \cdot
         \expect{\text{15pt prob score}} +  \expect{\text{20pt prob score}} \\
& = & 10 \cdot \left(\frac{3}{4} \cdot 2 + \frac{1}{4} \cdot 0 \right) + 4
         \cdot \left( 2 \cdot \frac{7}{2} + 3 \right) + \left(\frac{1}{2}
         \cdot 12 + \frac{1}{2} \cdot 18 \right) \\
& = & 10 \cdot \frac{3}{2} + 4 \cdot 10 + 15 \\
& = &  70
\end{eqnarray*}}

\ppart{5} What is the expected score on an exam graded by a lecturer?

\solution{
Now we find the expected value of $L$, the score a lecturer wouldgive.
Employing linearity again, we have:
\begin{eqnarray*}
\expect{L}
& = & \expect{\text{product of dice}} +
        \expect{\text{general impression}} \\
& = & \left(\frac{7}{2}\right)^2 + \left(  \frac{4}{10} \cdot 40 +
        \frac{3}{10} \cdot 50 +   \frac{3}{10} \cdot 60 \right) \\
& = & \frac{49}{4} + 49 \\
& = & 61 \frac{1}{4}
\end{eqnarray*}}

\ppart{3} What is the expected score on a 6.042 final exam?

\solution{
Let $X$ equal the true exam score.  The total expectation law implies:

\begin{eqnarray*}
\expect{X}
& = & \frac{4}{7} \cdot \expect{T} +  \frac{2}{7} \cdot 
        \expect{L} +  \frac{1}{7} \cdot 84 \\
& = & \frac{4}{7} \cdot 70 +  \frac{2}{7} \cdot \paren{\frac{49}{4} +
        49} +  \frac{1}{7} \cdot 84\\
& = & 40 + \frac{7}{2} + 14 + 12 \\
& = & 69 \frac{1}{2}
\end{eqnarray*}}

\eparts 
\end{problem}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

%\problemdata      
%        {?} % latex-friendly label for the prob.
%        {Probability-Expectation} % The topic(s) of the problem content.
%        {?} % Source (if known)
 %       {F01, TU11-5} % Usage (list ones you are aware of).
 %       {Christos Kapoutsis, S02} % Last revision info (author, date).

%\begin{problem}{??}
%A biased coin will be flipped $n$ times.  The probability of Heads on each
%flip is $h$, and the flips are independent.  Write simple formulas for:
%
%\bparts
%\ppart{?} The probability of $k$ Heads.
%
%\solution{Let $H$ be the random variable equal to the number of Heads in
%$n$ trials.  Then $H$ has binomial distribution with parameters $n,h$, so
%\[
%\pr{H=k} = \binom{n}{k}h^k(1-h)^{n-k}
%\]
%for $0\leq k \leq n$.}
%
%\ppart{?} The probability of at least one Head.
%
%\solution{\[
%\prob{H>0}  = 1-\prob{\text{No Head}} = 1-(1-h)^n.
%\]
%}
%
%\ppart{?} The expected number of flips before the first Head.
%
%\solution{Let $T$ be a random variable representing the number of trials
%before the first Head.  Calculating $\expect{T}$ is similar to finding
%mean time to failure, except that we stop after $n$ trials.
%
%\begin{eqnarray*}
%\expect{T} &=& \sum_{i=0}^{\infty} \prob{T>i} \\
%           &=& \sum_{i=0}^{n-1} \prob{T>i} \\
%           &=& \sum_{i=0}^{n-1} (1-h)^i\\
%           &=& \frac{1-(1-h)^n}{h}.
%\end{eqnarray*}
%}
%
%\eparts
%
%\end{problem}
%
%

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%\problemdata     
%{S01-PS9-8}           
%{probability-k-events, mutual independence}              
%{} 		
%{X96, S01-PS9}
%{S01 Karger} revised 11/04 by ARM

\begin{problem}{32}

Suppose $n$ balls are thrown randomly into $n$ boxes, so each ball lands in
each box with uniform probability.  Also, suppose the outcome of each throw
is independent of all the other throws.

\bparts

\ppart{5} Let $X_i$ be an indicator random variable whose value is $1$ if box
$i$ is empty and $0$ otherwise.  Write a simple closed form expression for
the probability distribution of $X_i$.   Are $X_1, X_2, \ldots, X_n$
independent random variables?

\solution{
Box $i$ is empty iff all $n$ balls land in other boxes.  The probability
that a ball will land in another box in $(n-1)/n = 1- (1/n)$, and since
the balls are thrown independently, we have
\begin{equation}\label{pxi}
\prob{X_i=1} = \paren{1 - \frac{1}{n}}^n.
\end{equation}
The $X_i$'s are not independent.  For example, 
\[
\prob{X_1=X_2=\cdots = X_n=1}=0 < \prod_{i=1}^n \prob{X_i = 1}.
\]
}

\ppart{5} Find a constant, $c$, such that the expected number of empty boxes
is asymptotically equal ($\sim$) to $cn$.

\solution{The number of empty boxes is the sum of the $X_i$'s.  So the
expected number of empty boxes is the sum of the expectations of the
$X_i$'.  By~\eqref{pxi}, we now have
\[
\ex{\text{number of empty boxes}} = n\ex{X_1} = n\paren{1 - \frac{1}{n}}^n \sim
n\cdot\frac{1}{e}
\]
That is,
\[
c = \frac{1}{e}
\]
}

\ppart{5} Show that
\[
\prob{\text{at least $k$ balls fall in the first box}}
\leq {\binom{n}{k}} \left(\frac{1}{n}\right)^k.
\]

\solution{Let $S$ be a set of $k$ of the $n$ balls, and let $E_S$ be the
event that each of these $k$ balls falls in the first box.  Since the
probability that a ball lands in this box is $1/n$, and the throws are
independent, we have
\begin{equation}\label{ES}
\prob{E_S}=\paren{\frac{1}{n}}^k.
\end{equation}
The event that \emph{at least} $k$ balls land in the first box is the
union of all the events $E_S$.  There are $\binom{n}{k}$ subsets, $S$, of
$k$ balls, so by the Union Bound,
\[
\prob{\text{at least $k$ balls fall in the first box}}
\leq {\binom{n}{k}} \cdot \prob{E_S}.
\]
Using the value for $\prob{E_S}$ from~\eqref{ES} in the preceding
inequality yields the required bound.
}

\ppart{7} Let $R$ be the maximum of the numbers of balls that land in each of
the boxes.  Conclude from the previous parts that
\[
\pr{R \geq k} \leq \frac{n}{k!}.
\]

\solution{ Note that $R \geq k$ exactly when some box has at least $k$
balls.  Since the bound on the probability of at least $k$ balls in the
first box applies just as well to any box, we can apply the Union Bound to
having at least $k$ balls in at least one of the $n$ boxes:
\[
\prob{R \geq k} \leq n\prob{\text{at least $k$ balls fall in the first
box}}.
\]
So from the previous problem part, we have
\begin{align*}
\prob{R \geq k} & \leq n\binom{n}{k} \paren{\frac{1}{n}}^{k}\\
                & = n\paren{\frac{n(n-1)\cdots(n-k+1)}{k!\, n^k}}\\
                & = \frac{n}{k!}
                        \paren{\frac{n}{n} \cdot \frac{n-1}{n} \cdots \frac{n-k+1}{n}}
                    \\
                & \leq \frac{n}{k!}
\end{align*}
}

\ppart{10} Conclude that 
\[
\lim_{n \to \infty} \pr{R \geq n^{\epsilon}} = 0
\]
for all $\epsilon >0$.

\solution{Using Stirling's formula, and the upper bound from the previous
part, we have
\[
\pr{R \geq k} \leq
\frac{n}{k!} \sim \frac{n}{\sqrt{2\pi k}(k/e)^k} \leq
\frac{n}{(k/e)^k} = \frac{ne^k}{k^k} = 
\frac{e^{k + \ln n}}{e^{k \ln k}}.
\]
Now let $k= n^{\epsilon}$.  Then the exponent of $e$ in the numerator
above is $n^{\epsilon} + \ln n$, and the exponent of $e$ in the
denominator is $n^{\epsilon} \ln n^{\epsilon}$.  Since
\[
n^{\epsilon} + \ln n = o(n^{\epsilon} \ln n^{\epsilon}),
\]
we conclude
\[
\pr{R \geq n^{\epsilon}} \leq \frac{e^{n^{\epsilon} + \ln
n}}{e^{n^{\epsilon} \ln n^{\epsilon}}} \to 0
\]
as $n$ approaches $\infty$.
}

\iffalse

\medskip

Two special cases of the Theorem are worth singling out because they come
up all the time.
\begin{corollary*}
Suppose an event has probability $1/m$.  Then the probability that the
even will occur at least once in $m$ independent trials is approximately
$1- 1/e \approx 63\%$.  There is a 50\% chance the event will occur in $n
= \log 2 m \approx 0.69m$ trials.
\end{corollary*}


\ppart{?}
Prove the Corollary.

\solution{
From the Taylor's expansion of $e^x$, we have
\begin{equation} \label{1+xesim}
1 + x \approx e^x,
\end{equation}
for $0 \leq x \leq 1$.

In this case, $\pr{A_i}=1/m$ for $1\leq i \leq n$ and
\[
\expect{\text{\# occurrences}} = n \frac{1}{m} = \frac{n}{m}.
\]
So by the Theorem,
\[
\pr{\text{no occurrence}} \leq e^{-(n/m)},
\]
and therefore
\begin{equation}\label{1-enm}
\pr{\text{at least one occurrence}} \geq 1 - e^{-(n/m)}.
\end{equation}
In fact, it follows from by~(\ref{1+xesim}), that the $\geq$
in~(\ref{1-enm}) is an approximate equality.

So if the number, $n$ of trials is $m$, we have
\[
\pr{\text{at least one occurrence}} \approx 1- e^{-(m/m)} = 1-\frac{1}{e}.
\]

If we want
\[
1 - e^{-(n/m)} \approx \pr{\text{at least one occurrence}} \approx
\frac{1}{2},
\]
then we need
\[
e^{-(n/m)} \approx \frac{1}{2},
\]
so taking log's we conclude
\[
n \approx m\log 2.
\]
}

\fi

\eparts

\end{problem}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\end{document}
