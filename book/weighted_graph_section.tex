\begin{editingnotes}
This was in the simple graphs chapter, and then was commented out to
shorten the chapter.  Really belongs in the digraphs chapter.
\end{editingnotes}

\section{Weighted Graphs}

Sometimes we'll be interested in connections between nodes that have a
\emph{capacity} or \emph{weight}.  For example, we might be interested in
quantities such as the
\begin{editingnotes}
Include some directed edge examples: diodes, one-way streets, head-winds, \dots
\end{editingnotes}
\begin{itemize}

\item resistance of a wire between a pair of terminals, 

\item capacity of an Internet fiber between a pair of computers,

\item tension of a spring connecting a pair of devices in a dynamical system,

\item tension of a bond between a pair of atoms in a molecule,

\item distance of a highway between a pair of cities.

\end{itemize}
To model such cases, we attach a numerical \term{weight} to each edge.
\begin{definition}
  A \term{weight function} for a graph $G$ is a function $\text{wgt}:
  \edges{G} \to \reals$, where $\weight{e}$ is called the \term{weight
    of edge} $e$.  An \term{edge-weighted graph} consists of a simple
  graph along with a weight function for the graph.
\end{definition}
We'll just say \term{weighted graph} when we mean
edge-weighted.\footnote{Vertex-weighted graphs can be defined similarly,
  but we won't need these.}
For example, Figure~\ref{fig:weighted_graph} shows a weighted graph
where the weight of edge $\edge{a}{b}$ is~5.

\begin{figure}

\graphic{Fig_5D}

\caption{A 4-node weighted graph where the edge~$\edge{a}{b}$ has
  weight~5.}
\label{fig:weighted_graph}
\end{figure}

\subsection{Weight Matrices}

\iffalse
%\begin{editingnotes}
  \textcolor{red}{replaced by ARM with the next paragraph}: There are many
  ways to represent a graph.  We have already seen two ways: you can draw
  it, as in Figure~\ref{fig:weighted_graph} for example, or you can
  represent it with sets of vertices and edges.  Another common
  representation is with an adjacency matrix.
%\end{editingnotes}
\fi

For weighted graphs, adjacency matrices get generalized to weight
matrices.  For a graph with vertices $v_0,\dots,v_{n-1}$, the $ij$th
entry of the weight matrix is the weight of edge $\edge{v_i}{v_j}$.
Several formulas get simpler if, instead of numbering the vertices, we
simply index the matrix by the vertices themselves:
\begin{definition}\label{def:weighted_adjacency_matrix}
The \term{weight matrix} for a weighted graph~$G$ with $n$ vertices is
the $n \by n$ matrix $A_G$ indexed by vertices $u,v \in \vertices{G}$
where
\[
  (A_G)_{uv} \eqdef \begin{cases}
                \weight{\edge{u}{v}} & \text{if $\edge{u}{v} \in \edges{G}$}, \\
                \infty         & \text{otherwise.}
              \end{cases}
\]
\end{definition}

For example, Figure~\ref{fig:adjacency_matrix} shows the weight matrix
for the graph in Figure~\ref{fig:weighted_graph}.

\begin{figure}\redrawntrue
\[
 \begin{pmatrix}
\infty & 5 & \infty & \infty \\
5 & \infty & 6 & \infty \\
\infty & 6 & \infty & -3 \\
\infty & \infty & -3 & \infty
       \end{pmatrix}
\]

\caption{The weight matrix for the weighted graph in
  Figure~\ref{fig:weighted_graph} with nodes in order $a,b,c,d$.}
\label{fig:adjacency_matrix}
\end{figure}

\subsection{Weighted Paths}
When you drive home for vacation, you generally want to take the
shortest-time route.  It turns out that shortest paths in can be
determined in pretty much the same way that numbers of paths were
counted using powers of digraph adjacency matrices in
Section~\ref{sec:adjacency-matrix-digraph}.

\begin{definition}\label{def:5H}
  The \index{path!weight of}\term{weight of a walk} \index{weighted graph,
    path weight} in a \idx{weighted graph} is the sum of the weights of
  the successive edges in the walk.
\end{definition}

\iffalse
There is good news and bad news to report on this front.  The good
news is that it is not very hard to find a shortest path.  The bad
news is that you can't win one of those million dollar prizes for
doing it.

In fact, there are several good algorithms known for finding a
shortest path between nodes $u$ and $v$ in an $n$-node graph $G$.  The
simplest to explain (but not quite the fastest) is to compute the
successive powers of $A_G$ one by one up to the $n$th, watching for
the first power at which the $uv$th entry is nonzero.  That's because
Theorem~\ref{thm:CkDm} implies that the length of the shortest path,
if any, between $u$ and~$v$ will be the smallest value~$k$ for which
$(A_G)_{uv}^k$ is nonzero, and if there is a shortest path, its length
will be $\leq n$.
%\end{editingnotes}
\fi

\begin{definition}
  The \term{minimum-weight matrix} for length $k$ walks in an $n$-vertex
  weighted graph $G$ is the $n \times n$ matrix $W$ such that for $u,v \in \vertices{G}$,
\begin{equation}\label{def:weight_matrix}
K_{uv} \eqdef
\begin{cases} w & \text{if $w$ is the minimum weight among length $k$
                            walks from $u$ to $v$},\\
              \infty & \text{if there is no length $k$ walk from $u$ to $v$}.
\end{cases}
\end{equation}
\end{definition}

So the minimum-weight matrix for length $1$ walks in a weighted graph
$G$ is precisely its weight matrix $A_G$.  To find minimum weight
walks, we can modify the definition of matrix multiplication,
replacing products of elements by addition, and the sums by minimums.
\begin{definition}\label{def:minplus}
  The $\minplus$ product of two $n\times n$ matrices $K$ and $M$ with
  entries in $\reals\union \set{\infty}$ is the $n \times n$ matrix
  $W\minplusop M$ whose $ij$ entry is
\[
(K\minplusop M )_{ij} \eqdef \min_{k=1}^n \paren{K_{ik} + M_{kj}}.
\]
\end{definition}

\begin{theorem}\label{thm:weightmatrix-min+}
  If $K$ is the minimum weight matrix for length $k$ walks in a
  weighted graph $G$, and $M$ is the minimum weight matrix for length
  $m$ walks, then $K \minplusop M$ is the minimum weight matrix for
  length $k+m$ walks.
\end{theorem}

\begin{proof}
  The proof is virtually the same as the proof of Theorem~\ref{thm:CkDm}
  with multiplication of elements replaced by addition, and the sum of the
  multiplications by the minimum of the additions:

  Any length $k+m$ path between vertices $u$ and $v$ begins with a length
  $k$ path starting at $u$ and ending at some vertex $x$ followed by a
  length $m$ path starting at $x$ and ending at $v$.  So the minimum
  weight of a length $k+m$ path from $u$ to $v$ that goes through $x$ at
  the $k$th step equals the minimum weight $K_{ux}$ of length $k$ walks
  from $u$ to $x$, plus the minimum weight $M_{xv}$ of length $m$ walks
  from $w$ to $v$.  So we can get the minimum weight of length $k+m$ walks
  from $u$ to $v$ by taking the minimum over all possible vertices $x$ of
  the minimum weight of such walks that go through $x$ at the $k$th step.
  In other words,
\begin{equation}\label{ln-min+nuv}
\text{min weight of a length $n+m$ path from $u$ to $v$} =
              \min_{x \in \vertices{G}} W_{ux}+M_{xv}\, .
\end{equation}
But the right-hand side of~\eqref{ln-min+nuv} is precisely the
definition of $(K \minplusop M)_{uv}$.  Thus, $K \minplusop M$ is
indeed the minimum weight matrix for walks of length $k+m$.
\end{proof}

Now Theorem~\ref{thm:weightmatrix-min+} implies that the $k$th $\minplus$ power
of $A_G$, that is,
\[
(A_G)^{k, \minplus} \eqdef \underbrace{(A_G\ \minplusop\ (A_G\
      \minplusop\ (\cdots (A_G\ \minplusop\ A_G)) ))}_{k\ A_G\text{'s}},
\]
is the minimum weight matrix for the length $k$ walks.

This takes us most of the way, but we really want the minimum weight
regardless of the lengths of the walks.  To get this, we use the fact
that as long as all weights are \emph{nonnegative}, the minimum weight
walk between two vertices will be a path; this follows by the same
reasoning used for Theorem~\ref{shortestwalk_thm}.  Since $n-1$ is the
longest a \emph{path} can be in an $n$-node graph, we have an upper
bound on the length of minimum weight paths we have to look at.  We
could now find the minimum weight paths by computing all the
$\minplus$ powers of $A_G$ up to the $n-1$st.

A small modification to the weight matrix will cut down the number of
$\minplus$ matrix multiplications a lot.  Namely, for any graph $G$,
let $G_0$ be the same as $G$ except that self-loops of weight zero
appear at every vertex.  So a path of length $k$ in $G$ can be
extended to a path in $G_0$ with the same weight but with any desired
length $\geq k$---just repeatedly follow weight zero self-loops after
the $k$th step.  This means that $(A_{G_0})^{k, \minplus}$ is the
minimum-weight matrix for walks of length \emph{less than or equal} to
$k$ in $G$.  So we can choose $k = n-1$ to get a matrix with the
actual minimum weights among all walks between vertices.

\begin{theorem}\label{thm:minweightmatrix}
Let $G$ be an $n$-vertex weighted graph with nonnegative weights, and let
$D_G$ be the weight matrix of $G$ with the diagonal entries set to 0.
Then $(D_G)^{n-1, \minplus}$ is the minimum-weight walk matrix for $G$, that
is,
\[
((D_G)^{n-1, \minplus})_{uv} = \text{the minimum weight of walks in $G$ from
 $u$ to $v$}\,.
\]
\end{theorem}
Theorem~\ref{thm:weightmatrix-min+} now justifies using repeated
squaring to compute $(D_G)^{n-1, \minplus}$ with about $\log n$
$\minplus$ matrix multiplications.  Computing a single entry in a
\minplus\ matrix product takes about $n$ pairwise min's and plusses,
and since the product matrix has $n^2$ entries, about $n^3$ min's and
plusses are required to compute one product.  The upshot is that the
shortest distances between all pairs of vertices can be computed with
about $n^3\log n$ pairwise min's and plusses.\footnote{Faster methods
  using proportional to $n^2\log n$ operations are typically presented
  in introductory algorithms classes.}
