\chapter{Random Variables}\label{ran_var_chap}
%\hyperdef{random}{vars}

Thus far, we have focused on probabilities of \idx{events}.  For
example, we computed the probability that you win the Monty Hall game
or that you have a rare medical condition given that you tested
positive.  But, in many cases we would like to know more.  For
example, \emph{how many} contestants must play the Monty Hall game
until one of them finally wins?  \emph{How long} will this condition
last?  \emph{How much} will I lose gambling with strange dice all
night?  To answer such questions, we need to work with random
variables.

\section{Random Variable Examples}\label{ran_var_examples_sec}

\begin{definition}
  A \term{random variable}~$R$ on a probability space is a total function
  whose domain is the sample space.
\end{definition}
The codomain of $R$ can be anything, but will usually be a subset of
the real numbers.  Notice that the name ``random variable'' is a
misnomer; random variables are actually functions.

For example, suppose we toss three independent, unbiased coins.  Let
$C$ be the number of heads that appear.  Let $M = 1$ if the three
coins come up all heads or all tails, and let $M = 0$ otherwise.  Now
every outcome of the three coin flips uniquely determines the values
of $C$ and $M$.  For example, if we flip heads, tails, heads, then $C
= 2$ and $M = 0$.  If we flip tails, tails, tails, then $C = 0$ and $M
= 1$.  In effect, $C$ counts the number of heads, and $M$ indicates
whether all the coins match.

Since each outcome uniquely determines $C$ and $M$, we can regard them
as functions mapping outcomes to numbers.  For this experiment, the
sample space is:
\[
\sspace  =  \set{ HHH, HHT, HTH, HTT, THH, THT, TTH, TTT }.
\]
Now $C$ is a function that maps each outcome in the sample space to a 
number as follows:
\[
\begin{array}{rclcrcl}
C(HHH) & = & 3 & \quad & C(THH) & = & 2 \\
C(HHT) & = & 2 & \quad & C(THT) & = & 1 \\
C(HTH) & = & 2 & \quad & C(TTH) & = & 1 \\
C(HTT) & = & 1 & \quad & C(TTT) & = & 0.
\end{array}
\]
Similarly, $M$ is a function mapping each outcome another way:
\[
\begin{array}{rclcrcl}
M(HHH) & = & 1 & \quad & M(THH) & = & 0 \\
M(HHT) & = & 0 & \quad & M(THT) & = & 0 \\
M(HTH) & = & 0 & \quad & M(TTH) & = & 0 \\
M(HTT) & = & 0 & \quad & M(TTT) & = & 1.
\end{array}
\]
So $C$ and $M$ are \idx{random variables}.

\subsection{Indicator Random Variables}

An \term{indicator random variable} is a random variable that maps
every outcome to either 0 or~1.  Indicator random variables are also
called \term{Bernoulli variables}.  The random variable $M$ is an
example.  If all three coins match, then $M=1$; otherwise, $M = 0$.

Indicator random variables are closely related to events.  In
particular, an indicator random variable partitions the sample space
into those outcomes mapped to~1 and those outcomes mapped to~0.  For
example, the indicator $M$ partitions the sample space into two blocks
as follows:
\[
\underbrace{HHH \quad TTT}_{\text{$M = 1$}} \quad
\underbrace{HHT \quad HTH \quad HTT \quad
        THH \quad THT \quad TTH}_{\text{$M = 0$}}.
\]

In the same way, an event~$E$ partitions the sample space into those
outcomes in $E$ and those not in $E$.  So $E$ is naturally associated
with an indicator random variable, \index{$I_E$, indicator for event
  $E$} $I_E$, where $I_E(\omega) = 1$ for outcomes $\omega \in E$ and
$I_E(\omega) = 0$ for outcomes $\omega \notin E$.  This means that
event $E$ is the same as the event $[I_E = 1]$.  For example the
variable $M$ above is really just the indicator variable $I_E$, where
$E$ is the event that all three coins match.

\subsection{Random Variables and Events}

There is a strong relationship between events and more general random
variables as well.  A random variable that takes on several values
partitions the sample space into several blocks.  For example, $C$
partitions the sample space as follows:
\[
\underbrace{TTT}_{\text{$C = 0$}} \quad
\underbrace{TTH \quad THT \quad HTT}_{\text{$C = 1$}} \quad
\underbrace{THH \quad HTH \quad HHT}_{\text{$C = 2$}} \quad
\underbrace{HHH}_{\text{$C = 3$}}.
\]
Each block is a subset of the sample space and is therefore an event.
So the assertion that $C = 2$ defines the event
\[
[C = 2] = \set{THH, HTH, HHT},
\]
and this event has probability
\[
\pr{C = 2} = \pr{THH} + \pr{HTH} +\pr{HHT} = \frac{1}{8} + \frac{1}{8} +\frac{1}{8} =3/8.
\]
Likewise $[M = 1]$ is the event $\set{TTT, HHH}$ and has probability
$1/4$.

\iffalse
As another example:
\[
\pr{M = 1} =  \pr{TTT} + \pr{HHH} =  \frac{1}{8} + \frac{1}{8} =  \frac{1}{4}\,.
\]
\fi

More generally, any assertion about the values of random variables
defines an event.  For example, the assertion that $C \leq 1$ defines
\[
[C \leq 1] = \set{TTT, TTH, THT, HTT},
\]
and so $\pr{C \leq 1}= 1/2$.

Another example is the assertion that $C \cdot M$ is an odd number.
If you think about it for a minute, you'll realize that this is an
obscure way of saying that all three coins came up heads, namely,
\[
[C \cdot M \text{ is odd}] = \set{HHH}.
\]

\iffalse

Naturally enough, we can talk about the probability of events defined
by properties of random variables.  For example,
\[
\pr{C = 2}  =  \pr{THH} + \pr{HTH} + \pr{HHT}  =  \frac{1}{8} +
\frac{1}{8} + \frac{1}{8} =  \frac{3}{8}.
\]

\begin{eqnarray*}
\pr{C = 2}
        & = &   \pr{THH} + \pr{HTH} + \pr{HHT} \\
        & = &   \frac{1}{8} + \frac{1}{8} + \frac{1}{8} =  \frac{3}{8}\,.
\end{eqnarray*}

As another example:
\[
\pr{M = 1} =  \pr{TTT} + \pr{HHH} =  \frac{1}{8} + \frac{1}{8} =  \frac{1}{4}\,.
\]


\begin{eqnarray*}
\pr{M = 1}
        & = &   \pr{TTT} + \pr{HHH}\\
        & = &   \frac{1}{8} + \frac{1}{8} =  \frac{1}{4}.
\end{eqnarray*}
\fi

\begin{editingnotes}
\subsection{Conditional Probability}

Mixing conditional probabilities and events involving random variables
creates no new difficulties.  For example, $\prcond{C \geq 2}{M = 0}$
is the probability that at least two coins are heads ($C \geq 2$),
given that not all three coins are the same ($M = 0$).  We can compute
this probability using the definition of conditional probability:
\begin{eqnarray*}
\prcond{C \geq 2}{M = 0}
        & = &   \frac{\pr{[C \geq 2] \intersect [M = 0]}}{\pr{M = 0}} \\
        & = &   \frac{\pr{\set{THH, HTH, HHT}}}
                        {\pr{\set{THH, HTH, HHT, HTT, THT, TTH }}} \\
        & = &   \frac{3/8}{6/8} = \frac{1}{2}.
\end{eqnarray*}
The expression $[C \geq 2] \intersect [M = 0]$ on the first line may
look odd.  What is the set operation $\intersect$ doing between an
inequality and an equality?  We've described the event this way as a
reminder that $[C \geq 2]$ and $[M = 0]$ refer to \emph{events},
namely, \emph{sets} of outcomes.  Of course, we could just as well (or
better) have expressed this event as $[(C \geq 2) \QAND (M = 0)]$.

\end{editingnotes}

\section{Independence}

The notion of independence carries over from events to random variables as
well.  Random variables $R_1$ and $R_2$ are \index{independent random
  variables} \emph{independent} iff for all $x_1, x_2$, the two events
\[
[R_1 = x_1]  \quad \text{and}  \quad [R_2 = x_2]
\] 
are independent.

\iffalse
in the codomain of
$R_1$, and $x_2$ in the codomain of $R_2$, we have:
\[
\pr{R_1 = x_1 \QAND\ R_2 = x_2}  =  \pr{R_1 = x_1} \cdot \pr{R_2 = x_2}.
\]
As with events, we can formulate independence for random
variables in an equivalent, more intuitive way: random
variables $R_1$ and $R_2$ are independent if for all $x_1$ and $x_2$
\[
\prcond{R_1 = x_1}{R_2 = x_2}  =  \pr{R_1 = x_1}.
\]
whenever the left-hand conditional probability is defined, that is,
whenever $\pr{R_2 = x_2} > 0$.
\fi

For example, are $C$ and~$M$ independent?  Intuitively, the answer
should be ``no.''  The number of heads $C$ completely determines
whether all three coins match; that is, whether $M = 1$.  But, to
verify this intuition, we must find some $x_1, x_2 \in \reals$ such
that:
\[
\pr{C = x_1 \QAND\ M = x_2} \neq \pr{C = x_1} \cdot \pr{M = x_2}.
\]
One appropriate choice of values is $x_1 = 2$ and $x_2 = 1$.
In this case, we have:
\[
\pr{C = 2 \QAND\ M = 1} = 0 \neq \dfrac{1}{4} \cdot \dfrac{3}{8} = \pr{M
= 1} \cdot \pr{C = 2}.
\]
The first probability is zero because we never have exactly two heads ($C
= 2$) when all three coins match ($M = 1$).  The other two probabilities
were computed earlier.

On the other hand, let $H_1$ be the indicator variable for the event that the
first flip is a Head, so
\[
[H_1 = 1] = \set{HHH, HTH, HHT, HTT}.
\]
Then $H_1$ is independent of $M$, since
\begin{align*}
\pr{M=1} & = 1/4 = \prcond{M=1}{H_1=1} = \prcond{M=1}{H_1=0}\\
\pr{M=0} & = 3/4 = \prcond{M=0}{H_1=1} = \prcond{M=0}{H_1=0}
\end{align*}
This example is an instance of:
\begin{lemma}\label{lem:indicator-indep}
  Two events are independent iff their \idx{indicator variables} are
  independent.
\end{lemma}
The simple proof is left to Problem~\ref{TP_indicator_independence}.

Intuitively, the independence of two random variables means that
knowing some information about one variable doesn't provide any
information about the other one.  We can formalize what ``some
information'' about a variable $R$ is by defining it to be the value
of some quantity that depends on $R$.  This intuitive property of
independence then simply means that functions of independent variables
are also independent:

\begin{lemma}\label{lem:function-indep}
  Let $R$ and $S$ be independent random variables, and $f$ and $g$ be
  functions such that $\domain{f} = \codomain{R}$ and $\domain{g} =
  \codomain{S}$.  Then $f(R)$ and $g(S)$ are independent random
  variables.
\end{lemma}
The proof is another simple exercise left to
Problem~\ref{PS_independent_random_variables}.

As with events, the notion of independence generalizes to more than two
random variables.
\begin{definition}
Random variables $R_1, R_2, \dots, R_n$ are \term{mutually independent} iff
for all $x_1, x_2, \dots, x_n$, the $n$ events
\[
[R_1 = x_1], [R_2 = x_2], \dots, [R_n = x_n]
\] 
are mutually independent.  They are \term{$k$-way independent} iff
every subset of $k$ of them are mutually independent.
\iffalse

\begin{eqnarray*}
\lefteqn{\pr{R_1 = x_1 \QAND\ R_2 = x_2 \QAND \cdots \QAND\ R_n = x_n}}\\
        & = & \pr{R_1 = x_1} \cdot \pr{R_2 = x_2} \cdots \pr{R_n = x_n}.
\end{eqnarray*}
for all $x_1, x_2, \dots, x_n$.
\fi

\end{definition}

Lemmas~\ref{lem:indicator-indep} and~\ref{lem:function-indep} both
extend straightforwardly to $k$-way independent variables.

\iffalse
It is a simple exercise to show that the probability that any
\emph{subset} of the variables takes a particular set of values is equal
to the product of the probabilities that the individual variables take
their values.  Thus, for example, if $R_1, R_2, \dots, R_{100}$ are
mutually independent random variables, then it follows that:
\begin{align*}
\lefteqn{\pr{R_1 = 7 \QAND\ R_7 = 9.1 \QAND\ R_{23} = \pi}}\\
 & = \pr{R_1 = 7} \cdot \pr{R_7 = 9.1} \cdot \pr{R_{23} = \pi}.
\end{align*}
\fi

\begin{problems}

\practiceproblems
\pinput{TP_indicator_independence}

\homeworkproblems
\pinput{PS_equal_birthdays}
\pinput{PS_dependent_pairs}

\end{problems}


%% Probability Distributions %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Distribution Functions}\label{distributions_sec}

A random variable maps outcomes to values.  The \idx{probability
  density function,} $\pdf_R(x)$, of a random variable $R$ measures
the probability that $R$ takes the value $x$, and the closely related
cumulative distribution function $\cdf_R(x)$ measures the
probability that $R \leq x$.  Random variables that show up for
different spaces of outcomes often wind up behaving in much the same
way because they have the same probability of taking different values,
that is, because they have the same pdf/cdf.

\begin{definition}
Let $R$ be a random variable with codomain $V$.
The \term{probability density function} of $R$
is a function $\pdf_R : V \to [0,1]$ defined by:
%
\[
\pdf_R(x) \eqdef \begin{cases}
            \pr{R = x} & \text{if } x \in \range{R},\\
             0 & \text{if } x \notin \range{R}.
           \end{cases}
\]

If the codomain is a subset of the real numbers, then the
\term{cumulative distribution function} is the function $\cdf_R :
\reals \to [0, 1]$ defined by:
%
\[
\cdf_R(x) \eqdef \pr{R \leq x}.
\]
\end{definition}
%
A consequence of this definition is that
%
\[
\sum_{x \in \range{R}} \pdf_R(x) = 1.
\]
This is because $R$~has a value for each outcome, so summing the
probabilities over all outcomes is the same as summing over the
probabilities of each value in the range of~$R$.

As an example, suppose that you roll two unbiased, independent,
6-sided dice.  Let $T$~be the random variable that equals the sum of
the two rolls.  This random variable takes on values in the set $V =
\set{2, 3, \dots, 12}$.  A plot of the probability density function
for~$T$ is shown in Figure~\ref{fig:16F2}.  The lump in the middle
indicates that sums close to seven are the most likely.  The total area of
all the rectangles is 1 since the dice must take on exactly one of the
sums in $V = \set{2, 3, \dots, 12}$.

\begin{figure}

\graphic{Figure_16F2}

\caption{The probability density function for the sum of two 6-sided
  dice.}

\label{fig:16F2}

\end{figure}

The cumulative distribution function for $T$ is shown in
Figure~\ref{fig:16F3}:
%
\begin{figure}

\graphic{Figure_16F3}

\caption{The cumulative distribution function for the sum of two
  6-sided dice.}

\label{fig:16F3}

\end{figure}
%
The height of the $i$th bar in the cumulative distribution function
is equal to the \emph{sum} of the heights of the leftmost $i$ bars
in the probability density function.  This follows from the
definitions of pdf and~cdf:
\[
\cdf_R(x)  = \pr{R \leq x}
           = \sum_{y \leq x} \pr{R = y}
           = \sum_{y \leq x} \pdf_R(y).
\]

\iffalse
\begin{align*}
\cdf_R(x) & = \pr{R \leq x} \\
          & = \sum_{y \leq x} \pr{R = y} \\
          & = \sum_{y \leq x} \pdf_R(y).
\end{align*}
\fi

It also follows from the definition that
\[\lim_{x \to \infty}\cdf_R(x) = 1 \text{  and  } \lim_{x \to -\infty}
\cdf_R(x) = 0.
\]
Both $\pdf_R$ and $\cdf_R$ capture the same information about $R$, so
take your choice.  The key point here is that neither the probability
density function nor the cumulative distribution function involves the
sample space of an experiment.

\begin{editingnotes}
Thus, through these functions, we can study random variables without
reference to a particular experiment.
\end{editingnotes}

One of the really interesting things about density functions and
distribution functions is that many random variables turn out to have
the \emph{same} pdf and~cdf.  In other words, even though $R$ and~$S$
are different random variables on different probability spaces, it is
often the case that
\begin{equation*}
    \pdf_R = \pdf_S.
\end{equation*}
In fact, some pdf's are so common that they are given special names.
For example, the most important distributions in computer science
arguably are the \term{Bernoulli distribution}, the \term{Uniform
  distribution}, the \term{Binomial distribution}, and the
\term{Geometric distribution}.  We look more closely at these common
distributions in the next several sections.

\subsection{Bernoulli Distributions}\label{sec:bernoulli_dist}

A Bernoulli distribution is the distribution function for a Bernoulli
variable.  Specifically, the \term{Bernoulli distribution} has a
probability density function of the form $f_p: \set{0, 1} \to [0, 1]$
where
\begin{align*}
    f_p(0) &= p, \quad\text{and} \\
    f_p(1) &= q,
\end{align*}
for some $p \in [0, 1]$ with $q \eqdef 1-p$.  The corresponding
cumulative distribution function is $F_p: \reals \to [0, 1]$ where
\begin{equation*}
F_{p}(x) \eqdef
    \begin{cases}
        0 & \text{if $x < 0$} \\
        p & \text{if $0 \le x < 1$} \\
        1 & \text{if $1 \le x$}. \\
    \end{cases}
\end{equation*}

\subsection{Uniform Distributions}\label{sec:uniform_dist}

A random variable that takes on each possible value in its codomain
with the same probability is said to be \term{uniform}.  If the
codomain $V$ has $n$~elements, then the \term{uniform
  distribution} has a pdf of the form
\begin{equation*}
    f: V \to [0, 1]
\end{equation*}
where
\[
    f(v) = \frac{1}{n}
\]
for all $v \in V$.  

If the elements of $V$ in increasing order are $a_1, a_2, \dots,
a_n$, then the cumulative distribution function would be $F: \reals
  \to [0, 1]$ where
\[
F(x) \eqdef
    \begin{cases}
        0 & \text{if $x < a_1$} \\
        k/n & \text{if $a_k \le x < a_{k + 1}$ for $1 \le k < n$} \\
        1 & \text{if $a_n \le x$}.
    \end{cases}
\]

Uniform distributions come up all the time.  For example, the number
rolled on a fair die is uniform on the set $\set{1, 2, \dots, 6}$.  An
indicator variable is uniform when its pdf is $f_{1/2}$.

\subsection{The Numbers Game}\label{bigger_number_subsec}

Enough definitions---let's play a game!  We have two envelopes.  Each
contains an integer in the range $0, 1, \dots, 100$, and the numbers
are distinct.  To win the game, you must determine which envelope
contains the larger number.  To give you a fighting chance, we'll let
you peek at the number in one envelope selected at random.  Can you
devise a strategy that gives you a better than 50\% chance of winning?

For example, you could just pick an envelope at random and guess that
it contains the larger number.  But this strategy wins only 50\% of
the time.  Your challenge is to do better.

So you might try to be more clever.  Suppose you peek in one envelope
and see the number~12.  Since 12~is a small number, you might guess
that the number in the other envelope is larger.  But perhaps we've
been tricky and put small numbers in \textit{both} envelopes.  Then
your guess might not be so good!

An important point here is that the numbers in the envelopes may
\emph{not} be random.  We're picking the numbers and we're choosing
them in a way that we think will defeat your guessing strategy.  We'll
only use randomization to choose the numbers if that serves our
purpose: making you lose!

\subsubsection{Intuition Behind the Winning Strategy}

People are surprised when they first learn that there is a strategy
that wins more than 50\% of the time, regardless of what numbers we
put in the envelopes.

Suppose that you somehow knew a number~$x$ that was in between the
numbers in the envelopes.  Now you peek in one envelope and see a
number.  If it is bigger than~$x$, then you know you're peeking at the
higher number.  If it is smaller than $x$, then you're peeking at the
lower number.  In other words, if you know a number $x$ between the
numbers in the envelopes, then you are certain to win the game.

The only flaw with this brilliant strategy is that you do \emph{not}
know such an~$x$.  This sounds like a dead end, but there's a cool way
to salvage things: try to \emph{guess}~$x$!  There is some probability
that you guess correctly.  In this case, you win 100\% of the time.
On the other hand, if you guess incorrectly, then you're no worse off
than before; your chance of winning is still 50\%.  Combining these
two cases, your overall chance of winning is better than 50\%.

Many intuitive arguments about probability are wrong despite sounding
persuasive.  But this one goes the other way: it may not convince you,
but it's actually correct.  To justify this, we'll go over the
argument in a more rigorous way---and while we're at it, work out the
optimal way to play.


\subsubsection{Analysis of the Winning Strategy}

For generality, suppose that we can choose numbers from the integer
interval $\Zintv{0}{n}$.  Call the lower number~$L$ and the higher
number~$H$.

Your goal is to guess a number $x$ between $L$ and $H$.  It's simplest
if $x$ does not equal $L$ or $H$, so you should select $x$ at random
from among the half-integers:
%
\[
\frac{1}{2},\ \frac{3}{2},\ \frac{5}{2},\ \dots,\ \frac{2n-1}{2}
\]
%
But what probability distribution should you use?

The uniform distribution---selecting each of these half-integers with
equal probability---turns out to be your best bet.  An informal
justification is that if we figured out that you were unlikely to pick
some number---say $50\frac{1}{2}$---then we'd always put 50 and~51
in the envelopes.  Then you'd be unlikely to pick an~$x$ between $L$
and~$H$ and would have less chance of winning.

After you've selected the number~$x$, you peek into an envelope and
see some number~$T$.  If~$T > x$, then you guess that you're looking
at the larger number.  If~$T < x$, then you guess that the other
number is larger.

All that remains is to determine the probability that this strategy
succeeds.  We can do this with the usual four step method and a tree
diagram.

\paragraph{Step 1: Find the sample space.}

You either choose~$x$ too low ($< L$), too high ($> H$), or just right
($L < x < H$).  Then you either peek at the lower number ($T = L$) or
the higher number ($T = H$).  This gives a total of six possible
outcomes, as show in Figure~\ref{fig:16F4}.

\begin{figure}[h]

\graphic{numbers-game}

\caption{The tree diagram for the numbers game.}

\label{fig:16F4}

\end{figure}

\paragraph{Step 2: Define events of interest.}

The four outcomes in the event that you win are marked in the tree
diagram.

\paragraph{Step 3: Assign outcome probabilities.}

First, we assign edge probabilities.  Your guess $x$ is too low with
probability $L/n$, too high with probability $(n-H)/n$, and just right
with probability $(H-L)/n$.  Next, you peek at either the lower or
higher number with equal probability.  Multiplying along root-to-leaf
paths gives the outcome probabilities.

\paragraph{Step 4: Compute event probabilities.}

The probability of the event that you win is the sum of the
probabilities of the four outcomes in that event:
%
\begin{align*}
\pr{\text{win}}
    & = \frac{L}{2n} + \frac{H-L}{2n} + \frac{H-L}{2n}  + \frac{n-H}{2n} \\
    & = \frac{1}{2} + \frac{H-L}{2n} \\
    & \geq \frac{1}{2} + \frac{1}{2n}
\end{align*}
%
The final inequality relies on the fact that the higher number $H$ is
at least 1 greater than the lower number $L$ since they are required
to be distinct.

Sure enough, you win with this strategy more than half the time,
regardless of the numbers in the envelopes! So with numbers chosen
from the range $0, 1, \dots, 100$, you win with probability at least
$1/2 +1/200 = 50.5\%$.  If instead we agree to stick to numbers $0,
\dots, 10$, then your probability of winning rises to 55\%.  By Las
Vegas standards, those are great odds.

\subsubsection{Randomized Algorithms}

The best strategy to win the numbers game is an example of a
\term{randomized algorithm}---it uses random numbers to influence
decisions.  Protocols and algorithms that make use of random numbers
are very important in computer science.  There are many problems for
which the best known solutions are based on a random number generator.

For example, the most commonly-used protocol for deciding when to send
a broadcast on a shared bus or Ethernet is a randomized algorithm
known as \term{exponential backoff}.  One of the most commonly-used
sorting algorithms used in practice, called \term{quicksort}, uses
random numbers.  You'll see many more examples if you take an
algorithms course.  In each case, randomness is used to improve the
probability that the algorithm runs quickly or otherwise performs
well.


\subsection{Binomial Distributions}\label{binomial_distribution_section}

The third commonly-used distribution in computer science is the
\term{binomial distribution}.  The standard example of a random
variable with a binomial distribution is the number of heads that come
up in $n$~independent flips of a coin.  If the coin is fair, then the
number of heads has an \term{unbiased binomial distribution},
specified by the pdf $f_n: \Zintv{0}{n} \to [0, 1]$:
\[
f_n(k) \eqdef \binom{n}{k}2^{-n}.
\]
This is because there are $\binom{n}{k}$ sequences of $n$ coin tosses
with exactly $k$ heads, and each such sequence has probability~$2^{-n}$.

A plot of~$f_{20}(k)$ is shown in Figure~\ref{fig:16F5}.  The most
likely outcome is $k = 10$ heads, and the probability falls off
rapidly for larger and smaller values of $k$.  The falloff regions to
the left and right of the main hump are called the \term{tails of the
distribution}.

\begin{editingnotes}
We'll talk a lot more about these tails shortly.
\end{editingnotes}

\begin{figure}

\graphic{Figure_16F5}

\caption{The pdf for the unbiased binomial distribution for $n =
  20$,~$f_{20}(k)$.}

\label{fig:16F5}

\end{figure}

\iffalse
The cumulative distribution function for the unbiased binomial
distribution is $F_n: \reals \to [0, 1]$ where
\begin{equation*}
F_n(x) =
    \begin{cases}
        0 & \text{if $x < 1$} \\
        \sum_{i = 0}^k \binom{n}{i} 2^{-n}
            & \text{if $k \le x < k + 1$ for $1 \le k < n$} \\
        1 & \text{if $n \le x$}.
    \end{cases}
\end{equation*}
\fi

In many fields, including Computer Science, probability analyses come down
to getting small bounds on the \idx{tails} of the binomial distribution.
In the context of a problem, this typically means that there is very small
probability that something \emph{bad} happens, which could be a server
or communication link overloading or a randomized algorithm running for an
exceptionally long time or producing the wrong result.


The tails do get small very fast.  For example, the probability of
flipping at most 25 heads in 100 tosses is less than 1 in 3,000,000.
In fact, the tail of the distribution falls off so rapidly that the
probability of flipping exactly 25 heads is nearly twice the
probability of flipping exactly 24 heads \emph{plus} the probability
of flipping exactly 23 heads \emph{plus} \dots the probability of
flipping no heads.

\subsubsection{The General Binomial Distribution}

If the coins are biased so that each coin is heads with
probability~$p$ and tails with probability $q \eqdef 1-p$, then the
number of heads has a \term{general binomial density function}
specified by the pdf $f_{n,p} : \Zintv{0}{n} \to [0, 1]$ where
\begin{equation}\label{eq:binomfnp}
    f_{n, p}(k) = \binom{n}{k} p^k q^{n-k}.
\end{equation}
for some $n \in \nngint^+$ and $p \in [0, 1]$.  This is because
there are $\binom{n}{k}$ sequences with $k$ heads and $n - k$ tails,
but now $p^kq^{n-k}$ is the probability of each such sequence.

For example, the plot in Figure~\ref{fig:16F7} shows the probability
density function $f_{n, p}(k)$ corresponding to flipping $n=20$
independent coins that are heads with probability $p = 0.75$.  The
graph shows that we are most likely to get $k = 15$ heads, as
you might expect.  Once again, the probability falls off quickly for
larger and smaller values of $k$.
%
\begin{figure}

\graphic{Fig_16F7}

\caption{The pdf for the general binomial distribution~$f_{n, p}(k)$
  for $n = 20$ and $p = .75$.}

\label{fig:16F7}

\end{figure}

\iffalse
The cumulative distribution function for the general binomial
distribution is~$F_{n, p}: \reals \to [0, 1]$ where
\begin{equation}\label{eqn:16F5}
F_{n, p}(x) =
    \begin{cases}
        0 & \text{if $x < 1$} \\
        \sum_{i = 0}^k \binom{n}{i} p^i q^{n - i}
          & \text{if $k \le x < k + 1$ for $1 \le k < n$} \\
        1 & \text{if $n \le x$}.
    \end{cases}
\end{equation}
\fi

\begin{editingnotes}
\subsection{Approximating the Probability Density Function}

Computing the general binomial density function is daunting when $k$
and~$n$ are large.  Fortunately, there is an approximate closed-form
formula for this function based on an approximation for the binomial
coefficient.  In the formula below, $k$~is replaced by~$\alpha n$
where $\alpha$~is a number between 0 and~1.
%
\begin{lemma}\label{LN12:bincoeff-bound}
\begin{equation}\label{eqn:17BA}
\binom{n}{\alpha n}
        \sim \frac{2^{n H(\alpha)}}{\sqrt{2 \pi \alpha (1 - \alpha) n}}
\end{equation}
and
\begin{equation}\label{eqn:17A3}
\binom{n}{\alpha n} < \frac{ 2^{n H(\alpha)} }
                           { \sqrt{2 \pi \alpha (1 - \alpha) n} }
\end{equation}
where $H(\alpha)$ is the \term{entropy function}\footnote{$\log(x)$
  means $\log_2(x)$.}
\begin{equation*}
H(\alpha) \eqdef \alpha \log\paren{\frac{1}{\alpha}} +
                (1 - \alpha) \log\paren{\frac{1}{1 - \alpha}}.
\end{equation*}
Moreover, if $\alpha n > 10$ and $(1 - \alpha) n > 10$, then the left
and right sides of equation~\eqref{eqn:17BA} differ by at most~$2\%$.  If
$\alpha n > 100$ and $(1 - \alpha) n > 100$, then the difference is at
most~$0.2\%$.
\end{lemma}

The graph of~$H$ is shown in Figure~\ref{LN12:entropy}.

                                % GNUPLOT: LaTeX picture
\begin{figure}

\graphic{entropy}

\caption{The Entropy Function}
\label{LN12:entropy}

\end{figure}

Lemma~\eqref{LN12:bincoeff-bound} provides an excellent approximation
for binomial coefficients.  We'll skip its derivation, which consists
of plugging in Theorem~\ref{thm:stirling} for the factorials in the
binomial coefficient and then simplifying.

Now let's plug equation~\eqref{eqn:17BA} into the general binomial density
function.  The probability of flipping $\alpha n$~heads in $n$~tosses
of a coin that comes up heads with probability~$p$ is:
\begin{align}
f_{n, p}(\alpha n)
    &\sim \frac{ 2^{n H(\alpha)} p^{\alpha n} q^{(1 - \alpha)n} }
            { \sqrt{2 \pi \alpha (1 - \alpha) n} } \notag\\[4pt]
    &= \frac{ 2^{n \left(\alpha \log\paren{\frac{p}{\alpha}}
                    + (1 - \alpha) \log\paren{\frac{q}{1 - \alpha}}\right)} 
            }
            { \sqrt{2 \pi \alpha (1 - \alpha) n} }, \label{LN12:binbnd}
\end{align}
where the margin of error in the approximation is the same as in
Lemma~\ref{LN12:bincoeff-bound}.  From equation~\eqref{eqn:17A3}, we
also find that
\begin{equation}\label{eqn:17A4}
    f_{n, p}(\alpha n) < \frac{ 2^{n \paren{\alpha
        \log\paren{\frac{p}{\alpha}} + (1 - \alpha) \log\paren{\frac{1
          - p}{1 - \alpha}}}} }
                              { \sqrt{2 \pi \alpha (1 - \alpha) n} }.
\end{equation}

The formula in Equations~\ref{LN12:binbnd} and \ref{eqn:17A4} is as
ugly as a bowling shoe, but it's useful because it's easy to evaluate.
For example, suppose we flip a fair coin $n$ times.  What is the
probability of getting \emph{exactly} $pn$ heads?  Plugging $\alpha =
p$ into~equation~\eqref{LN12:binbnd} gives:
%
\begin{equation*}
f_{n, p}(pn)
    \sim \frac{1}{\sqrt{2 \pi p q n} }.
\end{equation*}
%
Thus, for example, if we flip a fair coin (where $p = 1/2$) \ $n =
100$ times, the probability of getting exactly 50 heads is within~2\%
of~$0.079$, which is about~8\%.

\subsection{Approximating the Cumulative Distribution Function}

In many fields, including computer science, probability analyses come
down to getting small bounds on the \idx{tails} of the binomial
distribution.  In a typical application, you want to bound the tails
in order to show that there is very small probability that too many
\emph{bad} things happen.  For example, we might like to know that it
is very unlikely that too many bits are corrupted in a message, or
that too many servers or communication links become overloaded, or
that a randomized algorithm runs for too long.

So it is usually good news that the binomial distribution has small
tails.  To get a feel for their size, consider the probability of
flipping at most 25~heads in 100~independent tosses of a fair coin.

The probability of getting at most $\alpha n$~heads is given by
the binomial cumulative distribution function
\begin{equation}\label{LN12:Jsum}
F_{n, p}(\alpha n)
    = \sum_{i = 0}^{\alpha n} \binom{n}{i} p^i q^{n - i}.
\end{equation}
We can bound this sum by bounding the ratio of successive terms.

In particular, for $i \le \alpha n$,
\begingroup
\openup3pt
\begin{align*}
\frac{ \displaystyle \binom{n}{i - 1} p^{i - 1} q^{n - (i - 1)} }
     { \displaystyle \binom{n}{i}     p^i       q^{n - i} }
    &=    \frac{\displaystyle
                  \frac{ n! p^{i - 1} q^{n - i + 1} }
                       { (i - 1)! (n - i + 1) ! }
              }
              {\displaystyle
                  \frac{ n! p^i q^{n - i} }
                       { i! (n - i)! }
              } \\
    &=    \frac{ i q }{ (n - i + 1) p } \\
    &\le  \frac{ \alpha n q }{ (n - \alpha n + 1) p } \\
    &\le  \frac{ \alpha q }{ (1 - \alpha) p }.
\end{align*}
\endgroup
This means that for $\alpha < p$,
\begingroup
\openup3pt
\begin{align}
F_{n, p}(\alpha n)
    &<  f_{n, p}(\alpha n)
        \sum_{i = 0}^\infty \left[ \frac{\alpha q}{(1 - \alpha)p}\right]^i
\notag \\
    &= \frac{\displaystyle f_{n, p}(\alpha n)}
            {\displaystyle 1 - \frac{\alpha q}{(1 - \alpha)p}}
            \notag\\
    &= \left( \frac{1 - \alpha}{1 - \alpha/p} \right) f_{n, p}(\alpha n).
\label{eqn:16F7}
\end{align}
\endgroup

In other words, the probability of at most $\alpha n$~heads is at most
\begin{equation*}
    \frac{1 - \alpha}{1 - \alpha/p}
\end{equation*}
times the probability of exactly $\alpha n$~heads. For our scenario,
where $p = 1/2$ and $\alpha = 1/4$,
\begin{equation*}
\frac{1 - \alpha}{1 - \alpha/p}
    = \frac{3/4}{1/2} % \\
    = \frac{3}{2}.
\end{equation*}
Plugging $n = 100$, \ $\alpha = 1/4$, and $p = 1/2$ into
equation~\eqref{eqn:17A4}, we find that the probability of at most
25~heads in 100~coin flips is
\begin{equation*}
F_{100, 1/2}(25)
    < \frac{3}{2} \cdot
        \frac{  \displaystyle
                2^{100 \left( \frac{1}{4} \log(2)
                     + \frac{3}{4} \log\paren{\frac{2}{3}} \right) }
             }
             { \sqrt{75 \pi /2} } % \\[3pt]
    \le 3 \cdot 10^{-7}.
\end{equation*}

This says that flipping 25 or fewer heads is extremely unlikely, which
is consistent with our earlier claim that the tails of the binomial
distribution are very small.  In fact, notice that the probability of
flipping \emph{25 or fewer} heads is only 50\% more than the
probability of flipping \emph{exactly 25} heads.  Thus, flipping
exactly 25 heads is twice as likely as flipping any number between 0
and 24!

\begin{caveat}
The upper bound on $F_{n, p}(\alpha n)$ in equation~\eqref{eqn:16F7}
holds only if $\alpha < p$.  If this is not the case in your problem,
then try thinking in complementary terms; that is, look at the number
of tails flipped instead of the number of heads.  In fact, this is
precisely what we will do in the next example.
\end{caveat}

\subsubsection{Noisy Channels}

Suppose you are sending packets of data across a communication channel
and that each packet is lost with probability~$p = .01$.  Also suppose
that packet losses are independent.  You need to figure out how much
redundancy (or error correction) to build into your communication
protocol.  Since redundancy is expensive overheard, you would like to
use as little as possible.  On the other hand, you never want to be
caught short.  Would it be safe for you to assume that in any batch
of 10,000~packets, only 200 (or~2\%) are lost?  Let's find out.

The noisy channel is analogous to flipping $n = 10{,}000$ independent
coins, each with probability~$p = .01$ of coming up heads, and asking
for the probability that there are at least $\alpha n$~heads
where~$\alpha = .02$.  Since $\alpha > p$, we cannot use
equation~\eqref{eqn:16F7}.  So we need to recast the problem by looking
at the numbers of tails.  In this case, the probability of tails is~$p
= .99$ and we are asking for the probability of at most $\alpha
n$~tails where~$\alpha = .98$.

Now we can use Equations \ref{eqn:17A4} and~\ref{eqn:16F7} to find
that the probability of losing~2\% or more of the 10,000~packets is at
most
\begin{equation*}
    \paren{\frac{1 - .98}{1 - .98/.99}}
    \frac{ 2^{10000\paren{.98 \log\paren{\frac{.99}{.98}}
                        + .02 \log\paren{\frac{.01}{.02}}} } }
         {\sqrt{2\pi (.98)(1 - .98) 10000}}
    < 2^{-60}.
\end{equation*}
This is good news.  It says that planning on at most 2\%~packet loss
in a batch of 10,000~packets should be very safe, at least for the
next few millennia.

\section{Estimation by Sampling}\label{sec:sampling}

Sampling is a very common technique for estimating the fraction of
elements in a set that have a certain property.  For example, suppose
that you would like to know how many Americans plan to vote for the
Republican candidate in the next presidential election.  It is
infeasible to ask every American how they intend to vote, so pollsters
will typically contact $n$~Americans selected at random and then
compute the fraction of \emph{those} Americans that will vote
Republican.  This value is then used as the \emph{estimate} of the
number of all Americans that will vote Republican.  For example, if
45\% of the $n$~contacted voters report that they will vote
Republican, the pollster reports that 45\% of all Americans will vote
Republican.  In addition, the pollster will usually also provide some
sort of qualifying statement such as
\begin{quote}
``There is a 95\% probability that the poll is accurate to within $\pm
  4$~percentage points.''
\end{quote}

The qualifying statement is often the source of confusion and
misinterpretation.  For example, many people interpret the qualifying
statement to mean that there is a 95\%~chance that between 41\%
and~49\% of Americans intend to vote Republican.  But this is wrong!
The fraction of Americans that intend to vote Republican is a fixed
(and unknown) value~$p$ that is \emph{not} a random variable.  Since
$p$~is not a random variable, we cannot say anything about the
probability that $.41 \le p \le .49$.

To obtain a correct interpretation of the qualifying statement and the
results of the poll, it is helpful to introduce some notation.

Define~$R_i$ to be the indicator random variable for the $i$th
contacted American in the sample.  In particular, set $R_i = 1$ if the
$i$th contacted American intends to vote Republican and $R_i = 0$
otherwise.  For the purposes of the analysis, we will assume that the
$i$th contacted American is selected uniformly at random (with
replacement) from the set of all Americans.\footnote{This means that
someone could be contacted multiple times.}  We will also assume
that every contacted person responds honestly about whether or not
they intend to vote Republican and that there are only two
options---each American intends to vote Republican or they don't.
Thus,
\begin{equation}
    \prob{R_i = 1} = p
\end{equation}
where $p$~is the (unknown) fraction of Americans that intend to vote
Republican.

We next define
\begin{equation*}
    T = R_1 + R_2 + \dots + R_n
\end{equation*}
to be the number of contacted Americans who intend to vote
Republican.  Then $T/n$~is a random variable that is the estimate of
the fraction of Americans that intend to vote Republican.

We are now ready to provide the correct interpretation of the
qualifying statement.  The poll results mean that
\begin{equation}\label{eqn:17CA}
    \Prob{\strut \abs{T/n - p} \le .04} \ge .95.
\end{equation}
In other words, there is a 95\%~chance that the sample group will
produce an estimate that is within $\pm 4$~percentage points of the
correct value for the overall population.  So either we were
``unlucky'' in selecting the people to poll or the results of the poll
will be correct to within $\pm 4$~points.

\subsection{How Many People Do We Need to Contact?}

There remains an important question: how many people~$n$ do we need to
contact to make sure that equation~\eqref{eqn:17CA} is true?  In
general, we would like $n$~to be as small as possible in order to
minimize the cost of the poll.

Surprisingly, the answer depends only on the desired \emph{accuracy}
and \emph{confidence} of the poll and \emph{not} on the number of
items in the set being sampled.  In this case, the desired accuracy
is~.04, the desired confidence is~.95, and the set being sampled is
the set of Americans.  It's a good thing that $n$~won't depend on the
size of the set being sampled---there are over 300 million~Americans!

The task of finding an~$n$ that satisfies equation~\eqref{eqn:17CA} is
made tractable by observing that $T$~has a general binomial
distribution with parameters $n$ and~$p$ and then applying Equations
\ref{eqn:17A4} and~\ref{eqn:16F7}.  Let's see how this works.

Since we will be using bounds on the tails of the binomial
distribution, we first do the standard conversion
\begin{equation*}
\Prob{\strut\abs{T/n - p} \le .04}
    \,=\, 1 - \Prob{\strut \abs{T/n - p} > .04}.
\end{equation*}
We then proceed to upper bound
\begin{align}
\Prob{\abs{T/n - p} > .04}
    & = \prob{T < (p - .04) n} + \prob{T > (p + .04) n} \notag\\
    &= F_{n, p}((p - 0.4)n) + F_{n, q}((q - .04) n).
        \label{eqn:17CB}
\end{align}
We don't know the true value of~$p$, but it turns out that the
expression on the right-hand side of equation~\eqref{eqn:17CB} is
maximized when~$p = 1/2$ and so
\begingroup
\openup\jot
\begin{align}
\Prob{\strut \abs{T/n - p} > .04}
    &\le 2 F_{n, 1/2}(.46n) \notag\\
    &<   2 \paren{\frac{1 - .46}{1 - (.46/.5)}} f_{n, 1/2}(.46n)
        \notag\\
    &< 13.5 \cdot 
        \frac{ 2^{n \paren{.46 \log\paren{\frac{.5}{.46}}
                         + .54 \log\paren{\frac{.5}{.54}} } } }
             {\sqrt{2\pi \cdot 0.46 \cdot 0.54 \cdot n}} \notag\\
    & < \frac{ 10.81 \cdot 2^{-.00462 n} }{ \sqrt{n} }.
        \label{eqn:17CD}
\end{align}
\endgroup
The second line comes from equation~\eqref{eqn:16F7} using~$\alpha =
.46$.  The third line comes from equation~\eqref{eqn:17A4}.

Equation~\eqref{eqn:17CD} provides bounds on the confidence of the poll
for different values of~$n$.  For example, if~$n = 665$, the bound in
equation~\eqref{eqn:17CD} evaluates to~$.04978\dots$.  Hence, if the
pollster contacts 665~Americans, the poll will be accurate to within
$\pm 4$~percentage points with at least 95\%~probability.

Since the bound in equation~\eqref{eqn:17CD} is exponential in~$n$, the
confidence increases greatly as $n$~increases.  For example, if $n =
6{,}650$ Americans are contacted, the poll will be accurate to within
$\pm 4$~points with probability at least~$1 - 10^{-10}$.  Of
course, most pollsters are not willing to pay the added cost of
polling 10~times as many people when they already have a confidence
level of~95\% from polling 665~people.

\end{editingnotes}

%% Probability Distributions Problems %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{problems}
\practiceproblems
\pinput{TP_3_random_variables}
%\pinput{TP_Binomial_Random_Variable} --already in probability chapter

\classproblems
\pinput{CP_bigger_number_game}
\pinput{CP_binomial_distribution_max}
\pinput{CP_max_ranvar_n}
%\pinput{CP_equal_ranvars}  subsumed by PS_equal_birthdays

\homeworkproblems
%\pinput{PS_sailor_random_walk}
\pinput{PS_drunken_sailor}
\end{problems}

%\hyperdef{great}{expectation}

\section{Great Expectations}\label{expectation_sec}

The \term{expectation} or \term{expected value} of a random variable
in simple cases is just an average value.  For example, the first
thing you typically want to know when you see your grade on an exam is
the average score of the class.  This average score is the same as the
expected score of a random student.

In general, the expected value of a random variable is the sum of all
it possible values when each value is weighted according to its
probability.  To make this work, we need to be able to add values and
multiply them by probabilities.  This will certainly be possible if
the values are real numbers; for technical reasons, we focus on
\emph{nonnegative} real values.  Now we can define expected value
formally as follows:
%\begin{definition}\label{expdef}
\begin{definition}\label{def:expectation}
If $R$~is a nonnegative real-valued random variable defined on a
sample space~$\sspace$, then the expectation of~$R$ is
\begin{equation}\label{eqn:expectation}
    \expect{R} \eqdef \sum_{\omega \in \sspace} R(\omega) \prob{\omega}.
\end{equation}
\end{definition}
The expectation of a random variable is also known as its \term{mean}.

From now on, we will assume our \emph{random variables are nonnegative
  real-valued} unless we explcitly say otherwise.

Let's work through some examples.

\subsection{The Expected Value of a Uniform Random Variable}

Rolling a 6-sided die provides an example of a uniform random
variable.  Let $R$~be the value that comes up when you roll a fair
6-sided die.  Then by~\eqref{eqn:expectation}, the expected value of
$R$ is
\[
\expect{R}
     = 1 \cdot \frac{1}{6} + 2 \cdot \frac{1}{6} + 3 \cdot \frac{1}{6} +
        4 \cdot \frac{1}{6} + 5 \cdot \frac{1}{6} + 6 \cdot \frac{1}{6}
     = \frac{7}{2}.
\]

\iffalse
\begin{align*}
\expect{R}
    & = \sum_{k=1}^6 k \cdot \frac{1}{6} \\
    & = 1 \cdot \frac{1}{6} + 2 \cdot \frac{1}{6} + 3 \cdot \frac{1}{6} +
        4 \cdot \frac{1}{6} + 5 \cdot \frac{1}{6} + 6 \cdot \frac{1}{6} \\
    & = \frac{7}{2}
\end{align*}
\fi

This calculation shows that the name ``expected'' value is a little
misleading; the random variable might \emph{never} actually take on that
value.  No one expects to roll a $3 \frac{1}{2}$ on an ordinary
die!

In general, if $R_n$~is a random variable with a uniform distribution
on~$\set{a_1,a_2, \dots, a_n}$, then the expectation of $R_n$ is
simply the average of the $a_i$'s:
\[
\expect{R_n} = \frac{a_1+a_2+\cdots+a_n}{n}\, .
\]

\subsection{The Expected Value of a Reciprocal Random Variable}

Define a random variable $S$ to be the reciprocal of the value that
comes up when you roll a fair 6-sided die.  That is, $S = 1/R$ where
$R$~is the value that you roll.  Now,
\[
\expect{S}  =\Expect{\frac{1}{R}}
            = \frac{1}{1} \cdot \frac{1}{6}
               + \frac{1}{2} \cdot \frac{1}{6}
               + \frac{1}{3} \cdot \frac{1}{6}
               + \frac{1}{4} \cdot \frac{1}{6}
               + \frac{1}{5} \cdot \frac{1}{6}
               + \frac{1}{6} \cdot \frac{1}{6}
               = \frac{49}{120}.
\]

\iffalse
\begin{align*}
\expect{S} & =\expect{\frac{1}{R}}\\
           &  = \frac{1}{1} \cdot \frac{1}{6}
+ \frac{1}{2} \cdot \frac{1}{6}
+ \frac{1}{3} \cdot \frac{1}{6}
+ \frac{1}{4} \cdot \frac{1}{6}
+ \frac{1}{5} \cdot \frac{1}{6}
+ \frac{1}{6} \cdot \frac{1}{6} % \\
= \frac{49}{120}.
\end{align*}
\fi

Notice that
\[
\Expect{1/R} \neq 1/\expect{R}.
\]
Assuming that these two quantities are equal is a common mistake.

\subsection{The Expected Value of an Indicator Random Variable}

The \idx{expected value} of an \index{indicator variable} indicator random
variable for an event is just the probability of that event.

\begin{lemma}\label{expindic}
If $I_A$ is the indicator random variable for event $A$, then
\[
\expect{I_A} = \prob{A}.
\]
\end{lemma}

\begin{proof}
\begin{align*}
\expect{I_A} =  1 \cdot \prob{I_A = 1} + 0 \cdot \prob{I_A = 0}
     & = \prob{I_A = 1} \\
     & =  \prob{A}.\qedhere & \text{(def of $I_A$)}
\end{align*}
\end{proof}
For example, if $A$~is the event that a coin with bias~$p$ comes up
heads, then $\expect{I_A} = \prob{I_A=1} = p$.

\subsection{Alternate Definition of Expectation}
There is another standard way to define expectation:
\begin{theorem}\label{thm:altexpdef}
For any  random variable~$R$,
\begin{equation}\label{eqn:altexpdef}
    \expect{R} = \sum_{x \in \range{R}} x \cdot \prob{R = x}.
\end{equation}
\end{theorem}

The proof of Theorem~\ref{thm:altexpdef}, like many of the elementary proofs
about expectation in this chapter, follows by regrouping of terms
in equation~\eqref{eqn:expectation}:
\begin{editingnotes}
Add further verbal explanation.
\end{editingnotes}
\begin{proof}
Suppose~$R$ is defined on a sample space~$\sspace$.  Then,
\begin{align*}
\expect{R}
    &\eqdef \sum_{\omega \in \sspace} R(\omega) \prob{\omega}\\
%            &\text{(Def~\ref{def:expectation} of expectation)} \\
    &= \sum_{x \in \range{R}} \, \sum_{\omega \in [R=x]} R(\omega) \prob{\omega}
                \\
    &= \sum_{x \in \range{R}} \, \sum_{\omega \in [R=x]} x \prob{\omega}
            &\text{(def of the event $[R=x]$)}\\
    &= \sum_{x \in \range{R}} x \paren{\sum_{\omega \in [R=x]} \prob{\omega}}
            & \text{(factoring $x$ from the inner sum)} \\
    &= \sum_{x \in \range{R}} x \cdot \prob{R = x}.
            & \text{(def of $\prob{R=x}$)}
\end{align*}
The first equality follows because the events~$[R=x]$ for $x \in
\range{R}$ partition the sample space~$\sspace$, so summing over the
outcomes in $[R=x]$ for $x \in \range{R}$ is the same as summing
over~$\sspace$.
\end{proof}

In general, equation~\eqref{eqn:altexpdef} is more useful than the
defining equation~\eqref{eqn:expectation} for calculating expected
values.  It also has the advantage that it does not depend on the
sample space, but only on the density function of the random variable.
On the other hand, summing over all outcomes as in
equation~\eqref{eqn:expectation} allows easier proofs of some basic
properties of expectation.

Notice that the order in which terms appear in the
sums~\eqref{eqn:altexpdef} and~\eqref{eqn:expectation} is not
specified, and the proof of Theorem~\ref{thm:altexpdef}---and lots of
proofs below---involve regrouping the terms in sums.  This is OK
because of a well-known property of countable sums of nonnegative real
numbers:
\begin{theorem}
A countable sum of nonnegative real numbers converges to the same
value, or else always diverges, regardless of the order in which the
numbers are summed.
\end{theorem}
\begin{editingnotes}
\TBA{ref calculus text} cite{Apostol68}?
\end{editingnotes}
In fact as long as reordering terms in the infinite
sum~\eqref{eqn:expectation} for expectation preserves convergence, we
can allow random variables $R$ taking negative as well as positive
values.  In this case, $\expect{R}$ will be well-defined and will have
all the basic properties we establish below for nonnegative variables.
But reordering does not preserve convergence for arbitrary sums of
positive and negative values (see
Problems~\bref{CP_conditional_convergence}
and~\bref{PS_alt_harmonic_converges}), and there is no useful
definition of the expectation for \emph{arbitrary} real-valued random
variables.

\begin{editingnotes}
\subsubsection{Medians}
The mean of a random variable is not the same as the \term{median}.
The median is the \emph{midpoint} of a distribution.
 
\begin{definition}%\label{def:17A2}
The \term{median} of a random variable~$R$ is the value~$x \in
\range{R}$ such that
\begin{align*}
    \prob{R \le x} & \le \frac{1}{2} \qquad \text{and} \\
    \prob{R > x}   & <    \frac{1}{2}.
\end{align*}
\end{definition}

We won't devote much attention to the median.  The expected value is
more useful and has much more interesting properties.
\end{editingnotes}

\iffalse
\footnote{Some texts define the median to be the
  value of $x \in \range{R}$ for which $\prob{R \le x} < 1/2$ and
  $\prob{R > x} \le 1/2$.  The difference in definitions is
  not important.}
\fi

\subsection{Conditional Expectation}

Just like event probabilities, expectations can be conditioned on some
event.  Given a random variable~$R$, the expected value of~$R$
conditioned on an event~$A$ is the probability-weighted average value
of~$R$ over outcomes in~$A$.  More formally:
\begin{definition}\label{condexpdef} %\begin{theorem}\label{alt:condexpdef} 
The \term{conditional expectation}~$\expcond{R}{A}$ of a random
variable $R$ given event~$A$ is:
\begin{equation}\label{condexpsumv}
\expcond{R}{A} \eqdef \sum_{r \in \range{R}} r \cdot\prcond{R=r}{A}.
\end{equation}
\end{definition}
\iffalse
In other words, it is the average value of the variable $R$ when values
are weighted by their conditional probabilities given $A$.
\fi

For example, we can compute the expected value of a roll of a fair
die, given that the number rolled is at least 4.  We do this by
letting $R$ be the outcome of a roll of the die.  Then by
equation~\eqref{condexpsumv},
\[
\expcond{R}{R \geq 4} = \sum_{i=1}^6 i \cdot \prcond{R=i}{R \ge 4}
%= \sum_{i=4}^6 i \cdot 1/3
= 1\cdot 0 + 2\cdot 0 + 3\cdot 0 +
  4\cdot\tfrac{1}{3} + 5\cdot\tfrac{1}{3} + 6\cdot\tfrac{1}{3}
= 5.
\]

Conditional expectation is useful in dividing complicated expectation
calculations into simpler cases.  We can find a desired expectation
by calculating the conditional expectation in each simple case and
averaging them, weighing each case by its probability.

For example, suppose that 49.6\% of the people in the world are male and
the rest female---which is more or less true.  Also suppose the expected
height of a randomly chosen male is $5'\,11''$, while the expected height
of a randomly chosen female is $5'\,5.''$  What is the expected height of a
randomly chosen person?  We can calculate this by averaging the
heights of men and women.  Namely, let $H$ be the height (in feet) of a
randomly chosen person, and let $M$ be the event that the person is male
and $F$ the event that the person is female.  Then
\begin{align*}
\expect{H} &= \expcond{H}{M} \pr{M} + \expcond{H}{F} \pr{F}\\
&= (5 + 11/12) \cdot 0.496  + (5 + 5/12) \cdot (1 - 0.496)\\
&= 5.6646\dots.
\end{align*}
which is a little less than~5'\,8."

This method is justified by:

\begin{theorem}[Law of Total Expectation]\label{total_expect} %\label{thm:condexp}
Let $R$~be a random variable on a sample space~$\sspace$, and suppose
that $A_1$, $A_2$, \dots, is a partition of~$\sspace$.  Then
\[
    \expect{R} = \sum_i \expcond{R}{A_i} \prob{A_i}.
\]
\end{theorem}

\begin{proof}
  \begin{align*}
    \expect{R} & = \sum_{r \in \range{R}} r \cdot \pr{R=r}
                   & \text{(by~\ref{eqn:altexpdef})}\\
    &= \sum_r r \cdot \sum_i \prcond{R=r}{A_i} \pr{A_i}
            & \text{(Law of Total Probability)}\\
    &= \sum_r \sum_i r \cdot \prcond{R=r}{A_i} \pr{A_i}
              & \text{(distribute constant $r$)}\\
    &= \sum_i \sum_r r \cdot \prcond{R=r}{A_i} \pr{A_i}
              & \text{(exchange order of summation)}\\
    &= \sum_i \pr{A_i} \sum_r r \cdot \prcond{R=r}{A_i}
             & \text{(factor constant $\pr{A_i}$)}\\
    &= \sum_i \pr{A_i} \expcond{R}{A_i}.
             & \text{(Def~\ref{condexpdef} of cond.\ expectation)}
  \end{align*}
\end{proof}

\subsection{Geometric Distributions}\label{mean_time_to_failure_subsec}

A computer program crashes at the end of each hour of use with
probability $p$, if it has not crashed already.  What is the expected
time until the program crashes?  This will be easy to figure out using
the \index{total expectation} Law of Total Expectation,
Theorem~\ref{total_expect}.  Specifically, we want to find
$\expect{C}$ where $C$ is the number of hours until the first crash.
We'll do this by conditioning on whether or not the crash occurs in
the first hour.

So define~$A$ to be the event that the system fails on the first step and
$\setcomp{A}$ to be the complementary event that the system does not fail
on the first step.  Then the mean time to failure~$\expect{C}$ is
\begin{equation}\label{eqn:17Y3}
    \expect{C} = \expcond{C}{A} \prob{A} + \expcond{C}{\setcomp{A}} \prob{\setcomp{A}}.
\end{equation}

Since $A$~is the condition that the system crashes on the first
step, we know that
\begin{equation}\label{eqn:17Y1}
    \expcond{C}{A} = 1.
\end{equation}
Since $\setcomp{A}$~is the condition that the system does \emph{not} crash on
the first step, conditioning on~$\setcomp{A}$ is equivalent to taking a first
step without failure and then starting over without conditioning.
Hence,
\begin{equation}\label{eqn:17Y2}
    \expcond{C}{\setcomp{A}} = 1 + \expect{C}.
\end{equation}

Plugging~\eqref{eqn:17Y1} and~\eqref{eqn:17Y2} into~\eqref{eqn:17Y3}:
\begin{align*}
\expect{C}
    &= 1 \cdot p + (1 + \expect{C}) q \\
    &= p + 1 - p + q \expect{C} \\
    &= 1 + q \expect{C}.
\end{align*}
Then, rearranging terms gives
\begin{align*}
    1   = \expect{C} - q \expect{C} % \\
        = p \expect{C},
\end{align*}
and thus 
\[
\expect{C} = 1/p.
\]

\begin{editingnotes}
We will use this sort of analysis extensively in
Chapter~\ref{ran_process_chap} when we examine the expected behavior
of random walks.
\end{editingnotes}

The general principle here is well-worth remembering.

\textbox{
\begin{center}
\textboxtitle{Mean Time to Failure}
\end{center}

If a system independently fails at each time step with probability
$p$, then the expected number of steps up to the first failure is
$1/p$.}

So, for example, if there is a 1\% chance that the program crashes at
the end of each hour, then the expected time until the program crashes
is $1 / 0.01 = 100$ hours.  

As a further example, suppose a couple insists on having children
until they get a boy, then how many baby girls should they expect
before their first boy?  Assume for simplicity that there is a 50\%
chance that a child will be a boy and that the genders of siblings
are mutually independent.

This is really a variant of the previous problem.  The question, ``How
many hours until the program crashes?'' is mathematically the same as
the question, ``How many children must the couple have until they get
a boy?''  In this case, a crash corresponds to having a boy, so we
should set $p = 1/2$.  By the preceding analysis, the couple should
expect a baby boy after having $1/p = 2$ children.  Since the last of
these will be a boy, they should expect just one girl.  So even in
societies where couples pursue this commitment to boys, the expected
population will divide evenly between boys and girls.

\begin{editingnotes}

\textcolor{red}{Is already---or should be placed---in exercises.}

\subsubsection*{Alternative Derivations for Mean Time to Failure}
Using the Law of Total Expectation to find expectations is a worthwhile
approach to keep in mind, but it's good review to derive the same
formula directly from the definition of expectation.  Namely, the
probability that the first crash occurs in the $i$th hour for some $i
>0$ is the probability $q^{i-1}$ that it does not crash in each
of the first $i-1$ hours, times the probability $p$ that it does
crash in the $i$th hour.  So
\begin{align}
\expect{C} & = \sum_{i \in \nngint} i \cdot \pr{C=i}
               & \text{(by~\eqref{eqn:altexpdef})}\notag\\
           & = \sum_{i \in \nngint} i q^{i-1}p\notag\\
           &= \frac{p}{q} \cdot \sum_{i \in \nngint} i q^i.\label{meantimefailsum}
\end{align}
But we've already seen a sum like this last one (you did remember
this, right?), namely, equation~\eqref{eqn:inf_ixi}:
\[
%\begin{equation}\label{Notes11form}
\sum_{i \in \nngint} ix^i =\frac{x}{(1-x)^2}.
%\end{equation}
\]
Combining~\eqref{eqn:inf_ixi} with~\eqref{meantimefailsum} gives
\[
\expect{C} = \frac{p}{q} \cdot \frac{q}{(1-q)^2} = \frac{1}{p}
\]
as expected.

A useful formula for calculating expectations of nonnegative integer
valued variables is given in the next lemma.  It yields still another
simple way to derive mean time to failure.

\begin{lemma}
If $R$ is a nonnegative integer-valued random variable, then:
%
\begin{equation}\label{R>i}
\expect{R} = \sum_{i \in \nngint} \pr{R > i}
\end{equation}
\end{lemma}

\begin{proof}
Look at the layout of the following sum:
%
\[
\begin{array}{ccccccc}
\pr{R = 1} & + & \pr{R = 2} & + & \pr{R = 3} & + & \cdots \\
           & + & \pr{R = 2} & + & \pr{R = 3} & + & \cdots \\
           &   &            & + & \pr{R = 3} & + & \cdots \\
           &   &            &   &            & + & \cdots
\end{array}
\]
%
The successive columns sum to $1 \cdot \pr{R = 1}$, $2 \cdot \pr{R = 2}$,
$3 \cdot \pr{R = 3}$, \dots.  Thus, the whole sum is equal to:
%
\[
\sum_{i \in \nngint} i \cdot \pr{R = i}
\]
which equals $\expect{R}$ by~\eqref{eqn:altexpdef}.  On the other hand, the
successive rows sum to $\pr{R > 0}$, $\pr{R > 1}$, $\pr{R > 2}$, \dots.
Thus, the whole sum is also equal to:
%
\[
\sum_{i \in \nngint} \pr{R > i},
\]
%
which therefore must equal $\expect{R}$ as well.
\end{proof}

Now $\pr{C > i}$ is easy to evaluate: a crash happens later than the $i$th
hour iff the system did not crash during the first $i$ hours, which
happens with probability $q^i$.  Plugging this into~\eqref{R>i} gives:
%
\begin{align*}
\expect{C} & = \sum_{i \in \nngint} q^i \\
       & = \frac{1}{1 - q} & \text{(sum of geometric series)}\\
       & = \frac{1}{p}
\end{align*}
\end{editingnotes}

There is a simple intuitive argument that explains the mean time to
failure formula~\eqref{exp_time_to_fail}.  Suppose the system is
restarted after each failure.  This makes the mean time to failure the
same as the mean time between successive repeated failures.  Now if
the probability of failure at a given step is $p$, then after $n$
steps we expect to have $pn$ failures.  Now the average number of
steps between failures is, by definition, equal to $n/pn = 1/p$.

For the record, we'll state a formal version of this result.  A random
variable like $C$ that counts steps to first failure is said to have a
\emph{geometric distribution} with parameter $p$.
\begin{definition}\label{def:geometric_distribution}
A random variable $C$ has a \term{geometric distribution} with
parameter $p$ iff $\codomain{C} = \integers^+$ and
\[
\pr{C=i} = q^{i-1}p.
\]
\end{definition}

\begin{lemma}\label{lem:exp_time_to_fail}
If a random variable $C$ has a \idx{geometric distribution} with
parameter $p$, then
\begin{equation}\label{exp_time_to_fail}
    \expect{C} = \frac{1}{p}.
\end{equation}
\end{lemma}

\subsection{Expected Returns in Gambling Games}

Some of the most interesting examples of expectation can be explained
in terms of gambling games.  For straightforward games where you
win~$w$ dollars with probability~$p$ and you lose~$x$ dollars with
probability~$q = 1 - p$, it is easy to compute your \term{expected return}
or \term{winnings}.  It is simply
\[
    p w - q x \text{ dollars}.
\]
For example, if you are flipping a fair coin and you win~\$1 for heads
and you lose~\$1 for tails, then your expected winnings are
\[
    \frac{1}{2} \cdot 1 - \paren{1 - \frac{1}{2}} \cdot 1 = 0.
\]
In such cases, the game is said to be \term{fair} since your expected
return is zero.

\subsubsection{Splitting the Pot}

We'll now look at a different game which is fair---but only on first
analysis.

It's late on a Friday night in your neighborhood hangout when two new
biker dudes, Eric and Nick, stroll over and propose a simple wager.
Each player will put~\$2 on the bar and secretly write ``heads'' or
``tails'' on their napkin.  Then you will flip a fair coin.
The \$6 on the bar will then be ``split''---that is, be divided
equally---among the players who correctly predicted the outcome of
the coin toss.  Pot splitting like this is a familiar feature in poker
games, betting pools, and lotteries.

This sounds like a fair game, but after your regrettable encounter
with strange dice (Section~\ref{sec:strange-dice}), you are definitely
skeptical about gambling with bikers.  So before agreeing to play, you
go through the four-step method and write out the tree diagram to
compute your expected return.  The tree diagram is shown in
Figure~\ref{fig:17E1}.

\begin{figure}

\graphic{Fig_17E1}

\caption{The tree diagram for the game where three players each
  wager~\$2 and then guess the outcome of a fair coin toss.  The
  winners split the pot.}

\label{fig:17E1}

\end{figure}

The ``payoff'' values in Figure~\ref{fig:17E1} are computed by
dividing the \$6~pot\footnote{The money invested in a wager is
  commonly referred to as the \emph{pot}.} among those players who
guessed correctly and then subtracting the~\$2 that you put into the
pot at the beginning.  For example, if all three players guessed
correctly, then your payoff is~\$0, since you just get back your
\$2~wager.  If you and Nick guess correctly and Eric guessed wrong,
then your payoff is
\begin{equation*}
    \frac{6}{2} - 2 = 1.
\end{equation*}
In the case that everyone is wrong, you all agree to split the pot
and so, again, your payoff is zero.

To compute your expected return, you use
equation~\eqref{eqn:altexpdef}:
\begin{align*}
\expect{\text{payoff}}
    &= 0 \cdot \frac{1}{8} + 1 \cdot \frac{1}{8} + 1 \cdot \frac{1}{8}
        + 4 \cdot \frac{1}{8} \\
        & \quad {}+ (-2) \cdot \frac{1}{8} + (-2) \cdot \frac{1}{8}
        + (-2) \cdot \frac{1}{8}
        + 0 \cdot \frac{1}{8} \\
    &= 0.
\end{align*}
This confirms that the game is fair.  So, for old time's sake, you
break your solemn vow to never ever engage in strange gambling games.

\subsubsection{The Impact of Collusion}

Needless to say, things are not turning out well for you.  The more
times you play the game, the more money you seem to be losing.  After
1000~wagers, you have lost over~\$500.  As Nick and Eric are consoling
you on your ``bad luck,'' you do a back-of-the-envelope calculation
and decide that the probability of losing \$500 in 1000 fair
\$2~wagers is very, very small.

\begin{editingnotes}
Add pset problem on this.
\end{editingnotes}

\iffalse
using the bounds on the tails of the binomial distribution from
Section~\ref{binomial_distribution_section} NOT BINOMIAL

less than the probability of being being dealt four Aces in poker at
the same moment you are struck by lightning.  Vietnamese Monk waltzing
in and handing you one of those golden disks.  How can this be?
\fi

Now it is possible of course that you are very, very unlucky.  But it
is more likely that something fishy is going on.  Somehow the tree
diagram in Figure~\ref{fig:17E1} is not a good model of the game.

The ``something'' that's fishy is the opportunity that Nick and Eric
have to \index{collusion}{collude} against you.  The fact that the
coin flip is fair certainly means that each of Nick and Eric can only
guess the outcome of the coin toss with probability~$1/2$.  But when
you look back at the previous 1000 bets, you notice that Eric and Nick
never made the same guess.  In other words, Nick always guessed
``tails'' when Eric guessed ``heads,'' and vice-versa.  Modelling this
fact now results in a slightly different tree diagram, as shown in
Figure~\ref{fig:17E2}.

\begin{figure}

\graphic{Fig_17E2}

\caption{The revised tree diagram reflecting the scenario where Nick
  always guesses the opposite of Eric.}

\label{fig:17E2}

\end{figure}

The payoffs for each outcome are the same in Figures \ref{fig:17E1}
and~\ref{fig:17E2}, but the probabilities of the outcomes are
different.  For example, it is no longer possible for all three
players to guess correctly, since Nick and Eric are always guessing
differently.  More importantly, the outcome where your payoff is~\$4
is also no longer possible.  Since Nick and Eric are always guessing
differently, one of them will always get a share of the pot.  As you
might imagine, this is not good for you!

When we use equation~\eqref{eqn:altexpdef} to compute your expected
return in the collusion scenario, we find that
\begin{align*}
\expect{\text{payoff}}
    &= 0 \cdot 0 + 1 \cdot \frac{1}{4} + 1 \cdot \frac{1}{4}
        + 4 \cdot 0 \\
        & \quad {}+ (-2) \cdot 0 + (-2) \cdot \frac{1}{4}
        + (-2) \cdot \frac{1}{4}
        + 0 \cdot 0 \\
    &= -\frac{1}{2}.
\end{align*}
So watch out for these biker dudes!  By colluding, Nick and Eric have
made it so that you expect to lose~\$.50 every time you play.  No
wonder you lost~\$500 over the course of 1000~wagers.

\subsubsection{How to Win the Lottery}

Similar opportunities to collude arise in many betting games.
For example, consider the typical weekly football betting pool, where
each participant wagers~\$10 and the participants that pick the most
games correctly split a large pot.  The pool seems fair if you think
of it as in Figure~\ref{fig:17E1}.  But, in fact, if two or more
players collude by guessing differently, they can get an
``unfair'' advantage at your expense!

In some cases, the \idx{collusion} is inadvertent and you can profit
from it.  For example, many years ago, a former MIT Professor of
Mathematics named Herman Chernoff figured out a way to make money by
playing the state lottery.  This was surprising since the state
usually takes a large share of the wagers before paying
the winners, and so the expected return from a lottery ticket is
typically pretty poor.  So how did Chernoff find a way to make money?
It turned out to be easy!

In a typical state lottery,
\begin{itemize}

\item all players pay~\$1 to play and select 4~numbers from 1 to~36,

\item the state draws 4~numbers from 1 to~36 uniformly at random,

\item the states divides 1/2 of the money collected among the people
  who guessed correctly and spends the other half redecorating the
  governor's residence.

\end{itemize}
This is a lot like the game you played with Nick and Eric, except that
there are more players and more choices.  Chernoff discovered that a
small set of numbers was selected by a large fraction of the
population.  Apparently many people think the same way; they pick the
same numbers not on purpose as in the previous game with Nick and
Eric, but based on the Red Sox winning average or today's date.  The
result is as though the players were intentionally colluding to lose.
If any one of them guessed correctly, then they'd have to split the
pot with many other players.  By selecting numbers uniformly at
random, Chernoff was unlikely to get one of these favored sequences.
So if he won, he'd likely get the whole pot!  By analyzing actual
state lottery data, he determined that he could win an average of
7~cents on the dollar.  In other words, his expected return was
not~$-\${.}50$ as you might think, but~$+\${.}07$.\footnote{Most
  lotteries now offer randomized tickets to help smooth out the
  distribution of selected sequences.}
Inadvertent collusion often arises in betting pools and is a
phenomenon that you can take advantage of.

\begin{editingnotes}
For example, suppose you
enter a Super Bowl betting pool where the goal is to get closest to
the total number of points scored in the game.  Also suppose that the
average Super Bowl has a total of 30~point scored and that everyone
knows this.  Then most people will guess around 30~points.  Where
should you guess?  Well, you should guess just outside of this range
because you get to cover a lot more ground and you don't share the pot
if you win.  Of course, if you are in a pool with math students and
they all know this strategy, then maybe you should guess 30~points
after all.
\end{editingnotes}

\begin{problems}
\practiceproblems
\pinput{TP_Binomial_Board_Breaking}
\pinput{TP_average_up_down}

\classproblems
\pinput{CP_carnival_dice_fair}
\pinput{CP_consecutive_coin_flips}
\pinput{CP_undefined_expectation}
\pinput{CP_sqrt_infinite_expectation}
%\pinput{CP_absolute_convergence}

\examproblems
\pinput{MQ_tournament_probabilistic_proof}
\pinput{MQ_expectHHH}
\pinput{MQ_expectHH_TT}

\homeworkproblems
\pinput{PS_max_algorithm_expectation}
\pinput{PS_conditional_expectation_even_throws}
%\pinput{PS_absolute_binomial_deviation}  %SOLN NEEDS MORE EXPLANATION
\end{problems}


\section{Linearity of Expectation}\label{finlin}

Expected values obey a simple, very helpful rule called
\term{Linearity of Expectation}.  Its simplest form says that the
expected value of a sum of random variables is the sum of the expected
values of the variables.

\begin{theorem}\label{expsum-2}
For any random variables $R_1$ and $R_2$,
\[
\expect{R_1 + R_2} = \expect{R_1} + \expect{R_2}.
\]
\end{theorem}

\begin{proof}
Let $T \eqdef R_1+R_2$.  The proof follows straightforwardly by
rearranging terms in equation~\eqref{eqn:expectation} in the
definition of expectation:
\begin{align*}
\expect{T}
     & \eqdef \sum_{\omega \in \sspace} T(\omega) \cdot \pr{\omega}\\
%                & \text{(by~\eqref{eqn:expectation})}\\
        & = \sum_{\omega \in \sspace} (R_1(\omega) + R_2(\omega)) \cdot \pr{\omega}
                         & \text{(def of $T$)}\\
        & = \sum_{\omega \in \sspace} R_1(\omega) \pr{\omega} + 
              \sum_{\omega \in \sspace} R_2(\omega) \pr{\omega} & \text{(rearranging terms)}\\
        & = \expect{R_1} + \expect{R_2}.
                & \text{(by~\eqref{eqn:expectation})}
\end{align*}
\end{proof}

Essentially the same proof implies:
\begin{theorem}
For random variables $R_1$, $R_2$ and constants $a_1,a_2 \in \reals$,
\[
\expect{a_1R_1 + a_2R_2} = a_1\expect{R_1} + a_2\expect{R_2}.
\]
\end{theorem}
In other words, expectation is a linear function.  A routine induction
extends the result to more than two variables:
\begin{corollary}[\idx{Linearity of Expectation}]\label{linexp-k-thm}
For any random variables $R_1, \dots, R_k$ and constants $a_1, \dots, a_k
\in \reals$,
\[
\Expect{\sum_{i=1}^k a_iR_i} = \sum_{i=1}^k a_i\expect{R_i}.
\]
\end{corollary}

The great thing about linearity of expectation is that \emph{no
  independence is required}.  This is really useful, because dealing
with independence is a pain, and we often need to work with random
variables that are not independent.

\begin{editingnotes}
Even when the random variables \emph{are} independent, we know
from previous experience that proving independence requires a lot of
work.
\end{editingnotes}

As an example, let's compute the expected value of the sum of two fair
dice.

\subsection{Expected Value of Two Dice}

What is the expected value of the sum of two fair dice?

Let the random variable $R_1$ be the number on the first die, and let
$R_2$ be the number on the second die.  We observed earlier that the
expected value of one die is 3.5.  We can find the expected value of the
sum using linearity of expectation:
\begin{equation*}
\expect{R_1 + R_2} 
 =   \expect{R_1} + \expect{R_2}
 =    3.5 + 3.5
 =    7.
\end{equation*}

Assuming that the dice were independent, we could use a tree diagram
to prove that this expected sum is seven, but this would be a bother
since there are 36 cases.  And without assuming independence, it's not
apparent how to apply the tree diagram approach at all.  But notice
that we did \emph{not} have to assume that the two dice were
independent.  For example, suppose the roll of the second die was
forced to match the roll of the first die.  Then the expected sum of
two dice remains equal to seven because the second die is still fair.

\subsection{Sums of Indicator Random Variables}\label{sec:hat_check}

Linearity of expectation is especially useful when you have a sum of
indicator random variables.  As an example, suppose there is a dinner
party where $n$~men check their hats.  The hats are mixed up during
dinner, so that afterward each man receives a random hat.  In
particular, each man gets his own hat with probability~$1/n$.  What is
the expected number of men who get their own hat?

Letting $G$ be the number of men that get their own hat, we want to
find the expectation of $G$.  But all we know about $G$ is that the
probability that a man gets his own hat back is $1/n$.  There are many
different probability distributions of hat permutations with this
property, so we don't know enough about the distribution of $G$ to
calculate its expectation directly using
equation~\eqref{eqn:expectation} or~\eqref{eqn:altexpdef}.  But
linearity of expectation lets us sidestep this issue.

We'll use a standard, useful trick to apply linearity, namely, we'll
express~$G$ as a sum of indicator variables.  In particular, let
$G_i$~be an indicator for the event that the $i$th man gets his own
hat.  That is, $G_i = 1$ if the $i$th man gets his own hat, and $G_i =
0$ otherwise.  The number of men that get their own hat is then the
sum of these indicator random variables:
\begin{equation}\label{GG}
    G = G_1 + G_2 + \cdots + G_n.
\end{equation}
These indicator variables are \emph{not} mutually independent.  For
example, if $n-1$ men all get their own hats, then the last man is
certain to receive his own hat.  But again, we don't need to worry
about this dependence, since linearity holds regardless.

Since $G_i$~is an indicator random variable, we know from
Lemma~\ref{expindic} that
\begin{equation}
    \expect{G_i}= \prob{G_i=1} = 1/n.
\end{equation} 
By Linearity of Expectation and equation~\eqref{GG}, this means that
\begin{align*}
\expect{G} & = \expect{G_1 + G_2 + \cdots + G_n} \\
       & = \expect{G_1} + \expect{G_2} + \cdots + \expect{G_n}\\
       & = \overbrace{\frac{1}{n} + \frac{1}{n} + \cdots + \frac{1}{n}}^n \\
       &= 1.
\end{align*}
So even though we don't know much about how hats are scrambled, we've
figured out that on average, just one man gets his own hat back,
regardless of the number of men with hats!

More generally, Linearity of Expectation provides a very good method
for computing the expected number of events that will happen.

\begin{theorem}\label{thm:17T3}
Given any collection of events $A_1, A_2, \dots, A_n$, the expected
number of events that will occur is
\begin{equation*}
    \sum_{i = 1}^n \prob{A_i}.
\end{equation*}
\end{theorem}

For example, $A_i$~could be the event that the $i$th man gets the
right hat back.  But in general, it could be any subset of the sample
space, and we are asking for the expected number of events that will
contain a random sample point.

\begin{proof}

Define~$R_i$ to be the indicator random variable for~$A_i$, where
$R_i(\omega) = 1$ if $w \in A_i$ and $R_i(\omega) = 0$ if $w \notin A_i$.  Let
$R = R_1 + R_2 + \dots + R_n$.  Then
\begin{align*}
\expect{R}
    &= \sum_{i = 1}^n \expect{R_i}
    & \text{(by Linearity of Expectation)}\\
%
    &= \sum_{i = 1}^n \prob{R_i = 1}
    & \text{(by Lemma~\ref{expindic})}\\
%
%    &= \sum_{i = 1}^n \sum_{w \in A_i} \prob{w}
%    & \text{(definition of indicator variable)}\\
%
    &= \sum_{i = 1}^n \prob{A_i}. \qedhere  
        & \text{(def of indicator variable)}\\
\end{align*}
\end{proof}

So whenever you are asked for the expected number of events that occur,
all you have to do is sum the probabilities that each event occurs.
Independence is not needed.

\subsection{Expectation of a Binomial Distribution}

Suppose that we independently flip $n$~biased coins, each with
probability~$p$ of coming up heads.  What is the expected number of
heads?

Let $J$~be the random variable denoting the number of heads.  Then
$J$~has a binomial distribution with parameters~$n$,~$p$, and
\begin{equation*}
    \prob{J = k} = \binom{n}{k} p^k q^{n - k}.
\end{equation*}
Applying equation~\eqref{eqn:altexpdef}, this means that
\begin{equation} \label{eqn:17T7}
\expect{J}
    = \sum_{k = 0}^n k \prob{J = k}
    = \sum_{k = 0}^n k \binom{n}{k} p^k q^{n - k}.
\end{equation}

\iffalse
\begin{align}
\expect{J}
    &= \sum_{k = 0}^n k \prob{J = k} \notag\\
    &= \sum_{k = 0}^n k \binom{n}{k} p^k q^{n - k}. \label{eqn:17T7}
\end{align}
\fi

This sum looks a tad nasty, but linearity of expectation leads to an
easy derivation of a simple closed form.  We just express~$J$ as a sum
of indicator random variables, which is easy.  Namely, let $J_i$~be
the indicator random variable for the $i$th~coin coming up heads, that
is,
\begin{equation*}
J_i \eqdef \begin{cases}
        1 & \text{if the $i$th coin is heads} \\
        0 & \text{if the $i$th coin is tails}.
      \end{cases}
\end{equation*}
Then the number of heads is simply
\begin{equation*}
    J = J_1 + J_2 + \dots + J_n.
\end{equation*}
By Theorem~\ref{thm:17T3},
\begin{equation}\label{eqn:17T8}
\expect{J} = \sum_{i = 1}^n \prob{J_i} = pn. 
\end{equation}

That really was easy.  If we flip $n$~mutually independent coins, we
expect to get $pn$~heads.  Hence the expected value of a binomial
distribution with parameters $n$ and~$p$ is simply~$pn$.

But what if the coins are not mutually independent?  It doesn't
matter---the answer is still~$p n$ because Linearity of Expectation
and Theorem~\ref{thm:17T3} do not assume any independence.

If you are not yet convinced that Linearity of Expectation and
Theorem~\ref{thm:17T3} are powerful tools, consider this: without even
trying, we have used them to prove a complicated looking identity,
namely,
\begin{equation}\label{binomial-expectsum}
    \sum_{k = 0}^n k \binom{n}{k} p^k q^{n - k} = p n,
\end{equation}
which follows by combining equations~\eqref{eqn:17T7}
and~\eqref{eqn:17T8} (see also
Exercise~\ref{PS_binomial_expectation_formula}).

The next section has an even more convincing illustration of the power
of linearity to solve a challenging problem.

\subsection{The Coupon Collector Problem}

Every time we purchase a kid's meal at Taco Bell, we are graciously
presented with a miniature ``Racin' Rocket'' car together with a
launching device which enables us to project our new vehicle across
any tabletop or smooth floor at high velocity.  Truly, our delight
knows no bounds.

There are different colored Racin' Rocket cars.  The color of car
awarded to us by the kind server at the Taco Bell register appears to
be selected uniformly and independently at random.  What is the
expected number of kid's meals that we must purchase in order to
acquire at least one of each color of Racin' Rocket car?

The same mathematical question shows up in many guises: for example,
what is the expected number of people you must poll in order to find
at least one person with each possible birthday?
\begin{editingnotes}
Here, instead of collecting Racin' Rocket cars, you're collecting
birthdays.
\end{editingnotes}
The general question is commonly called the \term{coupon collector
  problem} after yet another interpretation.

A clever application of linearity of expectation leads to a simple
solution to the coupon collector problem.  Suppose there are five
different colors of Racin' Rocket cars, and we receive this sequence:
%
\begin{center}
blue \quad green \quad green \quad red \quad blue \quad orange \quad
blue \quad orange \quad gray.
\end{center}
%
Let's partition the sequence into 5 segments:
%
\[
\underbrace{\strut\text{blue}}_{X_0} \quad
\underbrace{\strut\text{green}}_{X_1} \quad
\underbrace{\strut\text{green} \quad \text{red}}_{X_2} \quad
\underbrace{\strut\text{blue} \quad \text{orange}}_{X_3} \quad
\underbrace{\strut\text{blue} \quad \text{orange} \quad \text{gray}}_{X_4}.
\]
%
The rule is that a segment ends whenever we get a new kind of car.
For example, the middle segment ends when we get a red car for the
first time.  In this way, we can break the problem of collecting every
type of car into stages.  Then we can analyze each stage individually
and assemble the results using linearity of expectation.

In the general case there are $n$~colors of Racin' Rockets that we're
collecting.  Let $X_k$~be the length of the $k$th segment.  The total
number of kid's meals we must purchase to get all $n$ Racin' Rockets
is the sum of the lengths of all these segments:
\[
T = X_0 + X_1 + \cdots + X_{n-1}.
\]

Now let's focus our attention on~$X_k$, the length of the $k$th
segment.  At the beginning of segment $k$, we have $k$~different types
of car, and the segment ends when we acquire a new type.  When we own
$k$ types, each kid's meal contains a type that we already have with
probability~$k / n$.  Therefore, each meal contains a new type of car
with probability $1 - k / n = (n - k) / n$.  Thus, the expected number
of meals until we get a new kind of car is $n / (n - k)$ by the Mean
Time to Failure rule.  This means that
%
\[
    \expect{X_k} = \frac{n}{n - k}.
\]

Linearity of expectation, together with this observation, solves the
coupon collector problem:
%
\begingroup
\openup\jot
\begin{align}
\expect{T}
  & = \expect{X_0 + X_1 + \cdots + X_{n-1}} \notag\\
%
  & = \expect{X_0} + \expect{X_1} + \cdots + \expect{X_{n-1}} \notag\\
%
  & = \frac{n}{n - 0} + \frac{n}{n - 1} + \cdots + \frac{n}{3} +
    \frac{n}{2} + \frac{n}{1} \notag\\
%
  & = n \paren{\frac{1}{n} + \frac{1}{n-1} + \cdots + \frac{1}{3} +
  \frac{1}{2} + \frac{1}{1}} \notag\\
%
  &= n \paren{\frac{1}{1} + \frac{1}{2} + \frac{1}{3} + \cdots +
      \frac{1}{n-1} + \frac{1}{n}} \notag\\
%
  & = n H_n \label{eqn:17T11}\\
  & \sim n \ln n. \notag
\end{align}
\endgroup

Cool!  It's those Harmonic Numbers again.

We can use equation~\eqref{eqn:17T11} to answer some concrete questions.
For example, the expected number of die rolls required to see every
number from 1 to 6 is:
%
\[
    6 H_6 = 14.7 \dots.
\]
%
And the expected number of people you must poll to find at least one
person with each possible birthday is:
%
\[
    365 H_{365} = 2364.6\dots.
\]

\subsection{Infinite Sums}

Linearity of expectation also works for an infinite number of random
variables provided that the variables satisfy an absolute convergence
criterion.

\begin{theorem}[Linearity of Expectation]\label{linexp}
Let $R_0$, $R_1$, \dots, be random variables such that
\[
\sum_{i = 0}^\infty \expect{\, \abs{R_i}\, }
\]
converges.  Then
\[
   \Expect{\sum_{i = 0}^\infty R_i} = \sum_{i = 0}^\infty \expect{R_i}.
\]
\end{theorem}

\begin{proof}
Let $T \eqdef \sum_{i = 0}^\infty R_i$.

We leave it to the reader to verify that, under the given convergence
hypothesis, all the sums in the following derivation are absolutely
convergent, which justifies rearranging them as follows:
\begin{align*}
\sum_{i=0}^\infty \expect{R_i}
    &= \sum_{i=0}^\infty \sum_{s \in \sspace} R_i(s) \cdot \prob{s}
            & \text{(Def.~\ref{def:expectation})}\\
    &= \sum_{s \in \sspace} \sum_{i=0}^\infty R_i(s) \cdot \prob{s}
           & \text{(exchanging order of summation)}\\
    &= \sum_{s \in \sspace} \left[ \sum_{i=0}^\infty R_i(s) \right] \cdot \prob{s}
                & \text{(factoring out $\prob{s}$)}\\
    &= \sum_{s \in \sspace} T(s) \cdot \prob{s} & \text{(Def.\ of $T$)}\\
    &= \expect{T} & \text{(Def.~\ref{def:expectation})}\\
    &= \Expect{\sum_{i = 0}^\infty R_i}. &  \text{(Def.\ of $T$)}. &\qedhere
\end{align*}
\end{proof}

\subsection{A Gambling Paradox}

One of the simplest casino bets is on ``red'' or ``black'' at the roulette
table.  In each play at roulette, a small ball is set spinning around a
roulette wheel until it lands in a red, black, or green colored slot.
The payoff for a bet on red or black matches the bet; for example, if you bet
$\$10$ on red and the ball lands in a red slot, you get back your original
$\$10$ bet plus another matching $\$10$.

The casino gets its advantage from the green slots, which make the
probability of both red and black each less than 1/2.  In the US, a
roulette wheel has 2 green slots among 18 black and 18 red slots, so
the probability of red is $18/38 \approx 0.473$.  In Europe,
where roulette wheels have only 1 green slot, the odds for red are a
little better---that is, $18/37 \approx 0.486$---but still less
than even.

Of course you can't expect to win playing roulette, even if you had
the good fortune to gamble against a \emph{fair} roulette wheel.  To
prove this, note that with a fair wheel, you are equally likely win or
lose each bet, so your expected win on any spin is zero.  Therefore if
you keep betting, your expected win is the sum of your expected wins
on each bet: still zero.

Even so, gamblers regularly try to develop betting strategies to win
at roulette despite the bad odds.  A well known strategy of this kind
is \emph{bet doubling}, where you bet, say, $\$10$ on red and keep
doubling the bet until a red comes up.  This means you stop playing if
red comes up on the first spin, and you leave the casino with a \$10
profit.  If red does not come up, you bet \$20 on the second spin.
Now if the second spin comes up red, you get your \$20 bet plus \$20
back and again walk away with a net profit of $\$20-10 = \$10$.  If
red does not come up on the second spin, you next bet \$40 and walk
away with a net win of $\$40 -20-10 = \$10$ if red comes up on on the
third spin, and so on.

Since we've reasoned that you can't even win against a fair wheel,
this strategy against an unfair wheel shouldn't work.  But wait a
minute!  There is a 0.486 probability of red appearing on each spin of
the wheel, so the mean time until a red occurs is less than three.
What's more, red will come up \emph{eventually} with probability one,
and as soon as it does, you leave the casino $\$10$ ahead.  In other
words, by bet doubling you are \emph{certain} to win $\$10$, and so
your expectation is $\$10$, not zero!

Something's wrong here.

\subsection{Solution to the Paradox}\label{infinitebankroll}

The argument claiming the expectation is zero against a fair wheel is
flawed by an implicit, invalid use of linearity of expectation for an
infinite sum.

To explain this carefully, let $B_n$ be the number of dollars you win
on your $n$th bet, where $B_n$ is defined to be zero if red comes up
before the $n$th spin of the wheel.  Now the dollar amount you win in
any gambling session is
\[
\sum_{n = 1}^\infty B_n,
\]
and your expected win is
\begin{equation}\label{sumBn}
\Expect{\sum_{n = 1}^\infty B_n}.
\end{equation}
Moreover, since we're assuming the wheel is fair, it's true that
$\expect{B_n} = 0$, so
\begin{equation}\label{sumEBn0}
\sum_{n = 1}^\infty \expect{B_n} = \sum_{n = 1}^\infty 0 = 0.
\end{equation}

The flaw in the argument that you can't win is the implicit appeal to
linearity of expectation to conclude that the
expectation~\eqref{sumBn} equals the sum of expectations
in~\eqref{sumEBn0}.  This is a case where linearity of expectation
fails to hold---even though the expectation~\eqref{sumBn} is 10 and
the sum~\eqref{sumEBn0} of expectations converges.  The problem is
that the expectation of the sum of the absolute values of the bets
diverges, so the condition required for infinite linearity fails.  In
particular, under bet doubling your $n$th bet is $10 \cdot 2^{n-1}$
dollars while the probability that you will make an $n$th bet is
$2^{-n}$.  So
\[
\expect{\abs{B_n}} = 10 \cdot 2^{n-1} 2^{-n} = 5.
\]
Therefore the sum
\[
\sum_{n = 1}^\infty \expect{\abs{B_n}} = 5 + 5 + 5 + \cdots
\]
diverges rapidly.

So the presumption that you can't beat a fair game, and the argument
we offered to support this presumption, are mistaken: by bet doubling,
you can be sure to walk away a winner.  Probability theory has led to
an apparently absurd conclusion.

But probability theory shouldn't be rejected because it leads to this
absurd conclusion.  If you only had a finite amount of money to bet
with---say enough money to make $k$ bets before going bankrupt---then
it would be correct to calculate your expection by summing
$B_1+B_2+\cdots+B_k$, and your expectation would be zero for the fair
wheel and negative against an unfair wheel.  In other words, in order
to follow the bet doubling strategy, you need to have an infinite
bankroll.  So it's absurd to assume you could actually follow a bet
doubling strategy, and we needn't be concerned when an absurd
assumption leads to an absurd conclusion.

\subsection{Expectations of Products}

While the expectation of a sum is the sum of the expectations, the same is
usually not true for products.  For example, suppose that we roll a
fair 6-sided die and denote the outcome with the random variable~$R$.
Does $\expect{R \cdot R} = \expect{R} \cdot \expect{R}$?

We know that $\expect{R} = 3\frac{1}{2}$ and thus $\paren{\expect{R}}^2 =
12\frac{1}{4}$.  Let's compute~$\expect{R^2}$ to see if we get the same
result.
\begin{align*}
\Expect{R^2}
   & = \sum_{\omega \in \sspace} R^2(\omega) \prob{w}
     = \sum_{i=1}^6 i^2 \cdot \prob{R_i = i} \\
   & = \frac{1^2}{6} + \frac{2^2}{6} + \frac{3^2}{6} +
            \frac{4^2}{6} + \frac{5^2}{6} + \frac{6^2}{6} 
     =  15\; 1/6 \neq  12 \; 1/4.
\end{align*}

\iffalse
\begin{align*}
\Expect{R^2}
    & = \sum_{\omega \in \sspace} R^2(\omega) \prob{w} \\
    & = \sum_{i=1}^6 i^2 \cdot \prob{R_i = i} \\
    & = \frac{1^2}{6} + \frac{2^2}{6} + \frac{3^2}{6} +
            \frac{4^2}{6} + \frac{5^2}{6} + \frac{6^2}{6} \\
    & =   15\; 1/6\\
    & \neq  12 \; 1/4.
\end{align*}
\fi

That is,
\[
    \expect{R \cdot R} \neq \expect{R} \cdot \expect{R}.
\]
So the expectation of a product is not always equal to the product of
the expectations.

There is a special case when such a relationship \emph{does} hold
however; namely, when the random variables in the product are
\emph{independent}.

\begin{theorem}\label{th:prod}
For any two \emph{independent} random variables $R_1$, $R_2$,
\[
\expect{R_1 \cdot R_2} = \expect{R_1} \cdot \expect{R_2}.
\]
\end{theorem}

The proof follows by rearrangement of terms in the sum that
defines $\expect{R_1 \cdot R_2}$.  Details appear in
Problem~\ref{CP_independent_product}.

\iffalse
\begin{proof}
The event $[R_1 \cdot R_2=r]$ can be split up into events of the form
$[R_1 = r_1\ \text{ and }\ R_2 = r_2]$ where $r_1\cdot r_2=r$.  So
\begin{align*}
\lefteqn{\expect{R_1 \cdot R_2}}\\
& \eqdef \sum_{r \in \range{R_1\cdot R_2}} r\cdot \pr{R_1\cdot R_2=r}\\
\iffalse
& =      \sum_{\scriptsize \begin{aligned}
                       r_1 \in \range{R_1},\\
                       r_2 \in \range{R_2}
                      \end{aligned}}\fi
& =      \sum_{r_i \in \range{R_i}}
            r_1 r_2 \cdot \pr{R_1=r_1\ \text{ and }\ R_2=r_2}\\
& =      \sum_{r_1 \in \range{R_1}} \sum_{r_2 \in \range{R_2}}
            r_1 r_2 \cdot \pr{R_1=r_1\ \text{ and }\ R_2=r_2}
                    &\text{(ordering terms in the sum)}\\
& =      \sum_{r_1 \in \range{R_1}} \sum_{r_2 \in \range{R_2}}
            r_1 r_2 \cdot \pr{R_1=r_1}\cdot \pr{R_2=r_2}
                    &\text{(indep.\ of $R_1,R_2$)}\\
& =      \sum_{r_1 \in \range{R_1}} \paren{r_1\pr{R_1=r_1} \cdot
              \sum_{r_2 \in \range{R_2}} r_2 \pr{R_2=r_2}}
                    &\text{(factoring out $r_1\pr{R_1=r_1}$)}\\
& =      \sum_{r_1 \in \range{R_1}} r_1\pr{R_1=r_1} \cdot \expect{R_2}
                    &\text{(def of $\expect{R_2}$)}\\
& =       \expect{R_2} \cdot \sum_{r_1 \in \range{R_1}} r_1\pr{R_1=r_1}
                    &\text{(factoring out $\expect{R_2}$)}\\
& =       \expect{R_2} \cdot  \expect{R_1}.
                    &\text{(def of $\expect{R_1}$)}
\end{align*}

\end{proof}
\fi

Theorem~\ref{th:prod} extends routinely to a collection of mutually
independent variables.
\begin{corollary}\label{cor:indep_prod}[Expectation of Independent Product]

If random variables $R_1, R_2, \dots, R_k$ are mutually
independent, then
\[
\Expect{\prod_{i=1}^k R_i} = \prod_{i=1}^k \expect{R_i}.
\]
\end{corollary}

\section{Really Great Expectations}\label{infinite_expect_sec}

Making independent tosses of a fair coin until some desired pattern
comes up is a simple process you should feel in some command of by
now, right?  So how about a bet about the simplest such
process---tossing until a head comes up?  Ok, you're wary of betting
with us, but how about this: we'll let \emph{you set the odds}.

\subsection{Repeating Yourself}\label{infinite_repeat_subsec}
Here's the bet: you make independent tosses of a fair coin until a
head comes up.  Then you will repeat the process.  If a second head
comes up in the same or fewer tosses than the first, you have to start
over yet again.  You keep starting over until you finally toss a run
of tails longer than your first one.  The payment rules are that you
will pay me 1 cent each time you start over.  When you win by finally
getting a run of tails longer than your first one, I will pay you some
generous amount.  Notice by the way that you're certain to
win---whatever your initial run of tails happened to be, a longer run
will eventually occur again with probability 1!

For example, if your first tosses are \texttt{TTTH}, then you will
keep tossing until you get a run of 4 tails.  So your winning
flips might be
\[
\texttt{TTTHTHTTHHTTHTHTTTHTHHHTTTT}.
\]
In this run there are 10 heads, which means you had to start over 9
times.  So you would have paid me 9 cents by the time you finally won
by tossing 4 tails.  Now you've won, and I'll pay you generously
---how does 25 cents sound?  Maybe you'd rather have \$1?  How about
\$1000?

Of course there's a trap here.  Let's calculate your expected
winnings.

Suppose your initial run of tails had length $k$.  After that, each
time a head comes up, you have to start over and try to get $k+1$ tails
in a row.  If we regard your getting $k+1$ tails in a row as a
``failed'' try, and regard your having to start over because a head
came up too soon as a ``successful'' try, then the number of times you
have to start over is the number of tries till the first failure.  So
the expected number of tries will be the mean time to failure, which is
$2^{k+1}$, because the probability of tossing $k+1$ tails in a row is
$2^{-(k+1)}$.

Let $T$ be the length of your initial run of tails.  So $T = k$ means
that your initial tosses were $\texttt{T}^k\texttt{H}$.  Let $R$ be
the number of times you repeat trying to beat your original run of
tails.  The number of cents you expect to finish with is the number of
cents in my generous payment minus $\expect{R}$.  It's now easy to
calculate $\expect{R}$ by conditioning on the value of $T$:
\[
\expect{R}
     = \sum_{k \in \nngint} \expcond{R}{T=k} \cdot \prob{T=k}
     = \sum_{k \in \nngint}  2^{k+1} \cdot 2^{-(k+1)}
     = 1 + 1 + 1+ \cdots = \infty.
\]

\iffalse
\begin{align*}
\expect{R}
    & = \sum_{k \in \nngint} \expcond{R}{T=k} \cdot \prob{T=k}\\
    & = \sum_{k \in \nngint}  2^{k+1} \cdot 2^{-(k+1)}\\
    & = \sum_{k \in \nngint} 1 = \infty.
\end{align*}
\fi

So you can expect to pay me an infinite number of cents before winning
my ``generous'' payment.  No amount of generosity can make this bet
fair!  In fact this particular example is a special case of an
astonishingly general one: \iffalse worked out in
Problem~\ref{PS_infinite_repeat_expectation}\fi the expected waiting
time for \emph{any} random variable to achieve a larger value remains
infinite.

\begin{editingnotes}
\subsection{Coping with Infinite Expectation}

Infinite expectation arises directly from this very simple scenario in
Section~\ref{infinite_repeat_subsec}.  It came up previously in
Section~\ref{infinitebankroll} when we discussed bet doubling.
Another example will appear in Chapter~\ref{ran_process_chap}.

The Weak Law of Large Numbers requires finite expectation, so it
offers no direct guidance for understanding infinite expectation.
\end{editingnotes}

\begin{problems}
\classproblems
\pinput{MQ_infinite_repeat}
%\pinput{TP_infinite_repeat}
%\pinput{PS_infinite_repeat_expectation}
%\pinput{TP_repeathalf}

\examproblems
\pinput{FP_infinite_repeat}
\end{problems}

\begin{editingnotes}

\section{Expectations of Quotients}

If $S$ and~$T$ are random variables, we know from Linearity of
Expectation that
\begin{equation*}
    \expect{S + T} = \expect{S} + \expect{T}.
\end{equation*}
If $S$ and~$T$ are independent, we know from Theorem~\ref{th:prod}
that
\begin{equation*}
    \expect{ST} = \expect{S} \expect{T}.
\end{equation*}

Is it also true that
\begin{equation}\label{eqn:17P1}
    \expect{S/T} = \expect{S}/\expect{T}?
\end{equation}
Of course, we have to worry about the situation when~$\expect{T} = 0$,
but what if we assume that $T$~is always positive?  As we will soon
see, equation~\eqref{eqn:17P1} is usually not true, but let's see if we
can prove it anyway.

\begin{falseclm}\label{fc:17P2}
If $S$ and~$T$ are independent random variables with $T > 0$, then
\begin{equation}
    \expect{S/T} = \expect{S}/\expect{T}.
\end{equation}
\end{falseclm}

\begin{bogusproof}
\begin{align}
\expect{\frac{S}{T}} & = \expect{S \cdot \frac{1}{T}} \notag\\
       & = \expect{S} \cdot \Expect{\frac{1}{T}} & \text{(independence of $S$
       and $T$)}\label{indST}\\
      & = \expect{S} \cdot \frac{1}{\expect{T}}. \label{bugindST}\\
      & = \frac{\expect{S}}{\expect{T}}.\notag \qedhere
\end{align}
\end{bogusproof}
Note that line~\ref{indST} uses the fact that if $S$ and~$T$ are
independent, then so are $S$ and~$1/T$.  This holds because functions
of independent random variables are independent.  It is a fact that
needs proof (see Problem~\ref{PS_independent_random_variables}) which
we will leave to the reader, but it is not the bug.  The bug is in
line~\eqref{bugindST}, which assumes
\begin{falseclm}\label{false-inverse}
\[
\expect{\frac{1}{T}} =  \frac{1}{\expect{T}}.
\]
\end{falseclm}

Here is a counterexample.  Define~$T$ so that
\begin{equation*}
    \prob{T = 1} = \frac{1}{2} \quad\text{and}\quad
    \prob{T = 2} = \frac{1}{2}.
\end{equation*}
Then
\begin{equation*}
    \expect{T} = 1 \cdot \frac{1}{2} + 2 \cdot \frac{1}{2} =
    \frac{3}{2}
\end{equation*}
and
\begin{equation*}
    \frac{1}{\expect{T}} = \frac{2}{3}
\end{equation*}
and
\begin{equation*}
\Expect{\frac{1}{T}}
    = \frac{1}{1} \cdot \frac{1}{2} + \frac{1}{2} \cdot \frac{1}{2}
    = \frac{3}{4} 
    \ne \frac{1}{\expect{1/T}}.
\end{equation*}
This means that Claim~\ref{fc:17P2} is also false since we could
define~$S = 1$ with probability~1.  In fact, both Claims \ref{fc:17P2}
and~\ref{false-inverse} are untrue for most all choices of $S$
and~$T$.  Unfortunately, the fact that they are false does not keep
them from being widely used in practice!  Let's see an example.

\subsection{A RISC Paradox}

The data in Table~\ref{fig:17P4} is representative of data in a paper
by some famous professors.  They wanted to show that programs on a
RISC processor are generally shorter than programs on a CISC
processor.  For this purpose, they applied a RISC compiler and then a
CISC compiler to some benchmark source programs and made a table of
compiled program lengths.

\begin{table}

\begin{tabular}{lccc}
Benchmark        & RISC          & CISC          & CISC/RISC\\
\hline
E-string search  & 150           & 120           & 0.8 \\
F-bit test       & 120           & 180           & 1.5 \\
Ackerman         & 150           & 300           & 2.0 \\
Rec 2-sort       & 2800          & 1400          & 0.5 \\
\hline
Average          &               &               & 1.2
\end{tabular}

\caption{Sample program lengths for benchmark problems using RISC and
  CISC compilers.}

\label{fig:17P4}

\end{table}

Each row in Table~\ref{fig:17P4} contains the data for one benchmark.
The numbers in the second and third columns are program lengths for
each type of compiler.  The fourth column contains the ratio of the
CISC program length to the RISC program length.  Averaging this ratio
over all benchmarks gives the value 1.2 in the lower right.  The
conclusion is that CISC programs are 20\% longer on average.

However, some critics of their paper took the same data and argued this
way: redo the final column, taking the other ratio, RISC/CISC instead of
CISC/RISC, as shown in Table~\ref{fig:17P5}.

\begin{table}

\begin{tabular}{lccc}
Benchmark        & RISC          & CISC          & RISC/CISC\\
\hline
E-string search  & 150           & 120           & 1.25 \\
F-bit test       & 120           & 180           & 0.67 \\
Ackerman         & 150           & 300           & 0.5 \\
Rec 2-sort       & 2800          & 1400          & 2.0 \\
\hline
Average          &               &               & 1.1
\end{tabular}

\caption{The same data as in Table~\ref{fig:17P4}, but with the
  opposite ratio in the last column.}

\label{fig:17P5}

\end{table}

From Table~\ref{fig:17P5}, we would conclude that RISC programs are
10\% longer than CISC programs on average!  We are using the same
reasoning as in the paper, so this conclusion is equally
justifiable---yet the result is opposite.  What is going on?

\subsubsection{A Probabilistic Interpretation}

To resolve these contradictory conclusions, we can model the RISC
vs.\ CISC debate with the machinery of probability theory.

Let the sample space be the set of benchmark programs.  Let the random
variable $R$ be the length of the compiled RISC program, and let the
random variable $C$ be the length of the compiled CISC program.  We would
like to compare the average length~$\expect{R}$ of a RISC program to the
average length~$\expect{C}$ of a CISC program.

To compare average program lengths, we must assign a probability to
each sample point; in effect, this assigns a ``weight'' to each
benchmark.  One might like to weigh benchmarks based on how frequently
similar programs arise in practice.  Lacking such data, however, we
will assign all benchmarks equal weight; that is, our sample space is
uniform.

In terms of our probability model, the paper computes $C / R$ for each
sample point, and then averages to obtain $\expect{C / R} = 1.2$.
This much is correct.  The authors then conclude that CISC programs
are 20\% longer on average; that is, they conclude that $\expect{C} =
1.2\, \expect{R}$.  Therein lies the problem.  The authors have
implicitly used False Claim~\ref{fc:17P2} to assume that $\expect{C/R}
= \expect{C}/\expect{R}$.  By using the same false logic, the critics
can arrive at the opposite conclusion; namely, that RISC programs are
10\%~longer on average.

\subsubsection{The Proper Quotient}

We can compute $\expect{R}$ and $\expect{C}$ as follows:
\begin{align*}
\expect{R}  
    & = \sum_{i \in \text{Range(R)}} i \cdot \prob{R = i} \\
    & = \frac{150}{4}+\frac{120}{4}+\frac{150}{4}+\frac{2800}{4} \\
    & = 805, \\
\\
\expect{C}
    & = \sum_{i \in \text{Range(C)}} i \cdot \prob{C = i} \\
    & = \frac{120}{4}+\frac{180}{4}+\frac{300}{4}+\frac{1400}{4} \\
    & = 500
\end{align*}

Now since $\expect{R}/\expect{C} = 1.61$, we conclude that the
\emph{average RISC program} is 61\% longer than the \emph{average CISC
  program}.  This is a third answer, completely different from the
other two!  Furthermore, this answer makes RISC look really bad in
terms of code length.  This one is the correct conclusion, under our
assumption that the benchmarks deserve equal weight.  Neither of the
earlier results were correct---not surprising since both were based on
the same False Claim.

\subsubsection{A Simpler Example}

The source of the problem is clearer in the following, simpler
example.  Suppose the data were as follows.
\begin{center}
\begin{tabular}{lcccc}
Benchmark   & Processor $A$ & Processor $B$ & $B / A$   & $A / B$  \\
\hline
Problem 1   & 2             & 1             & 1/2       & 2 \\
Problem 2   & 1             & 2             & 2         & 1/2 \\
\hline
Average     &               &               & 1.25      & 1.25
\end{tabular}
\end{center}

Now the data for the processors $A$ and~$B$ is exactly symmetric; the
two processors are equivalent.  Yet, from the third column we would
mistakenly conclude that Processor~$B$ programs are 25\% longer on
average, and from the fourth column we would mistakenly conclude that
Processor~$A$ programs are 25\% longer on average.

The moral is that one must be very careful in summarizing data, we must
not take an average of ratios blindly!

%MAKE THIS A PROBLEM

\subsubsection{The Number-Picking Game}

Here is a game that you and I could play that reveals a strange
property of expectation.

First, you think of a probability density function on the natural
numbers.  Your distribution can be absolutely anything you like.  For
example, you might choose a uniform distribution on $1, 2, \dots, 6$,
like the outcome of a fair die roll.  Or you might choose a binomial
distribution on $0, 1, \dots, n$.  You can even give every natural
number a non-zero probability, provided that the sum of all
probabilities is 1.

Next, I pick a random number $z$ according to your distribution.
Then, you pick a random number $y_1$ according to the same
distribution.  If your number is bigger than mine ($y_1 > z$), then
the game ends.  Otherwise, if our numbers are equal or mine is bigger
($z \geq y_1$), then you pick a new number $y_2$ with the same
distribution, and keep picking values $y_3$, $y_4$, etc. until you get
a value that is strictly bigger than my number $z$.  What is the
expected number of picks that you must make?

Certainly, you always need at least one pick, so the expected number
is greater than one.  An answer like 2 or 3 sounds reasonable, though
one might suspect that the answer depends on the distribution.  Let's
find out whether or not this intuition is correct.

The number of picks you must make is a natural-valued random variable, so
from formula~\eqref{R>i} we have:
\begin{align}
\expect{\text{\# picks by you}}
    & = \sum_{k \in \nngint} \prob{\text{(\# picks by you)} > k} \label{eqn:1}
\end{align}
Suppose that I've picked my number $z$, and you have picked $k$
numbers $y_1, y_2, \dots, y_k$.  There are two possibilities:
%
\begin{itemize}

\item If there is a unique largest number among our picks, then my
number is as likely to be it as any one of yours.  So with probability
$1/(k+1)$ my number is larger than all of yours, and you must pick
again.

\item Otherwise, there are several numbers tied for largest.  My
number is as likely to be one of these as any of your numbers, so with
probability greater than $1/(k+1)$ you must pick again.

\end{itemize}
%
In both cases, with probability at least $1/(k+1)$, you need more than
$k$ picks to beat me.  In other words:
%
\begin{align}
\prob{\text{(\# picks by you)} > k} \geq \frac{1}{k+1} \label{eqn:2}
\end{align}

This suggests that in order to minimize your rolls, you should choose a
distribution such that ties are very rare.  For example, you might
choose the uniform distribution on $\set{1, 2, \dots, 10^{100}}$.  In
this case, the probability that you need more than $k$ picks to beat
me is very close to $1/(k+1)$ for moderate values of $k$.  For
example, the probability that you need more than 99 picks is almost
exactly 1\%.  This sounds very promising for you; intuitively, you
might expect to win within a reasonable number of picks on average!

Unfortunately for intuition, there is a simple proof that the expected
number of picks that you need in order to beat me is
\emph{infinite}, regardless of the distribution!  Let's
plug~\eqref{eqn:2} into~\eqref{eqn:1}:
%
\begin{align*}
\expect{\text{\# picks by you}}
    & = \sum_{k \in \nngint} \frac{1}{k+1} \\
    & = \infty
\end{align*}

\textcolor{blue}{For WALD'S theorem see F02 ln11-12.}
\end{editingnotes}

\begin{problems}
\practiceproblems
\pinput{TP_psets_and_laundry}
\pinput{TP_grading_final_exam}

\classproblems
\pinput{CP_sixteen_desks}
\pinput{CP_probable_satisfiability}
\pinput{CP_probable_satisfiability_nk}
\pinput{CP_class_expectation}
\pinput{CP_expected_number_of_keys}
\pinput{CP_independent_product}
\pinput{CP_fair_martingale}

\homeworkproblems
\pinput{PS_binomial_expectation_formula}
\pinput{PS_expected_gain}
\pinput{PS_expected_time_to_TTH}
\pinput{PS_testing_soldiers}
\pinput{PS_roulette_expectation}
\pinput{PS_independent_random_variables}
\pinput{PS_probability_of_peeta_bread}

\examproblems
\pinput{FP_expected_black}
\pinput{FP_martingale}
\pinput{FP_expected_adjacent}
\pinput{FP_expected_adjacent2}


\end{problems}

\endinput
