\chapter{Independence}

\section{Definitions}

Suppose that we flip two fair coins simultaneously on opposite sides
of a room.  Intuitively, the way one coin lands does not affect the
way the other coin lands.  The mathematical concept that captures
this intuition is called \term{independence}:
\begin{definition}\label{def:independence}
Events $A$ and $B$ are independent if $\pr{B} = 0$ or if
\begin{equation}\label{eqn:independence}
    \prcond{A}{B} = \pr{A}.
\end{equation}
\end{definition}
In other words, $A$ and~$B$ are independent if knowing that $B$
happens does not alter the probability that $A$~happens, as is the
case with flipping two coins on opposite sides of a room.

\subsection{Potential Pitfall}

Students sometimes get the idea that disjoint events are independent.
The \emph{opposite} is true: if $A \intersect B = \emptyset$, then
knowing that $A$ happens means you know that $B$ does not happen.  So
disjoint events are \emph{never} independent---unless one of them has
probability zero.

\subsection{Alternative Formulation}

Sometimes it is useful to express independence in an alternate form:

\begin{theorem}\label{thm:16A1}
$A$ and~$B$ are independent if and only if
\begin{equation}\label{eqn:15D3}
    \pr{A \intersect B} = \pr{A} \cdot \pr{B}.
\end{equation}
\end{theorem}

\begin{proof}
There are two cases to consider depending on whether or not $\prob{B} =
0$.
\begin{description}

\item[Case 1 $(\prob{B} = 0)$:]
If $\prob{B} = 0$, $A$ and~$B$ are independent by
Definition~\ref{def:independence}.  In addition,
Equation~\ref{eqn:15D3} holds since both sides are~0.  Hence, the
theorem is true in this case.

\item[Case 2 $(\prob{B} > 0)$:]
By Definition~\ref{LN12:prcond},
\begin{equation*}
    \prob{A \cap B} = \prcond{A}{B} \prob{B}.
\end{equation*}
So Equation~\ref{eqn:15D3} holds if
\begin{equation*}
    \prcond{A}{B} = \prob{A},
\end{equation*}
which, by Definition~\ref{def:independence}, is true iff $A$ and~$B$
are independent.  Hence, the theorem is true in this case as well.
\qedhere
\end{description}
\end{proof}

\section{Independence Is an Assumption}

Generally, independence is something that you \emph{assume} in
modeling a phenomenon.  For example, consider the experiment of
flipping two fair coins.  Let $A$~be the event that the first coin
comes up heads, and let $B$~be the event that the second coin is
heads.  If we assume that $A$ and~$B$ are independent, then the
probability that both coins come up heads is:
%
\begin{equation*}
\pr{A \intersect B}  = \pr{A} \cdot \pr{B} %\\[2pt]
               = \frac{1}{2} \cdot \frac{1}{2} %\\[2pt]
               = \frac{1}{4}.
\end{equation*}

In this example, the assumption of independence is reasonable.  The
result of one coin toss should have negligible impact on the outcome
of the other coin toss.  And if we were to repeat the experiment many
times, we would be likely to have~$A \cap B$ about~1/4 of the time.

There are, of course, many examples of events where assuming
independence is \emph{not} justified, For example, let $C$~be the
event that tomorrow is cloudy and $R$ be the event that tomorrow is
rainy.  Perhaps $\pr{C} = 1/5$ and $\pr{R} = 1/10$ in Boston.  If
these events were independent, then we could conclude that the
probability of a rainy, cloudy day was quite small:
%
\begin{equation*}
\pr{R \intersect C} = \pr{R} \cdot \pr{C} % \\[2pt]
               = \frac{1}{5} \cdot \frac{1}{10} % \\[2pt]
               = \frac{1}{50}.
\end{equation*}
%
Unfortunately, these events are definitely not independent; in
particular, every rainy day is cloudy.  Thus, the probability of a
rainy, cloudy day is actually~$1/10$.

Deciding when to \emph{assume} that events are independent is a tricky
business.  In practice, there are strong motivations to assume
independence since many useful formulas (such as
Equation~\ref{eqn:15D3}) only hold if the events are independent.  But
you need to be careful lest you end up deriving false conclusions.
We'll see several famous examples where (false) assumptions of
independence led to trouble over the next several chapters.  This
problem gets even trickier when there are more than two events in
play.

\section{Mutual Independence}

\subsection{Definition}

We have defined what it means for two events to be independent.  What
if there are more than two events?  For example, how can we say that
the flips of $n$~coins are all independent of one another?

Events $E_1, \ldots, E_n$ are said to be \term{mutually independent}
if and only if the probability of any event~$E_i$ is unaffected by
knowledge of the other events.  More formally:

\begin{definition}\label{def:mutual_independence}
A set of events~$E_1, E_2, \dots, E_n$, is \term{mutually independent}
if $\forall i \in [1, n]$ and $\forall S \subseteq [1, n] - \set{i}$,
either
\begin{equation*}
    \Prob{\bigcap_{j \in S} E_j} = 0
\quad
\text{or}
\quad
    \prob{E_i} = \prcond{E_i}{\bigcap_{j \in S} E_j}.
\end{equation*}
\end{definition}

In other words, no matter which other events are known to occur, the
probability that $E_i$~occurs is unchanged for any~$i$.

For example, if we toss 100~fair coins at different times, we might
reasonably assume that the tosses are mutually independent since the
probability that the $i$th coin is heads should be~$1/2$, no matter
which other coin tosses came out heads.

\subsection{Alternative Formulation}

Just as Theorem~\ref{thm:16A1} provided an alternative definition of
independence for two events, there is an alternative definition for
mutual independence.

\begin{theorem}\label{thm:16A2}
A set of events~$E_1, E_2, \dots, E_n$ is mutually independent iff
$\forall S \subseteq [1, n]$,
\begin{equation*}
    \Prob{\bigcap_{j \in S} E_j} = \prod_{j \in S} \prob{E_j}.
\end{equation*}
\end{theorem}

The proof of Theorem~\ref{thm:16A2} uses induction and reasoning
similar to the proof of Theorem~\ref{thm:16A1}.  We will not include
the details here.

Theorem~\ref{thm:16A2} says that $E_1, E_2, \dots, E_n$~are mutually
independent if and only if all of the following equations hold for all
distinct $i$, $j$, $k$, and~$l$:
%
\begin{align*}
\pr{E_i \intersect E_j}
    & = \pr{E_i} \cdot \pr{E_j}
%    & \text{for all distinct $i$, $j$}
 \\
\pr{E_i \intersect E_j \intersect E_k}
    & = \pr{E_i} \cdot \pr{E_j} \cdot \pr{E_k}
%     & \text{for all distinct $i$, $j$, $k$}
 \\
\pr{E_i \intersect E_j \intersect E_k \intersect E_l}
    & = \pr{E_i} \cdot \pr{E_j} \cdot \pr{E_k} \cdot \pr{E_l}
%    & \text{for all distinct $i$, $j$, $k$, $l$}
 \\
    & \XasWideAsY{\vdots}{${}={}$} \\
\pr{E_1 \intersect \cdots \intersect E_n} & = \pr{E_1} \cdots \pr{E_n}.
\end{align*}

For example, if we toss $n$~fair coins, the tosses are mutually
independent iff for all~$m \in [1, n]$ and every subset of~$m$~coins,
the probability that every coin in the subset comes up heads is~$2^{-m}$.

\subsection{DNA Testing}

Assumptions about independence are routinely made in practice.
Frequently, such assumptions are quite reasonable.  Sometimes,
however, the reasonableness of an independence assumption is not so
clear, and the consequences of a faulty assumption can be severe.

For example, consider the following testimony from the O. J. Simpson
murder trial on May 15, 1995:
\begin{description}

\item[Mr. Clarke:] When you make these estimations of frequency---and
I believe you touched a little bit on a concept called independence?

\item[Dr. Cotton:] Yes, I did.

\item[Mr. Clarke:] And what is that again?

\item[Dr. Cotton:] It means whether or not you inherit one allele that
you have is not---does not affect the second allele that you might
get.  That is, if you inherit a band at 5,000 base pairs, that doesn't
mean you'll automatically or with some probability inherit one at
6,000.  What you inherit from one parent is what you inherit from the
other.

\item[Mr. Clarke:] Why is that important?

\item[Dr. Cotton:] Mathematically that's important because if that
were not the case, it would be improper to multiply the frequencies
between the different genetic locations.

\item[Mr. Clarke:] How do you---well, first of all, are these markers
independent that you've described in your testing in this case?

\end{description}

Presumably, this dialogue was as confusing to you as it was for the
jury.  Essentially, the jury was told that genetic markers in blood
found at the crime scene matched Simpson's.  Furthermore, they were
told that the probability that the markers would be found in a
randomly-selected person was at most 1 in 170 million.  This
astronomical figure was derived from statistics such as:
%
\begin{itemize}
\item 1 person in 100 has marker $A$.
\item 1 person in 50 marker $B$.
\item 1 person in 40 has marker $C$.
\item 1 person in 5 has marker $D$.
\item 1 person in 170 has marker $E$.
\end{itemize}
%
Then these numbers were multiplied to give the probability that a
randomly-selected person would have all five markers:
%
\begin{align*}
\pr{A \intersect B \intersect C \intersect D \intersect E}
    & = \pr{A} \cdot \pr{B} \cdot \pr{C} \cdot \pr{D} \cdot \pr{E} \\[2pt]
    & = \frac{1}{100} \cdot \frac{1}{50} \cdot \frac{1}{40}
                      \cdot \frac{1}{5} \cdot \frac{1}{170} \\[2pt]
    & = \frac{1}{170{,}000{,}000}.
\end{align*}
%
The defense pointed out that this assumes that the markers appear
mutually independently.  Furthermore, all the statistics were based on
just a few hundred blood samples.  

After the trial, the jury was widely mocked for failing to
``understand'' the DNA evidence.  If you were a juror, would
\emph{you} accept the 1 in 170 million calculation?

\section{Pairwise Independence}

The definition of mutual independence seems awfully
complicated---there are so many subsets of events to consider!  Here's
an example that illustrates the subtlety of independence when more
than two events are involved.
Suppose that we flip three fair, mutually-independent coins.  Define
the following events:
%
\begin{itemize}
\item $A_1$ is the event that coin 1 matches coin 2.
\item $A_2$ is the event that coin 2 matches coin 3.
\item $A_3$ is the event that coin 3 matches coin 1.
\end{itemize}
%
Are $A_1$, $A_2$, $A_3$ mutually independent?

The sample space for this experiment is:
%
\[
    \set{HHH,\, HHT,\, HTH,\, HTT,\, THH,\, THT,\, TTH,\, TTT}.
\]
%
Every outcome has probability $(1/2)^3 = 1/8$ by our assumption that
the coins are mutually independent.

To see if events $A_1$, $A_2$, and $A_3$ are mutually independent, we
must check a sequence of equalities.  It will be helpful first to
compute the probability of each event $A_i$:
%
\begin{align*}
\pr{A_1} & = \pr{HHH} + \pr{HHT} + \pr{TTH} + \pr{TTT} \\[2pt]
         & = \frac{1}{8} + \frac{1}{8} + \frac{1}{8} + \frac{1}{8}\\[2pt]
         & = \frac{1}{2}.
\end{align*}
%
By symmetry, $\pr{A_2} = \pr{A_3} = 1/2$ as well.  Now we can begin
checking all the equalities required for mutual independence in
Theorem~\ref{thm:16A2}:
%
\begin{align*}
\pr{A_1 \intersect A_2}
	& = \pr{HHH} + \pr{TTT} \\[2pt]
        & = \frac{1}{8} + \frac{1}{8} \\[2pt]
        & = \frac{1}{4} \\[2pt]
        & = \frac{1}{2} \cdot \frac{1}{2}\\[2pt]
        & = \pr{A_1} \pr{A_2}.
\end{align*}
%
By symmetry, $\pr{A_1 \intersect A_3} = \pr{A_1} \cdot \pr{A_3}$ and
$\pr{A_2 \intersect A_3} = \pr{A_2} \cdot \pr{A_3}$ must hold also.
Finally, we must check one last condition:
%
\begin{align*}
\pr{A_1 \intersect A_2 \intersect A_3}      & = \pr{HHH} + \pr{TTT} \\[2pt]
                                & = \frac{1}{8} + \frac{1}{8} \\[2pt]
                                & = \frac{1}{4} \\[2pt]
                                & \neq \pr{A_1} \pr{A_2} \pr{A_3} = \frac{1}{8}.
\end{align*}
%
The three events $A_1$, $A_2$, and~$A_3$ are not mutually independent
even though any two of them are independent!  This not-quite mutual
independence seems weird at first, but it happens.  It even
generalizes:

\begin{definition}\label{kway_independent_events}
  A set $A_1$, $A_2$, \dots, of events is \term{$k$-way independent}
  iff every set of $k$ of these events is mutually independent.  The
  set is \term{pairwise independent} iff it is 2-way independent.
\end{definition}

So the sets $A_1$, $A_2$, $A_3$ above are pairwise independent, but
not mutually independent.  Pairwise independence is a much weaker
property than mutual independence.

For example, suppose that the prosecutors in the O.~J. Simpson trial
were wrong and markers $A$, $B$, $C$, $D$, and $E$ appear only
\emph{pairwise} independently.  Then the probability that a
randomly-selected person has all five markers is no more than:
%
\begin{align*}
\pr{A \intersect B \intersect C \intersect D \intersect E}
    & \leq \pr{A \intersect E} \\[2pt]
    & = \pr{A} \cdot \pr{E} \\[2pt]
    & = \frac{1}{100} \cdot \frac{1}{170} \\[2pt]
    & = \frac{1}{17{,}000}.
\end{align*}
%
The first line uses the fact that $A \intersect B \intersect C \intersect
D \intersect E$ is a subset of $A \intersect E$.  (We picked out the $A$
and $E$ markers because they're the rarest.)  We use pairwise independence
on the second line.  Now the probability of a random match is 1 in
17,000---a far cry from 1 in 170 million!  And this is the strongest
conclusion we can reach assuming only pairwise independence.

On the other hand, the 1 in 17,000 bound that we get by assuming
pairwise independence is a lot better than the bound that we would
have if there were no independence at all.  For example, if the
markers are dependent, then it is possible that
\begin{quote}
everyone with marker~$E$ has marker~$A$,

everyone with marker~$A$ has marker~$B$,

everyone with marker~$B$ has marker~$C$, and

everyone with marker~$C$ has marker~$D$.
\end{quote}
In such a scenario, the probability of a match is
\begin{equation*}
    \pr{E} = 1/170.
\end{equation*}

So a stronger independence assumption leads to a smaller bound on the
probability of a match.  The trick is to figure out what independence
assumption is reasonable.  Assuming that the markers are
\emph{mutually} independent may well \emph{not} be reasonable unless
you have examined hundreds of millions of blood samples.  Otherwise,
how would you know that marker~$D$ does not show up more frequently
whenever the other four markers are simultaneously present?

We will conclude our discussion of independence with a useful, and
somewhat famous, example known as the Birthday Paradox.

\begin{problems}
\classproblems
\pinput{CP_three_fair_coins}
\end{problems}


\section{The Birthday Paradox}\label{birthday_principle_sec}

Suppose that there are 100 students in a class.  What is the
probability that some birthday is shared by two people?  Comparing 100
students to the 365 possible birthdays, you might guess the
probability lies somewhere around~$1/3$---but you'd be wrong: the
probability that there will be two people in the class with matching
birthdays is actually~$0.999999692\dots$.  In other words, the
probability that all 100 birthdays are different is less than 1
in~3,000,000.

Why is this probability so small?  The answer involves a phenomenon
known as the \term{Birthday Paradox} (or the \term{Birthday
  Principle}), which is surprisingly important in computer science, as
we'll see later.

Before delving into the analysis, we'll need to make some modeling
assumptions:
\begin{itemize}

\item
For each student, all possible birthdays are equally likely.  The idea
underlying this assumption is that each student's birthday is
determined by a random process involving parents, fate, and, um, some
issues that we discussed earlier in the context of graph theory.
The assumption is not completely accurate, however; a disproportionate
number of babies are born in August and September, for example.

\item
Birthdays are mutually independent.  This isn't perfectly accurate
either.  For example, if there are twins in the class, then their
birthdays are surely not independent.

\end{itemize}
We'll stick with these assumptions, despite their limitations.  Part
of the reason is to simplify the analysis.  But the bigger reason is
that our conclusions will apply to many situations in computer science
where twins, leap days, and romantic holidays are not considerations.
After all, whether or not two items collide in a hash table really has
nothing to do with human reproductive preferences.  Also, in pursuit
of generality, let's switch from specific numbers to variables.  Let
$m$~be the number of people in the room, and let $N$~be the number of
days in a year.

We can solve this problem using the standard four-step method.
However, a tree diagram will be of little value because the sample
space is so enormous.  This time we'll have to proceed without the
visual aid!

\paragraph{Step 1: Find the Sample Space}

Let's number the people in the room from 1 to~$m$.  An outcome of the
experiment is a sequence $(b_1, \dots, b_m)$ where $b_i$~is the
birthday of the $i$th person.  The sample space is the set of all such
sequences:
\begin{equation*}
    \sspace = \{\, (b_1, \dots, b_m) \suchthat b_i \in \set{1, \dots
      N} \,\}.
\end{equation*}

\paragraph{Step 2: Define Events of Interest}

Our goal is to determine the probability of the event~$A$ in which
some pair of people have the same birthday.  This event is a little
awkward to study directly, however.  So we'll use a common trick,
which is to analyze the \term{complementary} event~$\setcomp{A}$, in
which all $m$~people have different birthdays:
\begin{equation*}
    \setcomp{A} = \set{\, (b_1, \dots, b_m) \in \sspace
                    \suchthat \text{all $b_i$ are distinct} \,}.
\end{equation*}
If we can compute $\pr{\setcomp{A}}$, then we can compute what
really want, $\pr{A}$, using the identity
\begin{equation*}
    \pr{A} + \pr{\setcomp{A}} = 1.
\end{equation*}

\paragraph{Step 3: Assign Outcome Probabilities}

We need to compute the probability that $m$~people have a particular
combination of birthdays ~$(b_1, \dots, b_m)$.  There are $N$~possible
birthdays and all of them are equally likely for each student.
Therefore, the probability that the $i$th person was born on day~$b_i$
is~$1/N$.  Since we're assuming that birthdays are mutually
independent, we can multiply probabilities.  Therefore, the
probability that the first person was born on day~$b_1$, the second
on~$b_2$, and so forth is~$(1/N)^m$.  This is the probability of every
outcome in the sample space, which means that the sample space is
uniform.  That's good news, because, as we have seen, it means that
the analysis will be simpler.

\paragraph{Step 4: Compute Event Probabilities}

We're interested in the probability of the event~$\setcomp{A}$ in
which everyone has a different birthday:
\begin{equation*}
    \setcomp{A} = \set{\, (b_1, \dots, b_n) \suchthat
                            \text{all $b_i$ are distinct} \,}.
\end{equation*}
This is a gigantic set.  In fact, there are $N$~choices for~$b_i$,
\ $N - 1$ choices for~$b_2$, and so forth.  Therefore, by the
Generalized Product Rule,
\begin{equation*}
\card{\setcomp{A}}
    = \frac{N!}{(N - m)!}
    = N (N - 1) (N - 2) \cdots (N - m + 1).
\end{equation*}
Since the sample space is uniform, we can conclude that
\begin{equation}\label{eqn:15E4}
\pr{\bar{A}}
    = \frac{\card{\bar{A}}}{N^m} \\
    = \frac{N!}{N^m (N - m)!}.
\end{equation}
We're done!

Or are we?  While correct, it would certainly be nicer to have a
closed-form expression for Equation~\ref{eqn:15E4}.  That means
finding an approximation for $N!$ and~$(N - m)!$.  But this is what we
learned how to do in Section~\ref{sec:closed_products}.  In fact, since
$N$ and~$N - m$ are each at least~100, we know from
Corollary~\ref{cor:9A2} that
\begin{equation*}
    \stirling{N} \quad \text{and} \quad \stirling*{N - m}
\end{equation*}
are excellent approximations (accurate to within~.09\%) of~$N!$ and~$(N
- m)!$, respectively.  Plugging these values into
Equation~\ref{eqn:15E4} means that (to within~.2\%)\footnote{If there
are two terms that can be off by~.09\%, then the ratio can be off by
at most a factor of~$(1.0009)^2 < 1.002$.}
\begingroup
\openup2\jot
\begin{align}
\prob{\bar{A}}
    &= \frac{ \stirling{N} }{ N^m \stirling*{N - m} } \notag\\
    &= \sqrt{\frac{N}{N - m}}
             \frac{ e^{N \ln(N) - N} }
                  { e^{m \ln(N)} e^{(N - m) \ln(N - m) - (N - m) } }
                  \notag\\
    &= \sqrt{\frac{N}{N - m}}
         e^{ (N - m)\ln(N) - (N - m) \ln(N - m) - m } \notag\\
    &= \sqrt{\frac{N}{N - m}}
         e^{ (N - m)\ln\paren{\frac{N}{N - m}} - m } \notag\\
    &= e^{ \paren{N - m + \frac{1}{2}}\ln\paren{\frac{N}{N - m}} - m }.
        \label{eqn:15E9}
\end{align}
\endgroup
We can now evaluate Equation~\ref{eqn:15E9} for $m = 100$ and $N =
365$ to find that the probability that all 100 birthdays are different
is\footnote{The possible .2\%~error is so small that
  it is lost in the \dots after 3.07.}
\begin{equation*}
    3.07\ldots \cdot 10^{-7}.
\end{equation*}

We can also plug in other values of~$m$ to find the number of people
so that the probability of a matching birthday will be about~$1/2$.
In particular, for $m = 23$ and $N = 365$, Equation~\ref{eqn:15E9}
reveals that the probability that all the birthdays differ is
0.49\dots.  So if you are in a room with 23 other people, the
probability that some pair of people share a birthday will be a little
better than~$1/2$.  It is because 23 seems like such a small number of
people for a match that the phenomenon is called the \term{Birthday
  Paradox}.

\subsection{Applications to Hashing}

Hashing is frequently used in computer science to map large strings of
data into short strings of data.  In a typical scenario, you have a
set of $m$~items and you would like to assign each item to a number
from 1 to~$N$ where no pair of items is assigned to the same number
and $N$~is as small as possible.  For example, the items might be
messages, addresses, or variables.  The numbers might represent
storage locations, devices, indices, or digital signatures.

If two items are assigned to the same number, then a \term{collision}
is said to occur.  Collisions are generally bad.  For example,
collisions can correspond to two variables being stored in the same
place or two messages being assigned the same digital signature.  Just
imagine if you were doing electronic banking and your digital
signature for a \$10~check were the same as your signature for a
\$10~million dollar check.  In fact, finding collisions is a common
technique in breaking cryptographic codes.\footnote{Such techniques
  are often referred to as \term{birthday attacks} because of the
  association of such attacks with the Birthday Paradox.}

In practice, the assignment of a number to an item is done using a
hash function
\begin{equation*}
    h: S \to [1, N],
\end{equation*}
where $S$~is the set of items and $m = \card{S}$.  Typically, the
values of~$h(S)$ are assigned randomly and are assumed to be equally
likely in~$[1, N]$ and mutually independent.

For efficiency purposes, it is generally desirable to make~$N$ as
small as necessary to accommodate the hashing of $m$~items without
collisions.  Ideally, $N$~would be only a little larger than~$m$.
Unfortunately, this is not possible for random hash functions.  To see
why, let's take a closer look at Equation~\ref{eqn:15E9}.

By Theorem~\ref{thm:stirling} and the derivation of
Equation~\ref{eqn:15E9}, we know that the probability that there are
no collisions for a random hash function is
\begin{equation}\label{eqn:16K}
    \sim e^{ \paren{N - m + \frac{1}{2}} \ln\paren{\frac{N}{N - m}} - m }.
\end{equation}
For any~$m$, we now need to find a value of~$N$ for which this
expression is at least~1/2.  That will tell us how big the hash table
needs to be in order to have at least a 50\%~chance of avoiding
collisions.  This means that we need to find a value of~$N$ for which
\begin{equation}\label{eqn:16P}
    \paren{N - m + \frac{1}{2}} \ln\paren{\frac{N}{N - m}} - m 
        \sim
    \ln\paren{\frac{1}{2}}.
\end{equation}

To simplify Equation~\ref{eqn:16P}, we need to get rid of the
$\ln\paren{\frac{N}{N - m}}$~term.  We can do this by using the Taylor
Series expansion for
\begin{equation*}
    \ln(1 - x) = -x - \frac{x^2}{2} - \frac{x^3}{3} - \cdots
\end{equation*}
to find that\footnote{This may not look like a simplification, but
  stick with us here.}
\begin{align*}
\ln\paren{\frac{N}{N - m}}
    &= - \ln \paren{\frac{N - m}{N}} \\
    &= - \ln \paren{1 - \frac{m}{N}} \\
    &= - \paren{ -\frac{m}{N} - \frac{m^2}{2N^2} - \frac{m^3}{3N^3} - \cdots }\\
    &= \frac{m}{N} + \frac{m^2}{2N^2} + \frac{m^3}{3N^3} + \cdots.
\end{align*}
Hence,
\begin{align}
\paren{N - m + \frac{1}{2}} \ln\paren{\frac{N}{N - m}} - m
    &= \paren{N - m + \frac{1}{2}}
        \paren{\frac{m}{N} + \frac{m^2}{2N^2} + \frac{m^3}{3N^3} + \cdots}
        - m \notag\\
    &= \paren{ m + \frac{m^2}{2N} + \frac{m^3}{3N^2} + \cdots }
            \notag\\
    &\phantom{=}\qquad - \paren{ \frac{m^2}{N} + \frac{m^3}{2N^2} +
          \frac{m^4}{3N^3} + \cdots }
            \notag\\
    &\phantom{=}\qquad + \frac{1}{2} \paren{\frac{m}{N} +
          \frac{m^2}{2N^2} + \frac{m^3}{3N^3} + \cdots} -m 
            \notag\\
    &= - \paren{ \frac{m^2}{2N} + \frac{m^3}{6N^2} + \frac{m^4}{12N^3}
            + \cdots} \notag\\
    &\phantom{=}\qquad
        + \frac{1}{2}\paren{ \frac{m}{N} + \frac{m^2}{2N^2} +
          \frac{m^3}{3N^3} + \cdots }.
    \label{eqn:16Q}
\end{align}

If $N$~grows faster than~$m^2$, then the value in
Equation~\ref{eqn:16Q} tends to~0 and Equation~\ref{eqn:16P} cannot be
satisfied.  If $N$~grows more slowly than~$m^2$, then the value in
Equation~\ref{eqn:16Q} diverges to negative infinity, and, once again,
Equation~\ref{eqn:16P} cannot be satisfied.  This suggests that we
should focus on the case where~$N = \Theta(m^2)$, when
Equation~\ref{eqn:16Q} simplifies to
\begin{equation*}
    \sim \frac{-m^2}{2N}
\end{equation*}
and Equation~\ref{eqn:16P} becomes
\begin{equation}\label{eqn:16R}
    \frac{-m^2}{2N} \sim \ln\paren{\frac{1}{2}}.
\end{equation}

Equation~\ref{eqn:16R} is satisfied when 
\begin{equation}\label{eqn:16S}
    N \sim \frac{m^2}{2 \ln(2)}.
\end{equation}

In other words, $N$~needs to grow quadratically with~$m$ in order to
avoid collisions.  This unfortunate fact is known as the
\term{Birthday Principle} and it limits the efficiency of hashing in
practice---either $N$~is quadratic in the number of items being hashed
or you need to be able to deal with collisions.

\problemsection

\endinput
