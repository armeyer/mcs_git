\documentclass[12pt]{article}
\usepackage{light}

%\newcommand{\Var}{\mathop{\textup{Var}}\nolimits}
%\newcommand{\Cov}{\mathop{\textup{Cov}}\nolimits}
%\newcommand{\expect}[1]{\Exp\left[#1\right]}
%\newcommand{\expectsq}[1]{{\Ex}^2\left[#1\right]}
%\newcommand{\expcond}[2]{\text{E}\{#1\mid#2\}}
%\newcommand{\variance}[1]{\Var\left[#1\right]}
%\newcommand{\varsq}[1]{{\Var}^2\left[#1\right]}
%\newcommand{\covar}[1]{\Cov\left[#1\right]}
%\newcommand{\covariance}[2]{\Cov\left[#1,#2\right]}

\hidesolutions
\showsolutions

\newcommand{\pdf}{\textsc{PDF}}
\newcommand{\cdf}{\textsc{CDF}}

\begin{document}

\recitation{22}{December 9, 2016}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\insolutions{
\section{Expected Value Rule for Functions of Random Variables}

In lecture, we have computed the expectation of a function of a random
variable without explicitly discussing the general rule. For example,
yesterday we saw that the expectation of the square of the roll of a
die is not equal to the square of the expectation of the roll. That
is, if $R$ is the outcome of a single roll of a die. Then $\ex{R^2}
\neq \ex{R}^2$. We will now explicitly present the rule for computing
the expection of a function of a random variable $R$.

\begin{mathrule*}[Expected Value for the Function of a Random Variable]\label{exfuncrv}
Let $R$ be a random variable, and let $f(R)$ be a function of
$R$. Then, the expected value of the random variable $f(R)$ is given
by
%
\[
\expect{f(R)} = \sum_{x\in Range(R)} f(x)\cdot \pr{R=x}
\]
\end{mathrule*}
}

\section{Properties of Variance}
In this problem we will study some properties of the variance and the
standard deviation of random variables.

\begin{itemize}

\item[a.] Show that for any random variable $R$, $\variance{R} = \expect{R^2} - \expectsq{R}$. 

\solution{Let $\mu = \expect{R}$.  Then
\begin{align*}
\variance{R} & =   \expect{(R - \expect{R})^2}
               & \text{(Definition of variance)}\\
        & = \expect{(R - \mu)^2} & \text{(def. of $\mu$)}\\
        & = \expect{R^2 - 2  \mu R + \mu^2} \\
        & = \expect{R^2} - 2 \mu \expect{R} + \mu^2 
                & \text{(linearity of expectation)}\\
        & = \expect{R^2} - 2 \mu^2 + \mu^2
              &  \text{(def. of $\mu$)}\\
        & = \expect{R^2} - \mu^2\\
        & = \expect{R^2} - \expectsq{R}.
                  &  \text{(def. of $\mu$)}
\end{align*}}

\item[b.] Show that for any random variable $R$ and constants $a$ and $b$, $\variance{a R + b} = a^2 \variance{R}.$ Conclude that the standard deviation of $aR + b$ is $a$ times the standard deviation of $R$.

\solution{We will transform the left side into the right side.  The
first step is to expand $\variance{a R + b}$ using the alternate
definition of variance.
\[
\variance{a R + b} =
\expect{(a R + b)^2} - \expectsq{a R + b}.
\]
We will work on the first term and then the second term.  For the first
term, note that by linearity of expectation,
\begin{equation}\label{R2}
\expect{(aR+b)^2} = \expect{a^2 R^2 + 2 a b R + b^2} =
a^2 \expect{R^2} + 2 a b \expect{R} + b^2.
\end{equation}
Similarly for the second term:
\begin{equation}\label{2R}
\expectsq{aR+b} = (a \expect{R} + b)^2 = a^2 \expectsq{R} + 2 a b
\expect{R} + b^2.
\end{equation}

Finally, we subtract the expanded second term from the first.
\begin{align*}
\variance{a R + b} & = \expect{(a R + b)^2} - \expectsq{a R + b}
      & \text{(previous part)}\\
  & = a^2 \expect{R^2} + 2 a b \expect{R} + b^2 -\\
  &\quad (a^2 \expectsq{R} + 2 a b \expect{R} + b^2)
     & \text{(by~(\ref{R2}) and~(\ref{2R}))}\\
  & = a^2 \expect{R^2} - a^2 \expectsq{R}\\
  & = a^2 (\expect{R^2} - \expectsq{R})\\
  & = a^2 \variance{R} 
     & \text{(previous part)}
\end{align*}
Since the standard deviation of a random variable is the square root of the variance, the standard deviation of $aR+b$ is $\sqrt{a^2 \variance{R}}$ which is just $a$ times the standard deviation of $R$.
}

\item[c.] Show that if $R_1$ and $R_2$ are independent random variables, then
\[
\variance{R_1 + R_2} = \variance{R_1} + \variance{R_2}.
\]

\solution{
We will transform the left side into the right side.  We begin by
applying the alternate definition of variance.
\[
\variance{R_1 + R_2} = \expect{(R_1 + R_2)^2} - \expectsq{R_1 + R_2}.
\]

We will work on the first term and then the second term separately.
For the first term, note\begin{eqnarray*}
\expect{(R_1+R_2)^2}
& = &   \expect{R_1^2 + 2 R_1 R_2 + R_2^2} \\
& = &   \expect{R_1^2} + \expect{2 R_1 R_2} + \expect{R_2^2} \\
& = &   \expect{R_1^2} + 2 \expect{R_1} \expect{R_2} + \expect{R_2^2}.
\end{eqnarray*}
First, we multiply out the squared expression.  The second step uses
linearity of expectation.  In the last step, we break the
expectation of the product $R_1 R_2$ into a product of expectations;
this is where we use the fact that $R_1$ and $R_2$ are independent.
Now we work on the second term.
\begin{eqnarray*}
\expectsq{R_1+R_2} & = & (\expect{R_1} + \expect{R_2})^2 \\
& = & \expectsq{R_1} + 2 \expect{R_1} \expect{R_2} + \expectsq{R_2}.
\end{eqnarray*}
The first step uses linearity of expectation, and in the second step
we multiply out the squared expression.  Now we subtract the
(expanded) second term from the first. Cancelling and rearranging
terms, we find that
\begin{eqnarray*}
\variance{R_1 + R_2} & = &   (\expect{R_1^2} - \expectsq{R_1}) +
(\expect{R_2^2}) - \expectsq{R_2}) \\
& = &   \variance{R_1} + \variance{R_2}.
\end{eqnarray*}
}

\item[d.] Give an example of random variables $R_1$ and $R_2$ for which 
\[
\variance{R_1 + R_2} \neq \variance{R_1} + \variance{R_2}.
\]

\solution{Suppose $R = R_1 = R_2$. If linearity of variance held, then
$\variance{R + R} = \variance{R} + \variance{R}$. However, by part b, 
$\variance{R+R} = \variance{2R} = 4\variance{R}$. This is only possible
if $\variance{R} = 0$. If, say, we choose $R$ to be the outcome of a fair coin
flip, $\variance{R} \neq 0$. In fact, any $R$ which holds at least $2$ distinct values each with positive probability will do.
}

\item[e.] Compute the variance and standard deviation of the Binomial distribution $H_{n,p}$ with parameters $n$ and $p$.

\solution{We know that $H_{n,p} = \sum_{k=1}^n I_k$ where the $I_k$ are mutually
independent 0-1-valued variables with $\pr{I_k=1}=p$. The variance of $I_k$ is $\expect{I_k^2} - \expect{I_k}^2 = \expect{I_k} - \expect{I_k}^2 = \expect{I_k}(1-\expect{I_k}) = p(1-p)$. Thus, by linearity of variance, we have
$\variance{H_{n,p}} = n \variance{I_k} = np(1-p).$ Thus, the standard deviation of $H_{n,p}$ is $\sqrt{np(1-p)}$.}

\item[f.] Let's say we have a random variable $T$ such that
  $T=\sum_{j=1}^n T_j$, where all of the $T_j$'s are mutually
  independent and take values in the range $[0,1]$. Prove that
  Var(T)$\leq$Ex(T). We'll use this result in lecture tomorrow. {\it
    Hint: Upper bound $\variance{T_j}$ with $\expect{T_j}$ using the
    definition of variance in part (a) and the rule for computing the
    expectation of a function of a random variable.}

\solution{

We know by linearity of variance for mutually-independent random
variables that

\begin{align*}
\variance{T} 
& = \variance{T_1+\ldots+T_n}\\
& = \variance{T_1} + \ldots + \variance{T_n}\\
\end{align*}

Now we evaluate the variance of $T_j$ for any $j$. Using the
definition of variance from part (a) above, we have

\[
\variance{T_j} = \expect{T_j^2} - \expect{T_j}^2 
\]

By the rule for computing the expectation of a function of a random
variable, we also know that

\[
\expect{T_j^2} = \sum_{x\in Range(T_j)} x^2\pr{T_j=x}
\]

Now we can use the fact that $T_j$ is in the range $[0,1]$ to say that
$x^2 \leq x$, and that therefore

\[
\sum_{x\in Range(T_j)} x^2\cdot \pr{T_j=x} \leq \sum_{x\in Range(T_j)} x\cdot\pr{T_j=x}
\]

Thus, $\expect{T_j^2}\leq\expect{T_j}$ and

\begin{align*}
\variance{T_j}
& \leq \expect{T_j} - \expect{T_j}^2\\
& \leq \expect{T_j}\\
\end{align*}

Since this holds for any $j$, we can now conclude that

\begin{align*}
\variance{T}
& = \variance{T_1} + \ldots + \variance{T_n}\\
& \leq \expect{T_1} + \ldots + \expect{T_n}\\
& = \expect{T}\\
\end{align*}
}

\end{itemize}
\newpage
\section{Gambler's Ruin}
  A gambler is placing \$1 bets on the ``1st dozen'' in roulette.
  This bet wins when a number from one to twelve comes in, and then
  the gambler gets his \$1 back plus \$2 more.  Recall that there are
  38 numbers on the roulette wheel.

  The gambler's initial stake in $\$n$ and his target is $\$T$.  He
  will keep betting until he runs out of money (``goes broke'') or
  reaches his target.  Let $w_n$ be the probability of the gambler
  winning, that is, reaching target $\$T$ before going broke.

\begin{itemize}

  \item[a.] Write a linear recurrence with boundary conditions for
  $w_n$.  You need \emph{not} solve the recurrence.

  \solution{
    The probability of winning a bet is $12/38$.  Thus, by the Law
    of Total Probability,
    \begin{align*}
      w_n & = \text{Pr}\{\text{win with $\$n$ start}\mid\text{won 1st bet}\}
                     \cdot \pr{\text{won 1st bet}} \\
         & \quad + \text{Pr}\{\text{win with $\$n$ start}\mid\text{lost 1st bet}\}
                     \cdot\pr{\text{lost 1st bet}}\\
         & = \pr{\text{win with $\$n+2$ start}}
                     \cdot \pr{\text{won 1st bet}}\\
         & \quad +\pr{\text{win with $\$n-1$ start}}
                     \cdot \pr{\text{lost 1st bet}},
     \end{align*}
    so
    \[
    w_{n} = \frac{12}{38}w_{n+2} + \frac{26}{38}w_{n-1}.
    \]

    Letting $m \eqdef n+2$ we get
    \[
    w_{m} = \frac{38}{12}w_{m-2} - \frac{26}{12}w_{m-3}.
    \]

    As boundary conditions, we have
    \[
    w_0 = 0, w_T = 1, w_{T+1} =1.
    \]
    Notice that for this degree 3 recurrence, three boundary values
    are necessary to determine the $w_n$ uniquely.
}

\item[b.] Let $e_n$ be the expected number of bets until the game
ends.  Write a linear recurrence with boundary conditions for
$e_n$.   You need \emph{not} solve the recurrence.

\solution{
By the Law of Total Expectation, Theorem,
\begin{align*}
  e_n &= \paren{1+\expcond{\text{\#bets with $\$n$ start}}{\text{won 1st bet}}}
       \cdot \pr{\text{won 1st bet}}\\
      & \quad + \paren{1+\expcond{\text{\#bets with $\$n$\ start}}{\text{lost 1st bet}}}
        \cdot \pr{\text{lost 1st  bet}}\\
      & = \paren{1+\expect{\text{\#bets with $\$n+2$ start}}}
        \cdot \pr{\text{won 1st bet}}\\
      & \quad + \paren{1+\expect{\text{number of bets starting with $\$n-1$}}}
        \cdot \pr{\text{lost 1st bet}},
\end{align*}
so
\[
e_n = \paren{e_{n+2} +1}\frac{12}{38} +
\paren{1+e_{n-1}}\frac{26}{38}
\]
Letting $m = n+2$ we get
\[
e_m = \frac{38}{12}e_{m-2} - \frac{26}{12} e_{m-3} - \frac{38}{12}
\]

As boundary conditions, we have
\[
e_0 = e_T = e_{T+1} = 0.
\]
Again, for this degree 3 recurrence three boundary values
    are necessary to determine the $e_n$ uniquely.
}

  \end{itemize}

\end{document}
