\chapter{First-Order Logic}

\newcommand{\solves}{\text{Solves}}
\newcommand{\probs}{\text{Probs}}
\newcommand{\even}{\text{Evens}}
\newcommand{\primes}{\text{Primes}}

\section{Quantifiers}

There are a couple of assertions commonly made about a predicate: that it
is \textit{sometimes} true and that it is \textit{always} true.  For
example, the predicate
%
\[
\text{``$x^2 \geq 0$''}
\]
%
is always true when $x$ is a real number.  On the other hand, the
predicate
%
\[
\text{``$5x^2 - 7 = 0$''}
\]
%
is only sometimes true; specifically, when $x = \pm \sqrt{7/5}$.

There are several ways to express the notions of ``always true'' and
``sometimes true'' in English.  The table below gives some general
formats on the left and specific examples using those formats on the
right.  You can expect to see such phrases hundreds of times in
mathematical writing!
%
\begin{center}
\begin{tabular}{ll}
\multicolumn{2}{c}{\textbf{Always True}} \\[1ex]
For all $n$, $P(n)$ is true. & For all $x$, $x^2 \geq 0$. \\
$P(n)$ is true for every $n$. & $x^2 \geq 0$ for every $x$. \\[2ex]
\multicolumn{2}{c}{\textbf{Sometimes True}} \\[1ex]
There exists an $n$ such that $P(n)$ is true. & There exists an $x$ such that $5x^2 - 7 = 0$.\\
$P(n)$ is true for some $n$. & $5x^2 - 7 = 0$ for some $x$.\\
$P(n)$ is true for at least one $n$. & $5x^2-7=0$ for at least one $x$.
\end{tabular}
\end{center}

All these sentences quantify how often the predicate is true.
Specifically, an assertion that a predicate is always true is called a
\term{universal} quantification, and an assertion that a predicate is
sometimes true is an \term{existential} quantification.  Sometimes the
English sentences are unclear with respect to quantification:
%
\begin{center}
  ``If you can solve any problem we come up with, then you get an \emph{A}
  for the course.''
\end{center}
%
The phrase ``you can solve any problem we can come up with'' could
reasonably be interpreted as either a universal or existential
quantification:
%
\begin{quote}
``you can solve \textit{every} problem we come up with,''
\end{quote}
or maybe
\begin{quote}
``you can solve \textit{at least one} problem we come up with.''
\end{quote}
%
In any case, notice that this quantified phrase appears inside a
larger if-then statement.  This is quite normal; quantified statements
are themselves propositions and can be combined with and, or, implies,
etc., just like any other proposition.

\subsection{More Cryptic Notation}

There are symbols to represent universal and existential
quantification, just as there are symbols for ``and'' ($\wedge$),
``implies'' ($\implies$), and so forth.  In particular, to say that a
predicate, $P$, is true for all values of $x$ in some set, $D$, one
writes:
%
\[
\forall x \in D.\; P(x)
\]
%
The symbol $\forall$ is read ``for all'', so this whole expression is
read ``for all $x$ in $D$, $P(x)$ is true''.  To say that a predicate
$P(x)$ is true for at least one value of $x$ in $D$, one writes:
%
\[
\exists x \in D.\; P(x)
\]
%
The backward-E is read ``there exists''.  So this expression would be
read, ``There exists an $x$ in $D$ such that $P(x)$ is true.''  The
symbols $\forall$ and $\exists$ are always followed by a variable
---usually with an indication of the set the variable ranges over ---and
then a predicate, as in the two examples above.

As an example, let $\probs$ be the set of problems we come up with,
$\solves(x)$ be the predicate ``You can solve problem $x$'', and $G$ be
the proposition, ``You get an \emph{A} for the course.''  Then the two
different interpretations of
%
\begin{quote}
``If you can solve any problem we come up with, then you get an \emph{A} for the course.''
\end{quote}
%
can be written as follows:
%
\[
(\forall x \in \probs.\; \solves(x)) \QIMP G,
\]
or maybe
\[
(\exists x \in \probs.\; \solves(x)) \QIMP G.
\]

\subsection{Mixing Quantifiers}

Many mathematical statements involve several quantifiers.  For
example, Goldbach's Conjecture states:
%
\begin{center}
``Every even integer greater than 2 is the sum of two primes.''
\end{center}
%
Let's write this more verbosely to make the use of quantification
clearer:
%
\begin{quote}
For every even integer $n$ greater than 2,
there exist primes $p$ and $q$ such that $n = p + q$.
\end{quote}
%
Let $\even$ be the set of even integers greater than 2, and let $\primes$ be the
set of primes.  Then we can write Goldbach's Conjecture in logic
notation as follows:
%
\[
\underbrace{\forall n \in \even}_{\substack
    {\text{for every even} \\
     \text{integer $n > 2$}}}
\
\underbrace{\exists p \in \primes\ \exists q \in \primes.}_{\substack
    {\text{there exist primes} \\
     \text{$p$ and $q$ such that}}}
\ n = p + q.
\]

\subsection{Order of Quantifiers}

Swapping the order of different kinds of quantifiers (existential or
universal) usually changes the meaning of a proposition.  For example,
let's return to one of our initial, confusing statements:
\begin{center}
``Every American has a dream.''
\end{center}

This sentence is ambiguous because the order of quantifiers is
unclear.  Let $A$ be the set of Americans, let $D$ be the set of
dreams, and define the predicate $H(a, d)$ to be ``American $a$ has
dream $d$.''.  Now the sentence could mean there is a single dream
that every American shares:
\[
\exists\, d \in D\; \forall a \in A.\; H(a, d)
\]
For example, it might be that every American shares the dream of owning
their own home.

Or it could mean that every American has a personal dream:
\[
\forall a \in A\; \exists\, d \in D.\; H(a, d)
\]
For example, some Americans may dream of a peaceful retirement, while
others dream of continuing practicing their profession as long as they
live, and still others may dream of being so rich they needn't think at
all about work.

Swapping quantifiers in Goldbach's Conjecture creates a patently false
statement that every even number $\geq 2$ is the sum of \emph{the same}
two primes:
\[
\underbrace{\exists\, p \in \primes\ \exists\, q \in \primes}_{\substack
    {\text{there exist primes} \\
     \text{$p$ and $q$ such that}}}
\
\underbrace{\forall n \in \even.}_{\substack
    {\text{for every even} \\
     \text{integer $n > 2$}}}
\ n = p + q.
\]

\subsubsection{Variables over One Domain}
When all the variables in a formula are understood to take values from the
same nonempty set, $D$, it's conventional to omit mention of $D$.  For
example, instead of $\forall x \in D\; \exists y \in D.\; Q(x,y)$ we'd write
$\forall x \exists y.\; Q(x,y)$.  The unnamed nonempty set that $x$ and
$y$ range over is called the \term{domain} of the formula.

It's easy to arrange for all the variables to range over one domain.  For
example, Goldbach's Conjecture could be expressed with all variables
ranging over the domain $\naturals$ as
\[
\forall n.\; n \in \even \QIMP (\exists\, p \exists\, q.\; p \in \primes \land
q \in \primes \land n = p + q).
\]

\subsection{Negating Quantifiers}

There is a simple relationship between the two kinds of quantifiers.  The
following two sentences mean the same thing:
%
\begin{quote}

It is not the case that everyone likes to snowboard.

There exists someone who does not like to snowboard.

\end{quote}
%
In terms of logic notation, this follows from a general property of
predicate formulas:
%
\[
\QNOT \forall x.\; P(x)
\hspace{0.1in} \text{is equivalent to} \hspace{0.1in}
\exists x.\; \QNOT P(x).
\]
%
Similarly, these sentences mean the same thing:
%
\begin{quote}
There does not exist anyone who likes skiing over magma.

Everyone dislikes skiing over magma.
\end{quote}
%
We can express the equivalence in logic notation this way:
%
\begin{equation}\label{nE}
(\QNOT \exists x.\; P(x))  \QIFF  \forall x.\; \QNOT P(x).
\end{equation}
%
The general principle is that \textit{moving a ``not'' across a
quantifier changes the kind of quantifier.}

\iffalse Logicians have worked very hard to define strict rules for the
use of logic notation so that ideas can be expressed with absolute rigor.
It's all quite charming and clever.  However, the sad irony is that
applied mathematicans usually use their beloved notation as a crude
shorthand, breaking the rules and abusing the notation willy-nilly ---sort
of like pounding nails with fine china.  \fi

\subsection{Validity}

A propositional formula is called \term{valid} when it evaluates to \true\
no matter what truth values are assigned to the individual propositional
variables.  For example, the propositional version of the Distributive Law
is that $P \QAND (Q \QOR R)$ is equivalent to $(P \QAND Q) \QOR (P \QAND
R)$.  This is the same as saying that
\[
[P \QAND (Q \QOR R)] \QIFF [(P \QAND Q) \QOR (P \QAND R)]
\]
is valid.

The same idea extends to predicate formulas, but to be valid, a
formula now must evaluate to true no matter what values its variables
may take over any unspecified domain, and no matter what
interpretation a predicate variable may be given.  For example, we
already observed that the rule for negating a quantifier is captured
by the valid assertion~\eqref{nE}.

Another useful example of a valid assertion is
\[
\exists x \forall y.\; P(x,y) \QIMP \forall y \exists x.\; P(x,y).
\]
We could prove this as follows:
\begin{proof}
Let $D$ be the domain for the variables and $P_0$ be some
binary predicate\footnote{That is, a predicate that depends on two variables.}
on $D$.  We need to show that if $\exists x \in D\; \forall y \in D.\;
P_0(x,y)$ holds under this interpretation, then so does $\forall y \in D\;
\exists x \in D.\; P_0(x,y)$.

So suppose $\exists x \in D\; \forall y \in D.\; P_0(x,y)$.  Then some
element $x_0 \in D$ has the property that $P_0(x_0, y)$ is true for all $y
\in D$.  So for every $y \in D$, there is some $x \in D$, namely $x_0$,
such that $P_0(x,y)$ is true.  That is, $\forall y \in D\exists x \in D.\;
P_0(x,y)$ holds; that is, $\forall y\; \exists x.\; P(x,y)$ holds under this
interpretation, as required.
\end{proof}

On the other hand,
\[
\forall y \exists x.\; P(x,y) \QIMP \exists x \forall y.\; P(x,y).
\]
is \emph{not} valid.  We can prove this simply by describing an
interpretation where the hypothesis, $\forall y \exists x.\; P(x,y)$, is
true but the conclusion, $\exists x \forall y.\; P(x,y)$, is not true.
For example, let the domain be the integers and $P(x,y)$ mean $x > y$.
Then the hypothesis would be true because, given a value, $n$, for $y$ we
could choose the value of $x$ to be $n+1$, for example.  But under this
interpretation the conclusion asserts that there is an integer that is
bigger than all integers, which is certainly false.  An interpretation like
this which falsifies an assertion is called a \emph{counter model} to the
assertion.

\begin{problems}
%\practiceproblems
%\pinput{}
\classproblems
\pinput{CP_logic_news_network}
\pinput{CP_assertions_about_binary_strings}
\pinput{CP_domain_of_discourse}
\pinput{CP_valid_vs_satisfiable}
\pinput{CP_counter_model}
\homeworkproblems
\pinput{PS_express_in_predicate_form}
\pinput{PS_emailed_exactly_2_others}
\end{problems}

\section{The Logic of Sets}

\subsection{Russell's Paradox}

Reasoning naively about sets turns out to be risky.  In fact, one of the
earliest attempts to come up with precise axiom for sets by a late
nineteenth century logican named Gotlob Frege was shot down by a three
line argument known as \emph{Russell's Paradox}:\footnote{Bertrand Russell
  was a Mathematician/Logician at Cambridge University at the turn of the
  Twentieth Century.  He reported that when he felt too old to do
  Mathematics, he began to study and write about Philosophy, and when he
  was no longer smart enough to do Philosophy, he began writing about
  Politics.  He was jailed as a conscientious objector during World War I.
  For his extensive philosophical and political writing, he won a Nobel
  Prize for Literature.}  This was an astonishing blow to efforts to
provide an axiomatic foundation for Mathematics.

\textbox{
\begin{quote}
Let $S$ be a variable ranging over all sets, and define
\[W \eqdef \set{S \suchthat S \not\in S}.\]
So by definition,
\[S \in W  \mbox{  iff  } S \not\in S,\]
for every set $S$.  In particular, we can let $S$ be $W$, and obtain
the contradictory result that
\[W \in W  \mbox{  iff  } W \not\in W.\]
\end{quote}}

A way out of the paradox was clear to Russell and others at the time:
\emph{it's unjustified to assume that $W$ is a set}.  So the step in the
proof where we let $S$ be $W$ has no justification, because $S$ ranges
over sets, and $W$ may not be a set.  In fact, the paradox implies that
$W$ had better not be a set!

But denying that $W$ is a set means we must \emph{reject} the very natural
axiom that every mathematically well-defined collection of elements is
actually a set.  So the problem faced by Frege, Russell and their
colleagues was how to specify \emph{which} well-defined collections are
sets.  Russell and his fellow Cambridge University colleague Whitehead
immediately went to work on this problem.  They spent a dozen years
developing a huge new axiom system in an even huger monograph called
\emph{Principia Mathematica}.


\subsection{The ZFC Axioms for Sets}
It's generally agreed that, using some simple logical deduction rules,
essentially all of Mathematics can be derived from some axioms about sets
called the Axioms of Zermelo-Frankel Set Theory with Choice (ZFC).

We're \textit{not} going to be working with these axioms in this course,
but we thought you might like to see them --and while you're at it, get
some practice reading quantified formulas:
%

\begin{description}

\item[Extensionality.] Two sets are equal if they have the same members.
In formal logical notation, this would be stated as:
\[
(\forall z.\; (z \in x \QIFF z \in y)) \QIMPLIES x = y.
\]


\item[Pairing.] For any two sets $x$ and $y$, there is a set,
     $\set{x,y}$, with $x$ and $y$ as its only elements:
\[
\forall x,y.\; \exists u.\; \forall z.\;
[z \in u \QIFF (z = x \QOR z = y)]
\]

\item[Union.] The union of a collection of sets is also a set:
\[
\exists u \forall x.\; (\exists y.\; x \in y \QAND y \in z) \QIFF x \in u.
\]

\item[Infinity.]  There is an infinite set.  Specifically, there is a
  nonempty set, $x$, such that for any set $y \in x$, the set $\set{y}$ is
  also a member of $x$.

\item[Subset.] Given any set, $x$, and any proposition $P(y)$, there is a
  set containing precisely those elements $y \in x$ for which $P(y)$ holds.

\item[Power Set.]  All the subsets of a set form another set:
\[
\forall x.\; \exists p.\; \forall u.\: u \subseteq x \QIFF u \in p.
\]

\item[Replacement.]  The image of a set under a function is a set:
\[
\forall w \exists y \forall z (\phi(w,z) \QIMPLIES z = y)
        \QIMPLIES \exists y \forall z (
            z \in y \QIFF \exists w (w \in x \QAND \phi(w,z)))
\]

\item[Foundation.] 
There cannot be an infinite sequence
\[
\cdots \in x_n \in \cdots \in x_1 \in x_0
\]
of sets each of which is a member of the previous one.

\iffalse  %USE FOR WELL-FOUNDED POSETS
For every non-empty set, $x$, there is a set $y \in x$
  such that $x$ and $y$ have no elements in common.  
\fi

\item[Choice.]  We can choose one element from each set in a collection of
  nonempty sets.  More precisely, if $c$ is a set, and every element
  of $c$ is itself a set that is nonempty, then there is a ``choice''
  function, $g$, such that $g(y) \in y$ for every $y \in c$:
\[\begin{array}{rlll}
\exists y \forall z \forall w & ( (z \in w \QAND w \in x) \QIMPLIES\\
                              &\quad \exists v \exists u (\exists t
                                           ((u \in w \QAND & w \in t)
                                                              & \QAND (u \in t \QAND t \in y))\\
                                                            &&& \QIFF u = v))
\end{array}\]

\end{description}


\subsection{Avoiding Russell's Paradox}

These modern ZFC axioms for set theory are much simpler than the system
Russell and Whitehead first came up with to avoid paradox.  In fact, the
ZFC axioms are as simple and intuitive as Frege's original axioms, with
one technical addition: the Foundation axiom.  Foundation captures the
intuitive idea that sets must be built up from ``simpler'' sets in certain
standard ways.  And in particular, Foundation implies that no set is ever
a member of itself.  So the modern resolution of Russell's paradox goes as
follows: since $S \not \in S$ for all sets $S$, it follows that $W$,
defined above, contains every set.  This means $W$ can't be a set ---or it
would be a member of itself.

\subsection{Power sets are strictly bigger}

It turns out that the ideas behind Russell's Paradox, which caused so much
trouble for the early efforts to formulate Set Theory, lead to a correct
and astonishing fact about infinite sets: they are \emph{not all the same
  size}.

In particular,
\begin{theorem}\label{powbig}
For any set, $A$, the power set, $\power(A)$, is strictly bigger than $A$.
\end{theorem}
\begin{proof}
  First of all, $\power(A)$ is as big as $A$: for example, the partial
  function $f:\power(A) \to A$, where $f(\set{a}) \eqdef a$ for $a \in A$
  and $f$ is only defined on one-element sets, is a surjection.

  To show that $\power(A)$ is strictly bigger than $A$, we have to show
  that if $g$ is a function from $A$ to $\power(A)$, then $g$ is not a
  surjection.  So, mimicking Russell's Paradox, define
  \[
  A_g \eqdef \set{a \in A \suchthat a \notin g(a)}.
  \]
  Now $A_g$ is a well-defined subset of $A$, which means it is a member of
  $\power(A)$.  But $A_g$ can't be in the range of $g$, because if it
  were, we would have
\[
A_g = g(a_0)
\]
for some $a_0 \in A$, so by definition of $A_g$,
\[
a \in g(a_0) \qiff a \in A_g \qiff a \notin g(a)
\]
for all $a \in A$.  Now letting $a = a_0$ yields the contradiction
\[
a_0 \in g(a_0) \qiff a_0 \notin g(a_0).
\]
So $g$ is not a surjection, because there is an element in the power set
of $A$, namely the set $A_g$, that is not in the range of $g$.
\end{proof}

\subsubsection{Larger Infinities}

There are lots of different sizes of infinite sets.  For example, starting
with the infinite set, $\naturals$, of nonnegative integers, we can build
the infinite sequence of sets
\[
\naturals,\ \power(\naturals),\ \power(\power(\naturals)),\
\power(\power(\power(\naturals))),\ \dots.
\]
By Theorem~\ref{powbig}, each of these sets is strictly bigger than all
the preceding ones.  But that's not all: the union of all the sets in the
sequence is strictly bigger than each set in the sequence.  In this way
you can keep going, building still bigger infinities.

\begin{notesproblem}{}
  Prove that the union of this sequence of sets is strictly bigger than
  each of the sets in the sequence.
\end{notesproblem}

So there is an endless variety of different size infinities.

\subsection{Does All This Really Work?}

So this is where mainstream mathematics stands today: there is a handful
of ZFC axioms from which virtually everything else in mathematics can be
logically derived.  This sounds like a rosy situation, but there are
several dark clouds, suggesting that the essence of truth in mathematics
is not completely resolved.

%
\begin{itemize}

\item The ZFC axioms weren't etched in stone by God.  Instead, they were
  mostly made up by some guy named Zermelo.  Probably some days he forgot
  his house keys.

  So maybe Zermelo, just like Frege, didn't get his axioms right and will
  be shot by some successor to Russell who will use his axioms to prove a
  proposition $P$ and its negation $\QNOT P$.  Then Math would be broken.
  This sounds crazy, but after all, it has happened before.

  In fact, while there is broad agreement that the ZFC axioms are capable
  of proving all of standard Mathematics, the axioms have some further
  consequences that sound paradoxical.  For example, the Banach-Tarski
  Theorem says that, as a consequence of the Axiom of Choice, a solid ball
  can be divided into six pieces and then the pieces can be rigidly
  rearranged to give \textit{two} solid balls, each the same size as the
  original!

\item Georg Cantor was a contemporary for Frege and Russell who first
  developed the theory of infinite sizes (because he thought he needed it
  in his study of Fourier series).  Cantor raised the question whether
  there is a set whose size is strictly between $\naturals$ and
  $\power(\naturals)$; he guessed not:

\textbf{Cantor's Continuum Hypothesis}: There is no set, $A$, such that
$\power(\naturals)$ is strictly bigger than $A$ and $A$ is strictly bigger
than $\naturals$.

The Continuum Hypothesis remains an open problem a century later.  Its
difficulty arises from one of the deepest results in modern Set Theory
---discovered in part by G\"odel in the 1930's and Paul Cohen in the
1960's ---namely, the ZFC axioms are not sufficient to settle the
Continuum Hypothesis: there are two collections of sets, each obeying the
laws of ZFC, and in one collection the Continuum Hypothesis is true, and
in the other it is false.  So settling the Continuum Hypothesis requires a
new understanding of what Sets should be to arrive at persuasive new
axioms that extend ZFC and are strong enough to determine the truth of the
Continuum Hypothesis one way or the other.

\item But even if we use more or diiferent axioms about sets, there are
  some unavoidable problems.  In the 1930's, G\"{o}del proved that,
  assuming that an axiom system like ZFC is consistent ---meaning you
  can't prove both $P$ and $\QNOT P$ for any proposition, $P$ ---then the
  very proposition that the system is consistent (which is not too hard to
  express as a logical formula) cannot be proved in the system.  In other
  words, no consistent system is strong enough to verify itself.
  
\end{itemize}

\subsection{Large Infinities in Computer Science}

If the romance of different size infinities and continuum hypotheses
doesn't appeal to you, not knowing about them is not going to lower your
professional abilities as a Computer Scientist.  These abstract issues
about infinite sets rarely come up in mainstream Mathematics, and they
don't come up at all in Computer Science, where the focus is generally on
``countable,'' and often just finite, sets.  In practice, only Logicians
and Set Theorists have to worry about collections that are too big to be
sets.  In fact, at the end of the 19th century, the general Mathematical
community doubted the relevance of what they called ``Cantor's paradise''
of unfamiliar sets of arbitrary infinite size.

But the proof that power sets are bigger gives the simplest form of what
is known as a ``diagonal argument.''  Diagonal arguments are used to prove
many fundamental results about the limitations of computation, such as the
undecidability of the Halting Problem for programs
\iffalse
%INSERT BACK IF PS2 USES THIS PROBLEM
(a variation of which is given in
\href{http://courses.csail.mit.edu/6.042/spring09/ps2.pdf#unrecognizable.set}
{Pset 2, prob 5})
\fi
and the inherent, unavoidable, inefficiency (exponential
time or worse) of procedures for other computational problems.  So
Computer Scientists do need to study diagonal arguments in order to
understand the logical limits of computation.

\begin{problems}
%\practiceproblems
%\pinput{}
\classproblems
\pinput{CP_undescribable_language}
\homeworkproblems
\pinput{PS_Russells_and_cardinality}
\end{problems}

\newpage
\section{Glossary of Symbols}
\begin{center}
\begin{tabular}{ll}
symbol &  meaning\\
\hline
$\eqdef$ & is defined to be\\
$\land$ & and\\
$\lor$ & or\\
$\implies$ & implies\\
$\neg$    & not\\
$\neg{P}$ & not $P$\\
$\bar{P}$ & not $P$\\
$\iff$    & iff\\
$\iff$    & equivalent\\
$\oplus$   & xor\\
$\exists$ & exists\\
$\forall$ & for all\\
$\in$   &  is a member of\\

\iffalse

$\subseteq$ & is a subset of\\
$\subset$ & is a proper subset of\\
$\union$  & set union\\
$\intersect$ & set intersection\\
$\bar{A}$ & complement of a set, $A$\\
$\power(A)$ & powerset of a set, $A$\\
$\emptyset$ & the empty set, $\set{}$\\
$\naturals$ & nonnegative integers \\
$\integers$ & integers\\
$\integers^+$ & positive integers\\
$\integers^-$ & negative integers\\
$\rationals$ & rational numbers\\
$\reals$ & real numbers\\
$\complexes$ & complex numbers\\
$\emptystring$ & the empty string/list
\fi

\end{tabular}
\end{center}

\endinput
