\chapter{Expectation}\label{chap:expectation}

%% Probability Distributions Problems %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{problems}
\classproblems
\pinput{CP_bigger_number_game}
\pinput{CP_3_random_variables}

%\homeworkproblems
%\pinput{PS_drunken_sailor}
\end{problems}

\section{Definitions and Examples}

The \term{expectation} or \term{expected value} of a random variable
is a single number that tells you a lot about the behavior of the
variable.  Roughly, the expectation is the average value of the random
variable where each value is weighted according to its probability.
Formally, the expected value (also known as the \term{average} or
\term{mean}) of a random variable is defined as follows.

\begin{definition}\label{def:expectation}
If $R$~is a random variable defined on a sample space~$\sspace$, then
the expectation of~$R$ is
\begin{equation}\label{eqn:expectation}
    \expect{R} \eqdef \sum_{w \in \sspace} R(w) \prob{w}.
\end{equation}
\end{definition}

For example, suppose $\sspace$~is the set of students in a class, and
we select a student uniformly at random.  Let $R$~be the selected
student's exam score.  Then $\expect{R}$~is just the class
average---the first thing everyone wants to know after getting their
test back!  For similar reasons, the first thing you usually want to
know about a random variable is its expected value.

Let's work through some examples.

\subsection{The Expected Value of a Uniform Random Variable}

Let $R$~be the value that comes up with you roll a fair 6-sided die.
The the expected value of~$R$ is
\begin{equation*}
\expect{R}
     = 1 \cdot \frac{1}{6} + 2 \cdot \frac{1}{6} + 3 \cdot \frac{1}{6} +
        4 \cdot \frac{1}{6} + 5 \cdot \frac{1}{6} + 6 \cdot \frac{1}{6}
     = \frac{7}{2}.
\end{equation*}
%
This calculation shows that the name ``expected value'' is a little
misleading; the random variable might \emph{never} actually take on that
value.  You don't ever expect to roll a $3 \frac{1}{2}$ on an ordinary
die!

Also note that the mean of a random variable is not the same as the
\emph{median}.  The median is the midpoint of a distribution.

\begin{definition}\label{def:17A2}
The \term{median}\footnote{Some texts define the median to be the
  value of $x \in \range{R}$ for which $\prob{R \le x} < 1/2$ and
  $\prob{R > x} \le 1/2$.  The difference in definitions is
  not important.} of a random variable~$R$ is the value~$x \in
\range{R}$ such that
\begin{align*}
    \prob{R \le x} & \le \frac{1}{2} \qquad \text{and} \\
    \prob{R > x}   & <    \frac{1}{2}.
\end{align*}
\end{definition}

In this text, we will not devote much attention to the median.
Rather, we will focus on the expected value, which is much more
interesting and useful.

Rolling a 6-sided die provides an example of a uniform random
variable.  In general, if $R_n$~is a random variable with a uniform
distribution on~$\set{1, 2, \dots, n}$, then
\begin{align*}
\expect{R_n}    = \sum_{i = 1}^n i \cdot \frac{1}{n} % \\
                = \frac{n (n + 1)}{2n} % \\
                = \frac{n + 1}{2}.
\end{align*}

\subsection{The Expected Value of an Indicator Random Variable}

The \idx{expected value} of an \index{indicator variable} indicator random
variable for an event is just the probability of that event.

\begin{lemma}\label{expindic}
If $I_A$ is the indicator random variable for event $A$, then
\[
\expect{I_A} = \prob{A}.
\]
\end{lemma}

\begin{proof}
\begin{align*}
\expect{I_A}
& =  1 \cdot \prob{I_A = 1} + 0 \cdot \prob{I_A = 0} \\
& = \prob{I_A = 1} \\
& =  \prob{A}. \qquad\qquad \text{(def of $I_A$)}\qedhere
\end{align*}
\end{proof}
For example, if $A$~is the event that a coin with bias~$p$ comes up
heads, then $\expect{I_A} = \prob{I_A=1} = p$.

\subsection{Alternate Definitions}

There are several equivalent ways to define expectation.

\begin{theorem}\label{thm:altexpdef}
If $R$~is a random variable defined on a sample space~$\sspace$ then
\begin{equation}\label{eqn:altexpdef}
    \expect{R} = \sum_{x \in \range{R}} x \cdot \prob{R = x}.
\end{equation}
\end{theorem}
The proof of Theorem~\ref{thm:altexpdef}, like many of the elementary proofs
about expectation in this chapter, follows by judicious regrouping of terms
in the Equation~\ref{eqn:expectation}:
\begin{proof}
\begin{align*}
\expect{R}
    &= \sum_{\omega \in \sspace} R(\omega) \prob{\omega}
            &\text{(Def~\ref{def:expectation} of expectation)} \\
    &= \sum_{x \in \range{R}} \, \sum_{\omega \in [R=x]} R(\omega) \prob{\omega}
                \\
    &= \sum_{x \in \range{R}} \, \sum_{\omega \in [R=x]} x \prob{\omega}
            &\text{(def of the event $[R=x]$)}\\
    &= \sum_{x \in \range{R}} x \paren{\sum_{\omega \in [R=x]} \prob{\omega}}
            & \text{(distributing $x$ over the inner sum)} \\
    &= \sum_{x \in \range{R}} x \cdot \prob{R = x}.
            & \text{(def of $\prob{R=x}$)}
\end{align*}
The first equality follows because the events~$[R=x]$ for $x \in
\range{R}$ partition the sample space~$\sspace$, so summing over the
outcomes in $[R=x]$ for $x \in \range{R}$ is the same as summing
over~$\sspace$.
\end{proof}

In general, Equation~\ref{eqn:altexpdef} is more useful than
Equation~\ref{eqn:expectation} for calculating expected values and has
the advantage that it does not depend on the sample space, but only on
the density function of the random variable.  It is especially useful
when the range of the random variable is~$\naturals$, as we will see
from the following corollary.

\begin{corollary}\label{cor:17A4}
If the range of a random variable~$R$ is~$\naturals$, then
\begin{align*}
\expect{R}
    = \sum_{i = 1}^\infty i \prob{R = i} % \notag\\
    = \sum_{i = 0}^\infty \prob{R > i}. % \label{eqn:17A6}
\end{align*}
\end{corollary}

\begin{proof}
The first equality follows directly from Theorem~\ref{thm:altexpdef}
and the fact that $\range{R} = \naturals$.  The second equality
is derived by adding the following equations:
\begin{equation*}
\begin{array}{c@{}r@{\;}c@{\;}c@{\,}c@{\,}c@{\,}c@{\,}c@{\,}c@{\,}c}
      & \prob{R > 0} &=&
            \prob{R = 1} &+& \prob{R = 2} &+& \prob{R = 3} &+& \cdots\\
      & \prob{R > 1} &=&
                         && \prob{R = 2} &+& \prob{R = 3} &+& \cdots\\
{}{} & \prob{R > 2} &=&
                         &&              && \prob{R = 3} &+&
            \cdots\\[\jot]
      & &&&\vdots \\
\hline\noalign{\vskip\jot}
      & \llap{$\displaystyle\sum_{i = 0}^\infty \prob{R > i}$} &=&
            1 \cdot \prob{R = 1} &+& 2 \cdot \prob{R = 2}
                &+& 3 \cdot \prob{R = 3} &+& \cdots\\[\jot]
      & &=& \multicolumn{7}{l}{\displaystyle\sum_{i = 1}^\infty i \prob{R = i}.}
\qedhere
\end{array}
\end{equation*}
\end{proof}

\subsection{Mean Time to Failure}\label{mean_time_to_failure_subsec}

The mean time to failure is a critical parameter in the design of most
any system.  For example, suppose that a computer program crashes at
the end of each hour of use with probability~$p$, if it has not
crashed already.  What is the expected time until the program crashes?

If we let $C$~be the number of hours until the crash, then the answer
to our problem is~$\expect{C}$.  \ $C$ is a random variable with values
in~$\naturals$ and so we can use Corollary~\ref{cor:17A4} to determine
that
\begin{equation}\label{eqn:17:16.7}
    \expect{C} = \sum_{i = 0}^\infty \prob{C > i}.
\end{equation}

$\prob{C > i}$ is easy to evaluate: a crash happens later than the
$i$th hour iff the system did not crash during the first $i$ hours,
which happens with probability $(1-p)^i$.  Plugging this
into~Equation~\ref{eqn:17:16.7} gives:
%
\begin{align}
\expect{C} & = \sum_{i = 0}^\infty (1-p)^i \notag\\
       & = \frac{1}{1 - (1-p)} & \text{(sum of geometric series)}\notag\\
       & = \frac{1}{p}. \label{eqn:17T10}
\end{align}

For example, if there is a 1\%~chance that the program crashes at the
end of each hour, then the expected time until the program crashes is
$1 / 0.01 = 100$ hours.

The general principle here is well-worth remembering: 
\begin{quote}
\textit{If a system fails at each time step with probability~$p$, then
  the expected number of steps up to (and including) the first failure
  is~$1/p$.}
\end{quote}

\subsubsection{Making Babies}

As a related example, suppose a couple really wants to have a baby
girl.  For simplicity, assume that there is a 50\%~chance that each
child they have is a girl, and that the genders of their children are
mutually independent.  If the couple insists on having children until
they get a girl, then how many baby boys should they expect first?

The question, ``How many hours until the program crashes?'' is
mathematically the same as the question, ``How many children must the
couple have until they get a girl?''  In this case, a crash
corresponds to having a girl, so we should set $p = 1/2$.  By the
preceding analysis, the couple should expect a baby girl after having
$1/p = 2$ children.  Since the last of these will be the girl, they
should expect just one boy.

\subsection{Dealing with Infinity}

The analysis of the mean time to failure was easy enough.  But if you
think about it further, you might start to wonder about the case when
the computer program \emph{never} fails.  For example, what if the
program runs forever?  How do we handle outcomes with an infinite
value?

These are good questions and we wonder about them too.  Indeed,
mathematicians have gone to a lot of work to reason about sample
spaces with an infinite number of outcomes or outcomes with infinite
value.

To keep matters simple in this text, we will follow the common
convention of ignoring the contribution of outcomes that have
probability zero when computing expected values.  This means that we
can safely ignore the ``never-fail'' outcome, because it has
probability
\begin{equation*}
    \lim_{n \to \infty} (1 - p)^n = 0.
\end{equation*}

In general, when we are computing expectations for infinite sample
spaces, we will generally focus our attention on a subset of outcomes
that occur with collective probability~one.  For the most part, this
will allow us to ignore the ``infinite'' outcomes because they will
typically happen with probability~zero.\footnote{If this still bothers
  you, you might consider taking a course on measure theory.}

This assumption does \emph{not} mean that the expected value of a
random variable is always finite, however.  Indeed, there are many
examples where the expected value is infinite.  And where infinity
raises its ugly head, trouble is sure to follow.  Let's see an
example.

\subsection{Pitfall: Computing Expectations by Sampling}
\label{sec:latency}

Suppose that you are trying to estimate a parameter such as the
average delay across a communication channel.  So you set up an
experiment to measure how long it takes to send a test packet from one
end to the other and you run the experiment 100~times.

You record the latency, rounded to the nearest millisecond, for each
of the hundred experiments, and then compute the average of the
100~measurements.  Suppose that this average is~8.3\,ms.

Because you are careful, you repeat the entire process twice more and
get averages of~7.8\,ms and 7.9\,ms.  You conclude that the average
latency across the channel is
\begin{equation*}
    \frac{7.8 + 7.9 + 8.3}{3} = 8\,\mathrm{ms}.
\end{equation*}

You might be right but you might also be horribly wrong.  In fact, the
expected latency might well be \term{infinite}.  Here's how.

Let $D$~be a random variable that denotes the time it takes for the
packet to cross the channel.  Suppose that 
\begin{equation}\label{eqn:17T1}
\prob{D = i} = \begin{cases}
                0 & \text{for $i = 0$} \\
                \frac{1}{i} - \frac{1}{i + 1}   & \text{for $i \in \naturals^+$}.
               \end{cases}
\end{equation}
It is easy to check that
\begin{align*}
\sum_{i = 0}^\infty \prob{D = i}
    = \paren{1 - \frac{1}{2}} + \paren{\frac{1}{2} - \frac{1}{3}}
        + \paren{\frac{1}{3} - \frac{1}{4}} + \cdots % \\
    = 1
\end{align*}
and so $D$ is, in fact, a random variable.

From Equation~\ref{eqn:17T1}, we might expect that $D$~is likely to be
small.  Indeed, $D = 1$ with probability~$1/2$, \ $D = 2$ with
probability~$1/6$, and so forth.  So if we took 100~samples of~$D$,
about 50~would be~1\,ms, about 16~would be~2\,ms, and very few would
be large.  In summary, it might well be the case that the average of
the 100~measurements would be under~10\,ms, just as in our example.

This sort of reasoning and the calculation of expected values by
averaging experimental values is very common in practice.  It can
easily lead to incorrect conclusions, however.  For example, using
Corollary~\ref{cor:17A4}, we can quickly (and accurately) determine
that
\begin{align*}
\expect{D}
    &= \sum_{i = 1}^\infty i \prob{D = i} \\
    &= \sum_{i = 1}^\infty i \paren{\frac{1}{i} - \frac{1}{i + 1}} \\
    &= \sum_{i = 1}^\infty i \paren{\frac{1}{i (i + 1)}} \\
    &= \sum_{i = 1}^\infty \paren{\frac{1}{i + 1}} \\
    &= \infty.
\end{align*}

Uh-oh! The expected time to cross the communication channel is
\emph{infinite}!  This result is a far cry from the 10\,ms that we
calculated.  What went wrong?

It is true that most of the time, the value of~$D$ will be small.  But
sometimes $D$~will be very large and this happens with sufficient
probability that the expected value of~$D$ is unbounded.  In fact, if
you keep repeating the experiment, you are likely to see some outcomes
and averages that are much larger than~10\,ms.  In practice, such
``outliers'' are sometimes discarded, which masks the true behavior
of~$D$.

In general, the best way to compute an expected value in practice is
to first use the experimental data to figure out the distribution as
best you can, and then to use Theorem~\ref{thm:altexpdef} or
Corollary~\ref{cor:17A4} to compute its expectation.  This method will
help you identify cases where the expectation is infinite, and will
generally be more accurate than a simple averaging of the data.

\subsection{Conditional Expectation}

Just like event probabilities, expectations can be conditioned on some
event.  Given a random variable~$R$, the expected value of~$R$
conditioned on an event~$A$ is the (probability-weighted) average
value of~$R$ over outcomes in~$A$.  More formally:
\begin{definition}\label{condexpdef}
The \term{conditional expectation}~$\expcond{R}{A}$ of a random
variable~$R$ given event~$A$ is:
\begin{equation}\label{condexpsumv}
\expcond{R}{A} \eqdef \sum_{r \in \range{R}} r \cdot\prcond{R=r}{A}.
\end{equation}
\end{definition}

For example, we can compute the expected value of a roll of a fair die,
\emph{given}, for example, that the number rolled is at least 4.  We do
this by letting $R$ be the outcome of a roll of the die.  Then
by equation~\eqref{condexpsumv},
\[
\expcond{R}{R \geq 4} = \sum_{i=1}^6 i \cdot \prcond{R=i}{R \ge 4}
%= \sum_{i=4}^6 i \cdot 1/3
= 1\cdot 0 + 2\cdot 0 + 3\cdot 0 +
  4\cdot\tfrac{1}{3} + 5\cdot\tfrac{1}{3} + 6\cdot\tfrac{1}{3}
= 5.
\]

As another example, consider the channel latency problem from
Section~\ref{sec:latency}.  The expected latency for this problem was
infinite.  But what if we look at the expected latency conditioned on
the latency not exceeding~$n$.  Then
\begin{align*}
\expect{D}
    &= \sum_{i = 1}^\infty i \prcond{D = i}{D \le n} \\
    &= \sum_{i = 1}^\infty i \frac{\prob{D = i \land D \le n}}
                                  {\prob{D \le n}} \\
    &= \sum_{i = 1}^n \frac{i \prob{D = i}}{\prob{D \le n}} \\
    &= \frac{1}{\prob{D \le n}} \sum_{i = 1}^n i \paren{\frac{1}{i(i + 1)}}\\
    &= \frac{1}{\prob{D \le n}} \sum_{i = 1}^n \frac{1}{i + 1} \\
    &= \frac{1}{\prob{D \le n}} (H_{n+ 1} - 1),
\end{align*}
where $H_{n + 1}$ is the $(n + 1)$st Harmonic number
\begin{equation*}
    H_{n + 1} = \ln(n + 1) + \gamma + \frac{1}{2n} + \frac{1}{12n^2} +
    \frac{\epsilon(n)}{120n^4}
\end{equation*}
and $0 \le \epsilon(n) \le 1$.  The second equality follows from
the definition of conditional expectation, the third equality follows
from the fact that $\prob{D = i \land D \le n} = 0$ for~$i > n$, and
the fourth equality follows from the definition of~$D$ in
Equation~\ref{eqn:17T1}.

To compute $\prob{D \le n}$, we observe that
\begin{align*}
\prob{D \le n}
    &= 1 - \prob{D > n} \\
    &= 1 - \sum_{i = n + 1}^\infty \paren{\frac{1}{i} - \frac{1}{i + 1}} \\
    &= 1 - \left[ \paren{\frac{1}{n + 1} - \frac{1}{n + 2}}
                + \paren{\frac{1}{n + 2} - \frac{1}{n + 3}} \right. \\
    &\qquad\qquad\qquad \left.           + \paren{\frac{1}{n + 3} - \frac{1}{n + 4}}
                + \cdots \right] \\
    &= 1 - \frac{1}{n + 1} \\
    &= \frac{n}{n + 1}.
\end{align*}
Hence,
\begin{equation}
    \expect{D} = \frac{n + 1}{n} (H_{n + 1} - 1).
\end{equation}
For $n = 1000$, this is about~6.5.  This explains why the expected
value of~$D$ appears to be finite when you try to evaluate it
experimentally.  If you compute 100~samples of~$D$, it is likely that
all of them will be at most~1000\,ms.  If you condition on not having
any outcomes greater than~1000\,ms, then the conditional expected
value will be about~6.5\,ms, which would be a commonly observed result
in practice.  Yet we know that $\expect{D}$~is infinite.  For this
reason, expectations computed in practice are often really just
conditional expectations where the condition is that rare ``outlier''
sample points are eliminated from the analysis.

\subsection{The Law of Total Expectation}

Another useful feature of conditional expectation is that it lets us
divide complicated expectation calculations into simpler cases.  We
can then find the desired expectation by calculating the conditional
expectation in each simple case and averaging them, weighing each case
by its probability.

For example, suppose that 49.8\% of the people in the world are male and
the rest female---which is more or less true.  Also suppose the expected
height of a randomly chosen male is $5'\,11''$, while the expected height
of a randomly chosen female is $5'\,5''$.  What is the expected height of a
randomly chosen individual?  We can calculate this by averaging the
heights of men and women.  Namely, let $H$ be the height (in feet) of a
randomly chosen person, and let $M$ be the event that the person is male
and $F$ the event that the person is female.  Then
\begin{align*}
\expect{H} &= \expcond{H}{M} \prob{M} + \expcond{H}{F} \prob{F}\\
&= (5 + 11/12) \cdot 0.498  + (5+ 5/12) \cdot 0.502\\
&= 5.665
\end{align*}
which is a little less than~5'\,8".

This method is justified by the Law of \term{Total Expectation}.

\begin{theorem}[Law of Total Expectation]\label{total_expect}
Let $R$~be a random variable on a sample space~$\sspace$ and suppose
that $A_1$, $A_2$, \dots, is a partition of~$\sspace$.  Then
\begin{equation*}
    \expect{R} = \sum_i \expcond{R}{A_i} \prob{A_i}.
\end{equation*}
\end{theorem}

\begin{proof}
  \begin{align*}
    \expect{R} &= \sum_{r \in \range{R}} r \cdot \prob{R=r}
                  & \text{(Equation~\ref{eqn:altexpdef})}\\
    &= \sum_r r \cdot \sum_i \prcond{R=r}{A_i} \prob{A_i}
            & \text{(Law of Total Probability)}\\
    &= \sum_r \sum_i r \cdot \prcond{R=r}{A_i} \prob{A_i}
              & \text{(distribute constant $r$)}\\
    &= \sum_i \sum_r r \cdot \prcond{R=r}{A_i} \prob{A_i}
              & \text{(exchange order of summation)}\\
    &= \sum_i \prob{A_i} \sum_r r \cdot \prcond{R=r}{A_i}
             & \text{(factor constant $\prob{A_i}$)}\\
    &= \sum_i \prob{A_i} \expcond{R}{A_i}.
             & \text{(Def~\ref{condexpdef} of cond.\ expectation)}
  \end{align*}
\end{proof}

As a more interesting application of the Law of Total Expectation,
let's take another look at the mean time to failure of a system that
fails with probability~$p$ at each step.  We'll define~$A$ to be the
event that the system fails on the first step and $\bar{A}$ to be the
complementary event (namely, that the system does not fail on the
first step).  Then the mean time to failure~$\expect{C}$ is
\begin{equation}\label{eqn:17Y3}
    \expect{C} = \expcond{C}{A} \prob{A} + \expcond{C}{\bar{A}} \prob{\bar{A}}.
\end{equation}

Since $A$~is the condition that the system crashes on the first
step, we know that
\begin{equation}\label{eqn:17Y1}
    \expcond{C}{A} = 1.
\end{equation}
Since $\bar{A}$~is the condition that the system does \emph{not} crash on
the first step, conditioning on~$\bar{A}$ is equivalent to taking a first
step without failure and then starting over without conditioning.
Hence,
\begin{equation}\label{eqn:17Y2}
    \expcond{C}{\bar{A}} = 1 + \expect{C}.
\end{equation}

Plugging Equations \ref{eqn:17Y1} and~\ref{eqn:17Y2} into
Equation~\ref{eqn:17Y3}, we find that
\begin{align*}
\expect{C}
    &= 1 \cdot p + (1 + \expect{C}) (1 - p) \\
    &= p + 1 - p + (1 - p) \expect{C} \\
    &= 1 + (1 - p) \expect{C}.
\end{align*}
Rearranging terms, we find that
\begin{align*}
    1   = \expect{C} - (1 - p) \expect{C} % \\
        = p \expect{C},
\end{align*}
and thus that
\begin{equation*}
    \expect{C} = \frac{1}{p},
\end{equation*}
as expected.

We will use this sort of analysis extensively in
Chapter~\ref{ran_process_chap} when we examine the expected behavior
of random walks.

\subsection{Expectations of Functions}

Expectations can also be defined for functions of random variables.

\begin{definition}\label{def:exp_func}
Let $R: \sspace \to V$ be a random variable and $f: V \to \reals$ be a
total function on the range of~$R$.  Then
\begin{equation}\label{eqn:17P8}
    \expect{f(R)} = \sum_{w \in \sspace} f(R(w)) \prob{w}.
\end{equation}
Equivalently,
\begin{equation}\label{eqn:17P9}
    \expect{f(R)} = \sum_{r \in \range{R}} f(r) \prob{R = r}.
\end{equation}
\end{definition}

For example, suppose that $R$~is the value obtained by rolling a fair
6-sided die.  Then
\begin{align*}
\Expect{\frac{1}{R}}
    = \frac{1}{1} \cdot \frac{1}{6}
     + \frac{1}{2} \cdot \frac{1}{6}
     + \frac{1}{3} \cdot \frac{1}{6}
     + \frac{1}{4} \cdot \frac{1}{6}
     + \frac{1}{5} \cdot \frac{1}{6}
     + \frac{1}{6} \cdot \frac{1}{6} % \\
    = \frac{49}{120}.
\end{align*}

\section{Expected Returns in Gambling Games}

Some of the most interesting examples of expectation can be explained
in terms of gambling games.  For straightforward games where you
win~\$$A$ with probability~$p$ and you lose~\$$B$ with probability~$1 -
p$, it is easy to compute your \term{expected return} or
\term{winnings}.  It is simply
\begin{equation*}
    p A - (1 - p) B.
\end{equation*}
For example, if you are flipping a fair coin and you win~\$1 for heads
and you lose~\$1 for tails, then your expected winnings are
\begin{equation*}
    \frac{1}{2} \cdot 1 - \paren{1 - \frac{1}{2}} \cdot 1 = 0.
\end{equation*}
In such cases, the game is said to be \term{fair} since your expected
return is zero.

Some gambling games are more complicated and thus more interesting.
For example, consider the following game where the winners split a
pot.  This sort of game is representative of many poker games, betting
pools, and lotteries.

\subsection{Splitting the Pot}

After your last encounter with biker dude, one thing lead to another
and you have dropped out of school and become a Hell's Angel.  It's
late on a Friday night and, feeling nostalgic for the old days, you
drop by your old hangout, where you encounter two of your former TAs,
Eric and Nick.  Eric and Nick propose that you join them in a simple
wager. Each player will put~\$2 on the bar and secretly write
``heads'' or ``tails'' on their napkin.  Then one player will flip a
fair coin.   The \$6 on the bar will then be divided
equally among the players who correctly predicted the outcome of the
coin toss.

After your life-altering encounter with strange dice, you are more
than a little skeptical.  So Eric and Nick agree to let you be the one
to flip the coin.  This certainly seems fair.  How can you lose?

But you have learned your lesson and so before agreeing, you go
through the four-step method and write out the tree diagram to compute
your expected return.  The tree diagram is shown in
Figure~\ref{fig:17E1}.

\begin{figure}

\graphic{Fig_17E1}

\caption{The tree diagram for the game where three players each
  wager~\$2 and then guess the outcome of a fair coin toss.  The
  winners split the pot.}

\label{fig:17E1}

\end{figure}

The ``payoff'' values in Figure~\ref{fig:17E1} are computed by
dividing the \$6~pot\footnote{The money invested in a wager is
  commonly referred to as the \emph{pot}.} among those players who
guessed correctly and then subtracting the~\$2 that you put into the
pot at the beginning.  For example, if all three players guessed
correctly, then you payoff is~\$0, since you just get back your
\$2~wager.  If you and Nick guess correctly and Eric guessed wrong,
then your payoff is
\begin{equation*}
    \frac{6}{2} - 2 = 1.
\end{equation*}
In the case that everyone is wrong, you all agree to split the pot
and so, again, your payoff is zero.

To compute your expected return, you use
Equation~\ref{eqn:expectation} in the definition of expected value.
This yields
\begin{align*}
\expect{\text{payoff}}
    &= 0 \cdot \frac{1}{8} + 1 \cdot \frac{1}{8} + 1 \cdot \frac{1}{8}
        + 4 \cdot \frac{1}{8} \\
        & \qquad {}+ (-2) \cdot \frac{1}{8} + (-2) \cdot \frac{1}{8}
        + (-2) \cdot \frac{1}{8}
        + 0 \cdot \frac{1}{8} \\
    &= 0.
\end{align*}
This confirms that the game is fair.  So, for old time's sake, you
break your solemn vow to never ever engage in strange gambling games.

\subsection{The Impact of Collusion}

Needless to say, things are not turning out well for you.  The more
times you play the game, the more money you seem to be losing.  After
1000~wagers, you have lost over~\$500.  As Nick and Eric are consoling
you on your ``bad luck,'' you do a back-of-the-napkin calculation
using the bounds on the tails of the binomial distribution from
Section~\ref{binomial_distribution_section} that suggests that the
probability of losing \$500 in 1000~wagers is less than the
probability of a Vietnamese Monk waltzing in and handing you one of
those golden disks.  How can this be?

It is possible that you are truly very very unlucky.  But it is more
likely that something is wrong with the tree diagram in
Figure~\ref{fig:17E1} and that ``something'' just might have
something to do with the possibility that Nick and Eric are colluding
against you.

To be sure, Nick and Eric can only guess the outcome of the coin toss
with probability~$1/2$, but what if Nick and Eric always guess
differently?  In other words, what if Nick always guesses ``tails''
when Eric guesses ``heads,'' and vice-versa?  This would result in a
slightly different tree diagram, as shown in Figure~\ref{fig:17E2}.

\begin{figure}

\graphic{Fig_17E2}

\caption{The revised tree diagram reflecting the scenario where Nick
  always guesses the opposite of Eric.}

\label{fig:17E2}

\end{figure}

The payoffs for each outcome are the same in Figures \ref{fig:17E1}
and~\ref{fig:17E2}, but the probabilities of the outcomes are
different.  For example, it is no longer possible for all three
players to guess correctly, since Nick and Eric are always guessing
differently.  More importantly, the outcome where your payoff is~\$4
is also no longer possible.  Since Nick and Eric are always guessing
differently, one of them will always get a share of the pot.  As you
might imagine, this is not good for you!

When we use Equation~\ref{eqn:expectation} to compute your expected
return in the collusion scenario, we find that
\begin{align*}
\expect{\text{payoff}}
    &= 0 \cdot 0 + 1 \cdot \frac{1}{4} + 1 \cdot \frac{1}{4}
        + 4 \cdot 0 \\
        & \qquad {}+ (-2) \cdot 0 + (-2) \cdot \frac{1}{4}
        + (-2) \cdot \frac{1}{4}
        + 0 \cdot 0 \\
    &= -\frac{1}{2}.
\end{align*}
This is very bad indeed.  By colluding, Nick and Eric have made it so
that you expect to lose~\$.50 every time you play.  No wonder you
lost~\$500 over the course of 1000~wagers.  

Maybe it would be a good idea to go back to school---your Hell's
Angels buds may not be too happy that you just lost their~\$500.

\subsection{How to Win the Lottery}

Similar opportunities to ``collude'' arise in many betting games.
For example, consider the typical weekly football betting pool, where
each participant wagers~\$10 and the participants that pick the most
games correctly split a large pot.  The pool seems fair if you think
of it as in Figure~\ref{fig:17E1}.  But, in fact, if two or more
players collude by guessing differently, they can get an
``unfair'' advantage at your expense!

In some cases, the collusion is inadvertent and you can profit
from it.  For example, many years ago, a former MIT Professor of
Mathematics named Herman Chernoff figured out a way to make money by
playing the state lottery.  This was surprising since state lotteries
typically have very poor expected returns.  That's because the state
usually takes a large share of the wagers before distributing the rest
of the pot among the winners.  Hence, anyone who buys a lottery ticket
is expected to \emph{lose} money.  So how did Chernoff find a way to
make money?  It turned out to be easy!

In a typical state lottery,
\begin{itemize}

\item all players pay~\$1 to play and select 4~numbers from 1 to~36,

\item the state draws 4~numbers from 1 to~36 uniformly at random,

\item the states divides 1/2 of the money collected among the people
  who guessed correctly and spends the other half redecorating the
  governor's residence.

\end{itemize}
This is a lot like the game you played with Nick and Eric, except that
there are more players and more choices.  Chernoff discovered that a
small set of numbers was selected by a large fraction of the
population.  Apparently many people think the same way; they pick the
same numbers not on purpose as in the previous game with Nick and
Eric, but based on Manny's batting average or today's date.

It was as if the players were colluding to lose!  If any one of them
guessed correctly, then they'd have to split the pot with many other
players.  By selecting numbers uniformly at random, Chernoff was
unlikely to get one of these favored sequences.  So if he won, he'd
likely get the whole pot!  By analyzing actual state lottery data, he
determined that he could win an average of 7~cents on the dollar.  In
other words, his expected return was not~$-\${.}50$ as you might
think, but~$+\${.}07$.\footnote{Most lotteries now offer randomized
  tickets to help smooth out the distribution of selected sequences.}

Inadvertent collusion often arises in betting pools and is a
phenomenon that you can take advantage of.  For example, suppose you
enter a Super Bowl betting pool where the goal is to get closest to
the total number of points scored in the game.  Also suppose that the
average Super Bowl has a total of 30~point scored and that everyone
knows this.  Then most people will guess around 30~points.  Where
should you guess?  Well, you should guess just outside of this range
because you get to cover a lot more ground and you don't share the pot
if you win.  Of course, if you are in a pool with math students and
they all know this strategy, then maybe you should guess 30~points
after all.


\section{Expectations of Sums}

\subsection{Linearity of Expectation}\label{finlin}

Expected values obey a simple, very helpful rule called
\term{Linearity of Expectation}.  Its simplest form says that the
expected value of a sum of random variables is the sum of the expected
values of the variables.

\begin{theorem}\label{expsum-2}
For any random variables $R_1$ and $R_2$,
\[
\expect{R_1 + R_2} = \expect{R_1} + \expect{R_2}.
\]
\end{theorem}

\begin{proof}
Let $T \eqdef R_1+R_2$.  The proof follows straightforwardly by
rearranging terms in Equation~\eqref{eqn:expectation}:
\begin{align*}
\expect{T}
    & = \sum_{\omega \in \sspace} T(\omega) \cdot \prob{\omega}
        && \text{(Definition~\ref{def:expectation})}\\
        & = \sum_{\omega \in \sspace} (R_1(\omega) + R_2(\omega)) \cdot \prob{\omega}
        && \text{(definition of $T$)}\\
        & = \sum_{\omega \in \sspace} R_1(\omega) \prob{\omega} +
              \sum_{\omega \in \sspace} R_2(\omega) \prob{\omega}
        && \text{(rearranging terms)}\\
        & = \expect{R_1} + \expect{R_2}.   
        && \text{(Definition~\ref{def:expectation})} && \qedhere
\end{align*}
\end{proof}

A small extension of this proof, which we leave to the reader, implies
\begin{theorem}
For random variables $R_1$, $R_2$ and constants $a_1,a_2 \in \reals$,
\[
\expect{a_1R_1 + a_2R_2} = a_1\expect{R_1} + a_2\expect{R_2}.
\]
\end{theorem}
In other words, expectation is a linear function.  A routine induction
extends the result to more than two variables:
\begin{corollary}[\idx{Linearity of Expectation}]\label{linexp-k-thm}
For any random variables $R_1, \dots, R_k$ and constants $a_1, \dots, a_k
\in \reals$,
\[
    \expect{\sum_{i=1}^k a_iR_i} = \sum_{i=1}^k a_i\expect{R_i}.
\]
\end{corollary}

The great thing about linearity of expectation is that \emph{no
independence is required}.  This is really useful, because dealing with
independence is a pain, and we often need to work with random variables
that are not known to be independent.

As an example, let's compute the expected value of the sum of two fair
dice.  Let the random variable $R_1$~be the number on the first die,
and let $R_2$~be the number on the second die.  We observed earlier
that the expected value of one die is 3.5.  We can find the expected
value of the sum using linearity of expectation:
\begin{equation*}
\expect{R_1 + R_2}
 =   \expect{R_1} + \expect{R_2}
 =    3.5 + 3.5
 =    7.
\end{equation*}

Notice that we did \emph{not} have to assume that the two dice were
independent.  The expected sum of two dice is 7, even if they are
glued together (provided each individual die remains fair after the
gluing).  Proving that this expected sum is 7 with a tree diagram
would be a bother: there are 36 cases.  And if we did not assume that
the dice were independent, the job would be really tough!

\subsection{Sums of Indicator Random Variables}\label{sec:hat_check}

Linearity of expectation is especially useful when you have a sum of
indicator random variables.  As an example, suppose there is a dinner
party where $n$~men check their hats.  The hats are mixed up during
dinner, so that afterward each man receives a random hat.  In
particular, each man gets his own hat with probability~$1/n$.  What is
the expected number of men who get their own hat?

Letting $G$ be the number of men that get their own hat, we want to find
the expectation of $G$.  But all we know about $G$ is that the probability
that a man gets his own hat back is $1/n$.  There are many different
probability distributions of hat permutations with this property, so we
don't know enough about the distribution of $G$ to calculate its
expectation directly.  But linearity of expectation makes the problem
really easy.

The trick\footnote{We are going to use this trick a lot so it is
  important to understand it.} is to express~$G$ as a sum of indicator
variables.  In particular, let $G_i$~be an indicator for the event
that the $i$th man gets his own hat.  That is, $G_i = 1$ if the $i$th
man gets his own hat, and $G_i = 0$ otherwise.  The number of men that
get their own hat is then the sum of these indicator random variables:
\begin{equation}\label{GG}
    G = G_1 + G_2 + \cdots + G_n.
\end{equation}
These indicator variables are \emph{not} mutually independent.  For
example, if $n-1$ men all get their own hats, then the last man is
certain to receive his own hat.  But, since we plan to use linearity
of expectation, we don't have worry about independence!

Since $G_i$~is an indicator random variable, we know from
Lemma~\ref{expindic} that
\begin{equation}
    \expect{G_i}= \prob{G_i=1} = 1/n.
\end{equation} 
By Linearity of Expectation and Equation~\ref{GG}, this means that
\begin{align*}
\expect{G} & = \expect{G_1 + G_2 + \cdots + G_n} \\
       & = \expect{G_1} + \expect{G_2} + \cdots + \expect{G_n}\\
       & = \overbrace{\frac{1}{n} + \frac{1}{n} + \cdots + \frac{1}{n}}^n \\
       &= 1.
\end{align*}
So even though we don't know much about how hats are scrambled, we've
figured out that on average, just one man gets his own hat back!

More generally, Linearity of Expectation provides a very good method
for computing the expected number of events that will happen.

\begin{theorem}\label{thm:17T3}
Given any collection of $n$~events $A_1, A_2, \dots, A_n
\subseteq \sspace$, the expected number of events that will occur is
\begin{equation*}
    \sum_{i = 1}^n \prob{A_i}.
\end{equation*}
\end{theorem}

For example, $A_i$~could be the event that the $i$th man gets the
right hat back.  But in general, it could be any subset of the sample
space, and we are asking for the expected number of events that will
contain a random sample point.

\begin{proof}

Define~$R_i$ to be the indicator random variable for~$A_i$, where
$R_i(w) = 1$ if $w \in A_i$ and $R_i(w) = 0$ if $w \notin A_i$.  Let
$R = R_1 + R_2 + \dots + R_n$.  Then
\begin{align*}
\expect{R}
    &= \sum_{i = 1}^n \expect{R_i}
    & \text{(by Linearity of Expectation)}\\
%
    &= \sum_{i = 1}^n \prob{R_i = 1}
    & \text{(by Lemma \ref{expindic})}\\
%
    &= \sum_{i = 1}^n \sum_{w \in A_i} \prob{w}
    & \text{(definition of indicator variable)}\\
%
    &= \sum_{i = 1}^n \prob{A_i}. & \qedhere
\end{align*}
\end{proof}

So whenever you are asked for the expected number of events that occur,
all you have to do is sum the probabilities that each event occurs.
Independence is not needed.

\subsection{Expectation of a Binomial Distribution}

Suppose that we independently flip $n$~biased coins, each with
probability~$p$ of coming up heads.  What is the expected number of
heads?

Let $J$~be the random variable denoting the number of heads.  Then
$J$~has a binomial distribution with parameters~$n$,~$p$, and
\begin{equation*}
    \prob{J = k} = \binom{n}{k} k^p (n - k)^{1 - p}.
\end{equation*}
Applying Equation~\ref{eqn:altexpdef}, this means that
\begin{align}
\expect{J}
    &= \sum_{k = 0}^n k \prob{J = k} \notag\\
    &= \sum_{k = 0}^n k \binom{n}{k} k^p (n - k)^{1 - p}. \label{eqn:17T7}
\end{align}
Ouch!  This is one nasty looking sum.  Let's try another approach.

Since we have just learned about linearity of expectation for sums of
indicator random variables, maybe Theorem~\ref{thm:17T3} will be
helpful.  But how do we express~$J$ as a sum of indicator random
variables?  It turns out to be easy.  Let $J_i$~be the indicator
random variable for the $i$th~coin.  In particular, define
\begin{equation*}
J_i = \begin{cases}
        1 & \text{if the $i$th coin is heads} \\
        0 & \text{if the $i$th coin is tails}.
      \end{cases}
\end{equation*}
Then the number of heads is simply
\begin{equation*}
    J = J_1 + J_2 + \dots + J_n.
\end{equation*}
By Theorem~\ref{thm:17T3},
\begin{align}
\expect{J}
    &= \sum_{i = 1}^n \prob{J_i} \notag\\
    &= np. \label{eqn:17T8}
\end{align}

That really was easy.  If we flip $n$~mutually independent coins, we
expect to get $pn$~heads.  Hence the expected value of a binomial
distribution with parameters $n$ and~$p$ is simply~$pn$.

But what if the coins are not mutually independent?  It doesn't
matter---the answer is still~$p n$ because Linearity of Expectation
and Theorem~\ref{thm:17T3} do not assume any independence.

If you are not yet convinced that Linearity of Expectation and
Theorem~\ref{thm:17T3} are powerful tools, consider this: without even
trying, we have used them to prove a very complicated identity,
namely\footnote{This follows by combining Equations \ref{eqn:17T7}
  and~\ref{eqn:17T8}.}
\begin{equation*}
    \sum_{k = 0}^n k \binom{n}{k} k^p (n - k)^{1 - p} = p n.
\end{equation*}

If you are still not convinced, then take a look at the next problem.

\subsection{The Coupon Collector Problem}

Every time we purchase a kid's meal at Taco Bell, we are graciously
presented with a miniature ``Racin' Rocket'' car together with a
launching device which enables us to project our new vehicle across
any tabletop or smooth floor at high velocity.  Truly, our delight
knows no bounds.

There are $n$~different types of Racin' Rocket cars (blue, green, red,
gray, etc.).  The type of car awarded to us each day by the kind woman
at the Taco Bell register appears to be selected uniformly and
independently at random.  What is the expected number of kid's meals
that we must purchase in order to acquire at least one of each type of
Racin' Rocket car?

The same mathematical question shows up in many guises: for example,
what is the expected number of people you must poll in order to find
at least one person with each possible birthday?  Here, instead of
collecting Racin' Rocket cars, you're collecting birthdays.  The
general question is commonly called the \term{coupon collector
problem} after yet another interpretation.

A clever application of linearity of expectation leads to a simple
solution to the coupon collector problem.  Suppose there are five
different types of Racin' Rocket cars, and we receive this sequence:
%
\begin{center}
blue \quad green \quad green \quad red \quad blue \quad orange \quad
blue \quad orange \quad gray.
\end{center}
%
Let's partition the sequence into 5 segments:
%
\[
\underbrace{\strut\text{blue}}_{X_0} \quad
\underbrace{\strut\text{green}}_{X_1} \quad
\underbrace{\strut\text{green} \quad \text{red}}_{X_2} \quad
\underbrace{\strut\text{blue} \quad \text{orange}}_{X_3} \quad
\underbrace{\strut\text{blue} \quad \text{orange} \quad \text{gray}}_{X_4}.
\]
%
The rule is that a segment ends whenever we get a new kind of car.
For example, the middle segment ends when we get a red car for the
first time.  In this way, we can break the problem of collecting every
type of car into stages.  Then we can analyze each stage individually
and assemble the results using linearity of expectation.

Let's return to the general case where we're collecting $n$~Racin'
Rockets.  Let $X_k$~be the length of the $k$th segment.  The total
number of kid's meals we must purchase to get all $n$ Racin' Rockets is
the sum of the lengths of all these segments:
%
\[
T = X_0 + X_1 + \cdots + X_{n-1}
\]

Now let's focus our attention on~$X_k$, the length of the $k$th
segment.  At the beginning of segment $k$, we have $k$~different types
of car, and the segment ends when we acquire a new type.  When we own
$k$ types, each kid's meal contains a type that we already have with
probability~$k / n$.  Therefore, each meal contains a new type of car
with probability $1 - k / n = (n - k) / n$.  Thus, the expected number
of meals until we get a new kind of car is $n / (n - k)$ by the ``mean
time to failure'' formula in Equation~\ref{eqn:17T10}.  This means
that
%
\[
    \expect{X_k} = \frac{n}{n - k}.
\]

Linearity of expectation, together with this observation, solves the
coupon collector problem:
%
\begingroup
\openup\jot
\begin{align}
\expect{T}
  & = \expect{X_0 + X_1 + \cdots + X_{n-1}} \notag\\
%
  & = \expect{X_0} + \expect{X_1} + \cdots + \expect{X_{n-1}} \notag\\
%
  & = \frac{n}{n - 0} + \frac{n}{n - 1} + \cdots + \frac{n}{3} +
    \frac{n}{2} + \frac{n}{1} \notag\\
%
  & = n \paren{\frac{1}{n} + \frac{1}{n-1} + \cdots + \frac{1}{3} +
  \frac{1}{2} + \frac{1}{1}} \notag\\
%
  &= n \paren{\frac{1}{1} + \frac{1}{2} + \frac{1}{3} + \cdots +
      \frac{1}{n-1} + \frac{1}{n}} \notag\\
%
  & = n H_n \\
  & \sim n \ln n. \label{eqn:17T11}
\end{align}
\endgroup

Wow!  It's those Harmonic Numbers again!  

We can use Equation~\ref{eqn:17T11} to answer some concrete questions.
For example, the expected number of die rolls required to see every
number from 1 to 6 is:
%
\[
    6 H_6 = 14.7 \dots.
\]
%
And the expected number of people you must poll to find at least one
person with each possible birthday is:
%
\[
    365 H_{365} = 2364.6\dots.
\]


\subsection{Infinite Sums}

Linearity of expectation also works for an infinite number of random
variables provided that the variables satisfy some stringent absolute
convergence criteria.

\begin{theorem}[Linearity of Expectation]\label{linexp}
Let $R_0$, $R_1$, \dots, be random variables such that
\[
\sum_{i = 0}^\infty \expect{\abs{R_i}}
\]
converges.  Then
\[
   \Expect{\sum_{i = 0}^\infty R_i} = \sum_{i = 0}^\infty \expect{R_i}.
\]
\end{theorem}

\begin{proof}
Let $T \eqdef \sum_{i = 0}^\infty R_i$.

We leave it to the reader to verify that, under the given convergence
hypothesis, all the sums in the following derivation are absolutely
convergent, which justifies rearranging them as follows:
\begin{align*}
\sum_{i=0}^\infty \expect{R_i}
    &= \sum_{i=0}^\infty \sum_{s \in \sspace} R_i(s) \cdot \prob{s}
            & \text{(Def.~\ref{def:expectation})}\\
    &= \sum_{s \in \sspace} \sum_{i=0}^\infty R_i(s) \cdot \prob{s}
           & \text{(exchanging order of summation)}\\
    &= \sum_{s \in \sspace} \left[ \sum_{i=0}^\infty R_i(s) \right] \cdot \prob{s}
                & \text{(factoring out $\prob{s}$)}\\
    &= \sum_{s \in \sspace} T(s) \cdot \prob{s} & \text{(Def.\ of $T$)}\\
    &= \expect{T} & \text{(Def.~\ref{def:expectation})}\\
    &= \expect{\sum_{i = 0}^\infty R_i}. &  \text{(Def.\ of $T$)}. &\qedhere
\end{align*}
\end{proof}

\section{Expectations of Products}

While the expectation of a sum is the sum of the expectations, the same is
usually not true for products.  For example, suppose that we roll a
fair 6-sided die and denote the outcome with the random variable~$R$.
Does $\expect{R \cdot R} = \expect{R} \cdot \expect{R}$?

We know that $\expect{R} = 3\frac{1}{2}$ and thus $\expect{R}^2 =
12\frac{1}{4}$.  Let's compute~$\expect{R^2}$ to see if we get the same
result.
\begin{align*}
\expect{R^2}
    & = \sum_{w \in \sspace} R^2(w) \prob{w} \\
    & = \sum_{i=1}^6 i^2 \cdot \prob{R_i = i} \\
    & = \frac{1^2}{6} + \frac{2^2}{6} + \frac{3^2}{6} +
            \frac{4^2}{6} + \frac{5^2}{6} + \frac{6^2}{6} \\
    & =   15\; 1/6\\
    & \neq  12 \; 1/4.
\end{align*}
Hence,
\begin{equation*}
    \expect{R \cdot R} \ne \expect{R} \cdot \expect{R}
\end{equation*}
and so the expectation of a product is not always equal to the product
of the expectations.

There is a special case when such a relationship \emph{does} hold
however; namely, when the random variables in the product are
\emph{independent}.

\begin{theorem}\label{th:prod}
For any two \emph{independent} random variables $R_1$, $R_2$,
\[
\expect{R_1 \cdot R_2} = \expect{R_1} \cdot \expect{R_2}.
\]
\end{theorem}

\begin{proof}
\dmj{This display still needs help.  I'll look at it again when I'm
  feeling stronger.}
The event $[R_1 \cdot R_2=r]$ can be split up into events of the form
$[R_1 = r_1\ \text{ and }\ R_2 = r_2]$ where $r_1\cdot r_2=r$.  So
\begingroup
\openup\jot
\begin{align*}
\lefteqn{\expect{R_1 \cdot R_2}}\\
& = \sum_{r \in \range{R_1\cdot R_2}} r\cdot \prob{R_1\cdot R_2=r}
                    &\text{(Theorem~\ref{thm:altexpdef})} \\
& =      \sum_{r_1 \in \range{R_1}} \, \sum_{r_2 \in \range{R_2}}
            r_1 r_2 \cdot \prob{R_1=r_1\ \text{ and }\ R_2=r_2} \\
& =      \sum_{r_1 \in \range{R_1}} \, \sum_{r_2 \in \range{R_2}}
            r_1 r_2 \cdot \prob{R_1=r_1}\cdot \prob{R_2=r_2}
                    &\text{(independence of $R_1,R_2$)}\\
& =      \sum_{r_1 \in \range{R_1}} r_1\prob{R_1=r_1}
              \paren{\sum_{r_2 \in \range{R_2}} r_2 \prob{R_2=r_2}}
                    &\text{(factor out $r_1\prob{R_1=r_1}$)}\\
& =      \sum_{r_1 \in \range{R_1}} r_1\prob{R_1=r_1} \cdot \expect{R_2}
                    &\text{(Theorem~\ref{thm:altexpdef})}\\
& =       \expect{R_2} \paren{ \sum_{r_1 \in \range{R_1}} r_1\prob{R_1=r_1}}
                    &\text{(factor out $\expect{R_2}$)}\\
& =       \expect{R_2} \cdot  \expect{R_1}.
                    &\text{(Theorem~\ref{thm:altexpdef})}
\end{align*}
\endgroup
\end{proof}

For example, let $R_1$ and~$R_2$ be random variables denoting the
result of rolling two independent and fair 6-sided dice.  Then
\begin{equation*}
\expect{R_1 \cdot R_2}
    = \expect{R_1} \expect{R_2} 
    = 3\frac{1}{2} \cdot 3\frac{1}{2} 
    = 12\frac{1}{4}.
\end{equation*}

Theorem~\ref{th:prod} extends by induction to a collection of mutually
independent random variables.
\begin{corollary}
If random variables $R_1, R_2, \dots, R_k$ are mutually
independent, then
\[
    \Expect{\prod_{i=1}^k R_i} = \prod_{i=1}^k \expect{R_i}.
\]
\end{corollary}

\section{Expectations of Quotients}

If $S$ and~$T$ are random variables, we know from Linearity of
Expectation that
\begin{equation*}
    \expect{S + T} = \expect{S} + \expect{T}.
\end{equation*}
If $S$ and~$T$ are independent, we know from Theorem~\ref{th:prod}
that
\begin{equation*}
    \expect{ST} = \expect{S} \expect{T}.
\end{equation*}

Is it also true that
\begin{equation}\label{eqn:17P1}
    \expect{S/T} = \expect{S}/\expect{T}?
\end{equation}
Of course, we have to worry about the situation when~$\expect{T} = 0$,
but what if we assume that $T$~is always positive?  As we will soon
see, Equation~\ref{eqn:17P1} is usually not true, but let's see if we
can prove it anyway.

\begin{falseclm}\label{fc:17P2}
If $S$ and~$T$ are independent random variables with $T > 0$, then
\begin{equation}
    \expect{S/T} = \expect{S}/\expect{T}.
\end{equation}
\end{falseclm}

\begin{bogusproof}
\begin{align}
\expect{\frac{S}{T}} & = \expect{S \cdot \frac{1}{T}} \notag\\
       & = \expect{S} \cdot \Expect{\frac{1}{T}} & \text{(independence of $S$
       and $T$)}\label{indST}\\
      & = \expect{S} \cdot \frac{1}{\expect{T}}. \label{bugindST}\\
      & = \frac{\expect{S}}{\expect{T}}.\notag \qedhere
\end{align}
\end{bogusproof}
Note that line~\ref{indST} uses the fact that if $S$ and~$T$ are
independent, then so are $S$ and~$1/T$.  This holds because functions
of independent random variables are independent.  It is a fact that
needs proof, which we will leave to the reader, but it is not the
bug. The bug is in line~\eqref{bugindST}, which assumes
\begin{falseclm}\label{false-inverse}
\[
\expect{\frac{1}{T}} =  \frac{1}{\expect{T}}.
\]
\end{falseclm}

Here is a counterexample.  Define~$T$ so that
\begin{equation*}
    \prob{T = 1} = \frac{1}{2} \quad\text{and}\quad
    \prob{T = 2} = \frac{1}{2}.
\end{equation*}
Then
\begin{equation*}
    \expect{T} = 1 \cdot \frac{1}{2} + 2 \cdot \frac{1}{2} =
    \frac{3}{2}
\end{equation*}
and
\begin{equation*}
    \frac{1}{\expect{T}} = \frac{2}{3}
\end{equation*}
and
\begin{equation*}
\Expect{\frac{1}{T}}
    = \frac{1}{1} \cdot \frac{1}{2} + \frac{1}{2} \cdot \frac{1}{2}
    = \frac{3}{4} 
    \ne \frac{1}{\expect{1/T}}.
\end{equation*}
This means that Claim~\ref{fc:17P2} is also false since we could
define~$S = 1$ with probability~1.  In fact, both Claims \ref{fc:17P2}
and~\ref{false-inverse} are untrue for most all choices of $S$
and~$T$.  Unfortunately, the fact that they are false does not keep
them from being widely used in practice!  Let's see an example.

\subsection{A RISC Paradox}

The data in Table~\ref{fig:17P4} is representative of data in a paper
by some famous professors.  They wanted to show that programs on a
RISC processor are generally shorter than programs on a CISC
processor.  For this purpose, they applied a RISC compiler and then a
CISC compiler to some benchmark source programs and made a table of
compiled program lengths.

\begin{table}

\begin{tabular}{lccc}
Benchmark        & RISC          & CISC          & CISC/RISC\\
\hline
E-string search  & 150           & 120           & 0.8 \\
F-bit test       & 120           & 180           & 1.5 \\
Ackerman         & 150           & 300           & 2.0 \\
Rec 2-sort       & 2800          & 1400          & 0.5 \\
\hline
Average          &               &               & 1.2
\end{tabular}

\caption{Sample program lengths for benchmark problems using RISC and
  CISC compilers.}

\label{fig:17P4}

\end{table}

Each row in Table~\ref{fig:17P4} contains the data for one benchmark.
The numbers in the second and third columns are program lengths for
each type of compiler.  The fourth column contains the ratio of the
CISC program length to the RISC program length.  Averaging this ratio
over all benchmarks gives the value 1.2 in the lower right.  The
conclusion is that CISC programs are 20\% longer on average.

However, some critics of their paper took the same data and argued this
way: redo the final column, taking the other ratio, RISC/CISC instead of
CISC/RISC, as shown in Table~\ref{fig:17P5}.

\begin{table}

\begin{tabular}{lccc}
Benchmark        & RISC          & CISC          & RISC/CISC\\
\hline
E-string search  & 150           & 120           & 1.25 \\
F-bit test       & 120           & 180           & 0.67 \\
Ackerman         & 150           & 300           & 0.5 \\
Rec 2-sort       & 2800          & 1400          & 2.0 \\
\hline
Average          &               &               & 1.1
\end{tabular}

\caption{The same data as in Table~\ref{fig:17P4}, but with the
  opposite ratio in the last column.}

\label{fig:17P5}

\end{table}

From Table~\ref{fig:17P5}, we would conclude that RISC programs are
10\% longer than CISC programs on average!  We are using the same
reasoning as in the paper, so this conclusion is equally
justifiable---yet the result is opposite.  What is going on?

\subsubsection{A Probabilistic Interpretation}

To resolve these contradictory conclusions, we can model the RISC
vs.\ CISC debate with the machinery of probability theory.

Let the sample space be the set of benchmark programs.  Let the random
variable $R$ be the length of the compiled RISC program, and let the
random variable $C$ be the length of the compiled CISC program.  We would
like to compare the average length~$\expect{R}$ of a RISC program to the
average length~$\expect{C}$ of a CISC program.

To compare average program lengths, we must assign a probability to
each sample point; in effect, this assigns a ``weight'' to each
benchmark.  One might like to weigh benchmarks based on how frequently
similar programs arise in practice.  Lacking such data, however, we
will assign all benchmarks equal weight; that is, our sample space is
uniform.

In terms of our probability model, the paper computes $C / R$ for each
sample point, and then averages to obtain $\expect{C / R} = 1.2$.
This much is correct.  The authors then conclude that CISC programs
are 20\% longer on average; that is, they conclude that $\expect{C} =
1.2\, \expect{R}$.  Therein lies the problem.  The authors have
implicitly used False Claim~\ref{fc:17P2} to assume that $\expect{C/R}
= \expect{C}/\expect{R}$.  By using the same false logic, the critics
can arrive at the opposite conclusion; namely, that RISC programs are
10\%~longer on average.

%% BEGIN INSERT Q

\subsubsection{The Proper Quotient}

We can compute $\expect{R}$ and $\expect{C}$ as follows:
\begin{align*}
\expect{R}  
    & = \sum_{i \in \text{Range(R)}} i \cdot \prob{R = i} \\
    & = \frac{150}{4}+\frac{120}{4}+\frac{150}{4}+\frac{2800}{4} \\
    & = 805, \\
\\
\expect{C}
    & = \sum_{i \in \text{Range(C)}} i \cdot \prob{C = i} \\
    & = \frac{120}{4}+\frac{180}{4}+\frac{300}{4}+\frac{1400}{4} \\
    & = 500
\end{align*}

Now since $\expect{R}/\expect{C} = 1.61$, we conclude that the
\emph{average RISC program} is 61\% longer than the \emph{average CISC
  program}.  This is a third answer, completely different from the
other two!  Furthermore, this answer makes RISC look really bad in
terms of code length.  This one is the correct conclusion, under our
assumption that the benchmarks deserve equal weight.  Neither of the
earlier results were correct---not surprising since both were based on
the same False Claim.

%% END INSERT Q

\subsubsection{A Simpler Example}

The source of the problem is clearer in the following, simpler
example.  Suppose the data were as follows.
\begin{center}
\begin{tabular}{lcccc}
Benchmark   & Processor $A$ & Processor $B$ & $B / A$   & $A / B$  \\
\hline
Problem 1   & 2             & 1             & 1/2       & 2 \\
Problem 2   & 1             & 2             & 2         & 1/2 \\
\hline
Average     &               &               & 1.25      & 1.25
\end{tabular}
\end{center}

Now the data for the processors $A$ and~$B$ is exactly symmetric; the
two processors are equivalent.  Yet, from the third column we would
conclude that Processor~$B$ programs are 25\% longer on average, and
from the fourth column we would conclude that Processor~$A$ programs
are 25\% longer on average.  Both conclusions are obviously wrong.

The moral is that one must be very careful in summarizing data, we must
not take an average of ratios blindly!

%% INSERT K GOES HERE

%% Average & Expected Value Problems %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{problems}
\practiceproblems
\pinput{TP_psets_and_laundry}
\pinput{TP_grading_final_exam}

\classproblems
\pinput{CP_carnival_dice_fair}
\pinput{CP_sixteen_desks}
\pinput{CP_probable_satisfiability}
\pinput{CP_consecutive_coin_flips}
\pinput{CP_independent_product}
\pinput{CP_probable_satisfiability_nk}
\pinput{CP_st_petersberg}

\homeworkproblems
\pinput{PS_independent_random_variables}
\end{problems}

%% Conclusion %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%\TBA{Conclusion...}


\problemsection

\begin{editingnotes}

MAKE THIS A PROBLEM

\subsubsection{The Number-Picking Game}

Here is a game that you and I could play that reveals a strange
property of expectation.

First, you think of a probability density function on the natural
numbers.  Your distribution can be absolutely anything you like.  For
example, you might choose a uniform distribution on $1, 2, \dots, 6$,
like the outcome of a fair die roll.  Or you might choose a binomial
distribution on $0, 1, \dots, n$.  You can even give every natural
number a non-zero probability, provided that the sum of all
probabilities is 1.

Next, I pick a random number $z$ according to your distribution.
Then, you pick a random number $y_1$ according to the same
distribution.  If your number is bigger than mine ($y_1 > z$), then
the game ends.  Otherwise, if our numbers are equal or mine is bigger
($z \geq y_1$), then you pick a new number $y_2$ with the same
distribution, and keep picking values $y_3$, $y_4$, etc. until you get
a value that is strictly bigger than my number, $z$.  What is the
expected number of picks that you must make?

Certainly, you always need at least one pick, so the expected number
is greater than one.  An answer like 2 or 3 sounds reasonable, though
one might suspect that the answer depends on the distribution.  Let's
find out whether or not this intuition is correct.

The number of picks you must make is a natural-valued random variable, so
from formula~\eqref{R>i} we have:
\begin{align}
\expect{\text{\# picks by you}}
    & = \sum_{k \in \naturals} \prob{\text{(\# picks by you)} > k} \label{eqn:1}
\end{align}
Suppose that I've picked my number $z$, and you have picked $k$
numbers $y_1, y_2, \dots, y_k$.  There are two possibilities:
%
\begin{itemize}

\item If there is a unique largest number among our picks, then my
number is as likely to be it as any one of yours.  So with probability
$1/(k+1)$ my number is larger than all of yours, and you must pick
again.

\item Otherwise, there are several numbers tied for largest.  My
number is as likely to be one of these as any of your numbers, so with
probability greater than $1/(k+1)$ you must pick again.

\end{itemize}
%
In both cases, with probability at least $1/(k+1)$, you need more than
$k$ picks to beat me.  In other words:
%
\begin{align}
\prob{\text{(\# picks by you)} > k} \geq \frac{1}{k+1} \label{eqn:2}
\end{align}

This suggests that in order to minimize your rolls, you should choose a
distribution such that ties are very rare.  For example, you might
choose the uniform distribution on $\set{1, 2, \dots, 10^{100}}$.  In
this case, the probability that you need more than $k$ picks to beat
me is very close to $1/(k+1)$ for moderate values of $k$.  For
example, the probability that you need more than 99 picks is almost
exactly 1\%.  This sounds very promising for you; intuitively, you
might expect to win within a reasonable number of picks on average!

Unfortunately for intuition, there is a simple proof that the expected
number of picks that you need in order to beat me is
\emph{infinite}, regardless of the distribution!  Let's
plug~\eqref{eqn:2} into~\eqref{eqn:1}:
%
\begin{align*}
\expect{\text{\# picks by you}}
    & = \sum_{k \in \naturals} \frac{1}{k+1} \\
    & = \infty
\end{align*}

\end{editingnotes}

\begin{editingnotes}

MAKE THIS A PROBLEM

One of the simplest casino bets is on ``red'' or ``black'' at the roulette
table.  In each play at roulette, a small ball is set spinning around a
roulette wheel until it lands in a red, black, or green colored slot.
The payoff for a bet on red or black matches the bet; for example, if you bet
$\$10$ on red and the ball lands in a red slot, you get back your original
$\$10$ bet plus another matching $\$10$.

In the US, a roulette wheel has 2 green slots among 18 black and 18 red
slots, so the probability of red is $p::= 18/38 \approx 0.473$.  In
Europe, where roulette wheels have only 1 green slot, the odds for red
are a little better---that is, $p = 18/37 \approx 0.486$---but still less
than even.  To make the game fair, we might agree to ignore green, so that
$p = 1/2$.

There is a notorious gambling strategy which seems to guarantee a profit
at roulette: bet $\$10$ on red, and keep doubling the bet until a red
comes up.  This strategy implies that a player will leave the game as a
net winner of $\$10$ as soon as the red first appears.  Of course the
player may need an awfully large bankroll to avoid going bankrupt before
red shows up---but we know that the mean time until a red occurs is $1/p$,
so it seems possible that a moderate bankroll might actually work out.
(In this setting, a ``win'' on red corresponds to a ``failure'' in a
mean-time-to-failure situation.)

Suppose we have the good fortune to gamble against a fair roulette wheel.
In this case, our expected win on any spin is zero, since at the $i$th
spin we are equally likely to win or lose $10 \cdot 2^{i-1}$ dollars.  So
our expected win after any finite number of spins remains zero, and
therefore our expected win using this gambling strategy is zero.  This is
just what we should have anticipated in a fair game.

But wait a minute.  As long as there is a fixed, positive probability of
red appearing on each spin of the wheel---even if the wheel is
unfair---it's \emph{certain} that red will eventually come up.  So with
probability one, we leave the casino having won $\$10$, and our expected
dollar win is obviously $\$10$, not zero!

Something's wrong here.  What?

\subsection{Solution to the Paradox}

The expected amount won is indeed $\$10$.

The argument claiming the expectation is zero is flawed by an invalid use
of linearity of expectation for an infinite sum.  To pinpoint this flaw,
let's first make the sample space explicit: a sample point is a sequence
$B^nR$ representing a run of $n \geq 0$ black spins terminated by a red
spin.  Since the wheel is fair, the probability of $B^nR$ is $2^{-(n+1)}$.

Let $C_i$ be the number of dollars won on the $i$th spin.  So
$C_i = 10 \cdot 2^{i-1}$
when red comes up for the first time on the $i$th spin, that is, at
precisely one sample point, namely $B^{i-1}R$.  Similarly,
$C_i = -10  \cdot  2^{i-1}$
when the first red spin comes up after the $i$th spin, namely, at the
sample points $B^nR$ for $n \geq i$.  Finally, we will define $C_i$ by
convention to be zero at sample points in which the session ends before the
$i$th spin, that is, at points $B^nR$ for $n < i-1$.

The dollar amount won in any gambling session is the value of the sum
$\sum_{i = 1}^\infty C_i$.  At any sample point $B^nR$, the value of this
sum is
\[
10 \cdot -(1 + 2 + 2^2 + \dots + 2^{n - 1}) + 10 \cdot 2^n  = 10,
\]
which trivially implies that its expectation is 10 as well.  That is, the
amount we are \emph{certain} to leave the casino with, as well as
expectation of the amount we win, is $\$10$.

Moreover, our reasoning that $\expect{C_i} = 0$ is sound, so
\[
\sum_{i = 1}^\infty \expect{C_i} = \sum_{i = 1}^\infty 0 = 0.
\]

The flaw in our argument is the claim that, since the expectation at each
spin was zero, therefore the final expectation would also be zero.
Formally, this corresponds to concluding that
\[
\expect{\mbox{amount won}}  =  \expect{\sum_{i = 1}^\infty C_i}
  =  \sum_{i = 1}^\infty \expect{C_i} = 0.
\]
The flaw lies exactly in the second equality.  This is a case where
linearity of expectation fails to hold---even though both $\sum_{i =
1}^\infty \expect{C_i}$ and $\expect{\sum_{i = 1}^\infty C_i}$ are
finite---because the convergence hypothesis needed for linearity is false.
Namely, the sum
\[
\sum_{i = 1}^\infty \expect{\abs{C_i}}
\]
does not converge.  In fact, the expected value of $\abs{C_i}$ is $10$
because $\abs{C_i} =  10 \cdot 2^{i}$  with probability $2^{-i}$ and
otherwise is zero, so this sum rapidly approaches infinity.

Probability theory truly leads to this apparently paradoxical conclusion: a
game allowing an unbounded---even though always finite---number of ``fair''
moves may not be fair in the end.  In fact, our reasoning leads to an even
more startling conclusion: even against an \emph{unfair} wheel, as long as
there is some fixed positive probability of red on each spin, we are
certain to win $\$10$!

This is clearly a case where naive intuition is unreliable: we don't
expect to beat a fair game, and we do expect to lose when the odds are
against us.  Nevertheless, the ``paradox'' that in fact we always win by
bet-doubling cannot be denied.

But remember that from the start we chose to assume that no one goes
bankrupt while executing our bet-doubling strategy.  This assumption is
crucial, because the expected loss while waiting for the strategy to
produce its ten dollar profit is actually infinite!  So it's not
surprising, after all, that we arrived at an apparently paradoxical
conclusion from an unrealistic assumption.

This example also serves a warning that in making use of infinite
linearity of expectation, the convergence hypothesis which justifies it
had better be checked.

\textcolor{blue}{For WALD'S theorem see F02 ln11-12.}

\end{editingnotes}

\endinput
