\documentclass[problem]{mcs}

\begin{pcomments}
  \pcomment{CP_drug_confidence}
  \pcomment{from: S07.cp13w}
  \pcomment{revised ARM 12/01/15, still needs work}
\end{pcomments}

\pkeywords{
  probability
  confidence
  false_positive
}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% Problem starts here
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\begin{problem}
	
An \emph{International Journal of Pharmacological Testing} has a
policy of publishing drug trial results only if the conclusion holds
at the 95\% confidence level.  The editors and reviewers always
carefully check that any results they publish came from a drug trial
that genuinely deserved this level of confidence.  They are also
careful to check that trials whose results they publish have been
conducted independently of each other.

The editors of the Journal reason that under this policy, their
readership can be confident that at most 5\% of the published studies
will be mistaken.  Later, the editors are embarrassed---and
astonished---to learn that \emph{every one} of the 20 drug trial
results they published during the year was wrong.  The editors thought
that because the trials were conducted independently, the probability
of publishing 20 wrong results was negligible, namely, $(1/20)^{20} <
10^{-25}$.

Write a brief explanation to these befuddled editors explaining what's
wrong with their reasoning and how it could be that all 20 published
studies were wrong.

\hint xkcd comic: ``significant'' \href{http://xkcd.com/882/}{\texttt{xkcd.com/882/}}

\begin{solution}
An assertion of 95\% confidence means that if very many trials were
carried out, we expect that close to 95\% of the trials would yield a
correct conclusion.  So if a random sample of drug test results were
submitted for publication, then the editors would be correct in
expecting that only 5\% of them would be wrong.

But that's not what happens: not all the trials are written up and
submitted---only the interesting ones get submitted.  In this context,
95\% confidence is not very important---remember that a phoney
weatherman can predict sunshine in the Sahara desert with much more
than 95\% confidence.  What matters is the rate of false
positives---when an ineffective drug is declared effective.  We've
seen examples---the TB test in Section~\bref{sec:
  Confidence_v_Prob}---where the probability that a positive finding
is correct can be much lower than the confidence level of the test.

For example, there may be more than 400 worthless ``alternative''
drugs being tested by proponents who are genuinely honest, if
misguided.  When they conduct careful trials correct at a 95\%
confidence level, we can expect that in twenty of the 400 trials,
worthless---even damaging---drugs will falsely be declared effective.
The remaining 380 of the 400 trials---which correctly identified their
drug as ineffective---would not be submitted for publication because
the trials did not find a drug worth attending to.  But the twenty
trials that mistakenly showed positive results might well all be
submitted by honest researchers with no intention to mislead.

This is why, unless there is an explanation of \emph{why} a therapy
works, scientists and doctors usually doubt trials claiming to confirm
the efficacy of some far-fetched medication at a high confidence
level.  It is also why some medical regulatory agencies are pushing
for a new policy that results of \emph{all} clinical trials be
published, not just the ones that show positive results.  This policy
would validate the editors' original expectation that only 5\% of
their published studies would be mistaken, but then 95\% of the
published papers would be uninteresting.

\end{solution}

\end{problem}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% Problem ends here
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\endinput
