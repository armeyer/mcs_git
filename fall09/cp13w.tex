\documentclass[handout]{mcs}

\begin{document}

\inclassproblems{13, Wed.}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% Problems start here
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\pinput{CP_missing_card_probability}
\pinput{CP_conditional_prob_says_so_bug}
\pinput{CP_three_fair_coins}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% Problems end here
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\instatements{%\newpage

\section*{Appendix}

\subsection*{The Four Step Method}

This is a good approach to questions of the form, ``What is the
probability that -------?''  Intuition can be misleading, but
this formal approach gives the right answer every time.

\begin{enumerate}
\item Find the sample space.  (Use a tree diagram.)

\item Define events of interest.  (Mark leaves corresponding to these
events.)

\item Determine outcome probabilities:

\begin{enumerate}

\item Assign edge probabilities.

\item Compute outcome probabilities.  (Multiply along root-to-leaf
paths.)

\end{enumerate}

\item Compute event probabilities.  (Sum the probabilities of all
outcomes in the event.)

\end{enumerate}

\iffalse
\subsection{Probability Spaces}

  A countable \term{sample space}, $\sspace$, is a nonempty countable set.
  An element $w \in \sspace$ is called an \term{outcome}.  A subset of
  $\sspace$ is called an \term{event}.

  A \term{probability space} consists of a sample
  space, $\sspace$, and a function $\pr{}: \sspace\to [0,1]$, called
  the \term{probability function}, such that
\[
\sum_{w \in \sspace} \pr{w} = 1.
\]

For any event, $E \subseteq \sspace$, the \term{probability of $E$} is
defined to be the sum of the probabilities of the outcomes in $E$:
\[
\pr{E} \eqdef \sum_{w \in E} \pr{w}.
\]


\subsection{Sum Rule \& Union Bound}

Let $E_0,E_1,\dots$ be a (possibly infinite) sequence of events.  These
events are said to be \emph{pairwise disjoint} iff no outcome is in more
than one of these events ---formally: $E_i \intersect E_j = \emptyset$
whenever $i \neq j$.

If these events are pairwise disjoint, then
\begin{equation}
\pr{\lgunion_{n \geq 0} E_n} = \sum_{n \geq 0} \pr{E_n}.\tag{Disjoint Sum Rule}
\end{equation}

Even if they are not pairwise disjoint,
\begin{equation}
\pr{\lgunion_{n \geq 0} E_n} \leq \sum_{i \geq n} \pr{E_n}.\tag{Union Bound}
\end{equation}

\fi

\subsection*{Conditional Probabilitiy}
For events $E,F$ such that $\pr{F} \neq 0$, the \emph{conditional
    probability} of $E$ given $F$ is:
\[
\prcond{E}{F} \eqdef \frac{\pr{E \intersect F}}{\pr{F}}
\]

\subsection*{Law of Total Probability}

Here is the Law stated for three sets: suppose $E,F,G$ are pairwise
disjoint events, and
\[
A \subseteq E \union F \union G. 
\]
Then
\begin{align*}
\pr{A} & = \pr{A \intersect E}+ 
           \pr{A \intersect F}+ 
           \pr{A \intersect G}\\
 & = \prcond{A}{E} \cdot \pr{E} +
         \prcond{A}{F} \cdot \pr{F} +
         \prcond{A}{G} \cdot \pr{G}.
\end{align*}

\subsection*{Independence}

Events $E,F$ are \term{independent} iff
\[
\pr{E \intersect F} = \pr{E}\cdot \pr{F}.
\]

Events $E_1, E_2, \dots, E_n$ are \term{mutually independent} if and only
if
%
\[
\pr{\lgintersect_{i \in J} E_i} = \prod_{i \in J} \pr{E_i}
\]
for all subsets $J \subseteq \set{1,\dots,n}$.

Events $E_1, E_2, \dots,$ are $k$-way independent iff every $k$
of these events are mutually independent.
}

\end{document}
