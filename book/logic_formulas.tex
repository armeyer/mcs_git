\chapter{Logical Formulas}\label{logicform_chap}
%\label{propform_chap}

It is amazing that people manage to cope with all the ambiguities in the
English language.  Here are some sentences that illustrate the issue:
\begin{itemize}
\item ``You may have cake, or you may have ice cream.''
\item ``If pigs can fly, then your account won't get hacked.''
\item ``If you can solve any problem we come up with, then you get an
  \emph{A} for the course.''
\item ``Every American has a dream.''
\end{itemize}
What \emph{precisely} do these sentences mean?  Can you have both
cake and ice cream or must you choose just one dessert?  Pigs can't
fly, so does the second sentence say anything about the security of
your account?  If you can solve some problems we come up with, can you
get an \emph{A} for the course?  And if you can't solve a single one
of the problems, does it mean you can't get an \emph{A}?  Finally,
does the last sentence imply that all Americans have the same
dream---say of owning a house---or might different Americans have
different dreams---say, Eric dreams of designing a killer software
application, Tom of being a tennis champion, Albert of being able to
sing?

Some uncertainty is tolerable in normal conversation.  But when we need to
formulate ideas precisely---as in mathematics and programming---the
ambiguities inherent in everyday language can be a real problem.  We can't
hope to make an exact argument if we're not sure exactly what the
statements mean.  So before we start into mathematics, we need to
investigate the problem of how to talk about mathematics.

To get around the ambiguity of English, mathematicians have devised a
special language for talking about logical relationships.  This
language mostly uses ordinary English words and phrases such as
``or,'' ``implies,'' and ``for all.''  But mathematicians give these
words precise and unambiguous definitions which don't always match
common usage.  \iffalse A pitfall to watch out for is confusing
ordinary language with mathematical language that sounds ordinary but
isn't.\fi

Surprisingly, in the midst of learning the language of logic, we'll come
across the most important open problem in computer science---a problem
whose solution could change the world.

\section{Propositions from Propositions}\label{propform_sec}

In English, we can modify, combine, and relate propositions with words
such as ``not,'' ``and,'' ``or,'' ``implies,'' and ``if-then.''
For example, we can combine three propositions into one like this:
\begin{center}
\textbf{If} all humans are mortal \textbf{and} all Greeks are human,
\textbf{then} all Greeks are mortal.
\end{center}

For the next while, we won't be much concerned with the internals of
propositions---whether they involve mathematics or Greek mortality---but
rather with how propositions are combined and related.  So, we'll
frequently use variables such as $P$ and $Q$ in place of specific
propositions such as ``All humans are mortal'' and ``$2 + 3 = 5$.''  The
understanding is that these \emph{propositional variables},
\index{propositional variable|textbf} like propositions, can 
take on only the values \true~(true) and \false~(false).
Propositional variables are also called \emph{Boolean variables}
\index{Boolean variable|textbf} 
after their inventor, the nineteenth century mathematician
\index{Boole, George}
George---you guessed it---Boole.

\subsection{\QNOT, \QAND, and \QOR}
Mathematicians use the words $\QNOT$, $\QAND$ and $\QOR$
for operations that change or combine propositions.  The precise
mathematical meaning of these special words can be specified by
\emph{truth \index{truth table|textbf} tables}.  For example, if $P$ is a proposition,
then so is ``$\QNOT(P)$,'' and the truth value of the proposition
``$\QNOT(P)$'' is determined by the truth value of $P$ according to the
following truth table:

\[
\begin{array}{c|c}
P & \QNOT(P) \\ \hline
\true & \false \\
\false & \true \\
\end{array}
\]

The first row of the table indicates that when proposition $P$ is true,
the proposition ``$\QNOT(P)$'' is false.  The second line indicates that
when $P$ is false, ``$\QNOT(P)$'' is true.  This is probably what you
would expect.

In general, a truth table indicates the true/false value of a proposition
for each possible set of truth values for the variables.  For example, the
truth table for the proposition ``$P \QAND Q$'' has four lines, since
there are four settings of truth values for the two variables:

\[
\begin{array}{cc|c}
P & Q & P \QAND Q \\ \hline
\true & \true & \true \\
\true & \false & \false \\
\false & \true & \false \\
\false & \false & \false
\end{array}
\]

According to this table, the proposition ``$P \QAND Q$'' is true only when
$P$ and $Q$ are both true.  This is probably the way you ordinarily think
about the word ``and.''

There is a subtlety in the truth table for ``$P \QOR Q$'':

\[
\begin{array}{cc|c}
P & Q & P \QOR Q \\ \hline
\true & \true & \true \\
\true & \false & \true \\
\false & \true & \true \\
\false & \false & \false
\end{array}
\]

The first row of this table says that ``$P \QOR Q$'' is true even if
\textit{both} $P$ and $Q$ are true.  This isn't always the intended
meaning of ``or'' in everyday speech, but this is the standard definition
in mathematical writing.  So if a mathematician says, ``You may have cake,
or you may have ice cream,'' he means that you \textit{could} have both.

If you want to exclude the possibility of having both cake \emph{and}
ice cream, you should combine them with the \term{exclusive-or}
operation, $\QXOR$:

\[\begin{array}{cc|c}
P & Q & P \QXOR Q \\ \hline
\true & \true & \false \\
\true & \false & \true \\
\false & \true & \true \\
\false & \false & \false
\end{array}
\]

\subsection{If and Only If}

Mathematicians commonly join propositions in an additional way that
doesn't arise in ordinary speech.  The proposition ``$P$ \emph{if and
  only if} $Q$'' asserts that $P$ and $Q$ have the same truth value.
Either both are true or both are false.
\[
\begin{array}{cc|c}
P & Q & P \QIFF Q \\ \hline
\true & \true & \true \\
\true & \false & \false \\
\false & \true & \false \\
\false & \false & \true
\end{array}
\]
For example, the following if-and-only-if statement is true for every real
number $x$:
\[
x^2 - 4 \geq 0\ \QIFF\ \abs{x} \geq 2.
\]
For some values of $x$, \textit{both} inequalities are true.  For
other values of $x$, \textit{neither} inequality is true.  In every
case, however, the \QIFF\ proposition as a whole is true.

\subsection{\QIMPLIES}
\index{implication|textbf}

The combining operation whose technical meaning is least intuitive is
``implies.''  Here is its truth table, with the lines labeled so we
can refer to them later.

\[
\begin{array}{cc|cr}
    P  &   Q    & \parbox[b]{13ex}{$P \QIMP Q$} \\ \hline
\true  & \true  & \true & \text{(tt)}\\
\true  & \false & \false  & \text{(tf)}\\
\false & \true  & \true  & \text{(ft)}\\
\false & \false & \true  & \text{(ff)}
\end{array}
\]

The truth table for implications can be summarized in words as
follows:
\textbox{
An implication is true exactly when the if-part is false or the
then-part is true.
}
This sentence is worth remembering; a large fraction of all
mathematical statements are of the if-then form!

Let's experiment with this definition.  For example, is the following
proposition true or false?
\begin{quote}
  If Goldbach's Conjecture is true, then $x^2 \geq 0$ for every real
  number $x$.
\end{quote}
We already mentioned that no one knows whether Goldbach's Conjecture,
Proposition~\ref{Goldbach}, is true or false.  But that doesn't
prevent us from answering the question!  This proposition has the form
$P \QIMP Q$ where the \emph{hypothesis} $P$ is ``Goldbach's Conjecture
is true'' and the \emph{conclusion} $Q$ is ``$x^2 \geq 0$ for every
real number $x$.''  Since the conclusion is definitely true, we're on
either line~(tt) or line~(ft) of the truth table.  Either way, the
proposition as a whole is \textit{true}!

Now let's figure out the truth of one of our original examples:
\begin{quote}
  If pigs fly, then your account won't get hacked.
\end{quote}
Forget about pigs, we just need to figure out whether this proposition
is true or false.  Pigs do not fly, so we're on either line (ft) or
line (ff) of the truth table.  In both cases, the proposition is
\textit{true}!

\iffalse
In contrast, here's an example of a false implication:
\begin{center}
``If the moon shines white, then the moon is made of white cheddar.''
\end{center}
Yes, the moon shines white.  But, no, the moon is not made of white
cheddar cheese.  So we're on line (tf) of the truth table, and the
proposition is false.
\fi

\subsubsection{False Hypotheses}
\index{implication!false hypothesis}

This mathematical convention---that an implication as a whole is
considered true when its hypothesis is false---contrasts with common
cases where implications are supposed to have some \emph{causal}
connection between their hypotheses and conclusions.

For example, we could agree---or at least hope---that the following
statement is true:
\begin{quote}
  If you followed the security protocal, then your account won't get
  hacked.
\end{quote}
We regard this implication as unproblematical because of the clear
\emph{causal} connection between security protocols and account
hackability.

On the other hand, the statement:
\begin{quote}
If pigs could fly, then your account won't get hacked,
\end{quote}
would commonly be rejected as false---or at least silly---because
porcine aeronautics have nothing to do with your account security.
But mathematically, this implication counts as true.

It's important to accept the fact that mathematical implications
ignore causal connections.  This makes them a lot simpler than causal
implications, but useful nevertheless.  To illustrate this, suppose we
have a system specification which consists of a series of, say, a
dozen rules,\footnote{
  Problem~\ref{CP_file_system_functioning_normally} concerns just such
  a system.}
\begin{center}
\begin{tabular}{rll}
If & the  & system sensors are in condition $1$,\\
   & then & the system takes action $1$.\\
If & the  & system sensors are in condition $2$,\\
   & then &  the system takes action $2$.\\
   &      &\vdots\\
If & the  & system sensors are in condition $12$,\\
   & then & the system takes action $12$.\\
\end{tabular}
\end{center}

Letting $C_i$ be the proposition that the system sensors are in
condition $i$, and $A_i$ be the proposition that system takes action
$i$, the specification can be restated more concisely by the logical
formulas
\begin{align*}
C_1  &\QIMP A_1,\\
C_2  &\QIMP A_2,\\
     &\vdots\\
C_{12}&\QIMP A_{12}.
\end{align*}
Now the proposition that the system obeys the specification can be
nicely expressed as a single logical formula by combining the formulas
together with $\QAND$s::
\begin{equation}\label{qandspec}
[C_1 \QIMP A_1] \QAND [C_2 \QIMP A_2] \QAND \cdots \QAND [C_{12} \QIMP A_{12}].
\end{equation}

For example, suppose only conditions $C_2$ and $C_5$ are true, and the
system indeed takes the specified actions $A_2$ and $A_5$.  So in this
case, the system is behaving according to specification, and we
accordingly want formula~\eqref{qandspec} to come out true.  The
implications $C_2 \QIMP A_2$ and $C_5 \QIMP A_5$ are both true because
both their hypotheses and their conclusions are true.  But in order
for~\eqref{qandspec} to be true, we need all the other implications,
all of whose hypotheses are false, to be true.  This is exactly what
the rule for mathematical implications accomplishes.

\begin{problems}

\practiceproblems
\pinput{TP_variant_implies}
\pinput{TP_Basic_Propositions}

\classproblems
\pinput{CP_differentiable_implies_continuous}

\homeworkproblems
\pinput{PS_printout_binary_strings}
\pinput{PS_bogus_pqr_true}

\end{problems}


\section{Propositional Logic in Computer Programs}\label{propositions_in_programs_sec}

Propositions and logical connectives arise all the time in computer
programs.  For example, consider the following snippet, which could be
either C, C++, or Java:
\begin{tabbing}
\hspace{1in} \= \quad\quad \= \quad\quad \= \quad\quad \= \kill
\> \texttt{if ( x > 0 || (x <= 0 \&\& y > 100) )} \\
\> \> \vdots\\
\> \textit{(further instructions)}
\end{tabbing}
Java uses the symbol \texttt{||} for ``\QOR,'' and the
symbol \texttt{\&\&} for ``\QAND.''  The \textit{further instructions}
are carried out only if the proposition following the word \texttt{if}
is true.  On closer inspection, this big expression is built from two
simpler propositions.  Let $A$ be the proposition that \texttt{x > 0},
and let $B$ be the proposition that \texttt{y > 100}.  Then we can
rewrite the condition as
\begin{equation}\label{ANAB}
A \QOR (\QNOT(A) \QAND B).
\end{equation}

\subsection{Truth Table Calculation}\label{truthtablecalc}
A truth table calculation reveals that the more complicated
expression~\ref{ANAB} always has the same truth value as
\begin{equation}\label{AOB}
A \QOR B.
\end{equation}
We begin with a table with just the truth values of $A$ and $B$:
\[
\begin{array}{cc|ccccc|c}
A      & B      & A  & \QOR  & (\QNOT(A)& \QAND & B) & A \QOR  B \\ \hline
\true  & \true \\
\true  & \false\\
\false & \true \\
\false & \false                       
\end{array}
\]
These values are enough to fill in two more columns:
\[
\begin{array}{cc|ccccc|c}
A      & B      & A & \QOR  & (\QNOT(A) & \QAND & B) & A \QOR  B \\ \hline
\true  & \true  &   &       & \ \false   &       &    & \lgtrue \\
\true  & \false &   &       & \ \false   &       &    & \lgtrue \\
\false & \true  &   &       & \ \true    &       &    & \lgtrue \\
\false & \false &   &       & \ \true    &       &    & \lgfalse\\
\end{array}
\]
Now we have the values needed to fill in the \QAND\ column:
\[
\begin{array}{cc|ccccc|c}
A      & B      & A & \QOR  & (\QNOT(A) & \QAND   & B) & A \QOR  B \\ \hline
\true  & \true  &   &       & \ \false   &  \false &    & \lgtrue \\
\true  & \false &   &       & \ \false   &  \false &    & \lgtrue \\
\false & \true  &   &       & \ \true    &  \true  &    & \lgtrue \\
\false & \false &   &       & \ \true    &  \false &    & \lgfalse\\
\end{array}
\]
and this provides the values needed to fill in the remaining column for the first \QOR:
\[
\begin{array}{cc|ccccc|c}
A      & B      & A & \QOR     &(\QNOT(A) & \QAND   & B) & A \QOR  B \\ \hline
\true  & \true  &   & \lgtrue  & \false &    \false &    & \lgtrue \\
\true  & \false &   & \lgtrue  & \false &    \false &    & \lgtrue \\
\false & \true  &   & \lgtrue  & \true  &    \true  &    & \lgtrue \\
\false & \false &   & \lgfalse & \true  &    \false &    & \lgfalse\\
\end{array}
\]
Expressions whose truth values always match are called
\index{equivalence (logic)|textbf}
\emph{equivalent}.  Since the two emphasized columns of truth values
of the two expressions are the same, they are equivalent.  So we can
simplify the code snippet without changing the program's behavior by
replacing the complicated expression with an equivalent simpler one:
\begin{tabbing}
\hspace{1in} \= \quad\quad \= \quad\quad \= \quad\quad \= \kill
\> \texttt{if ( x > 0 || y > 100 )} \\
\> \> \vdots\\
\> \emph{(further instructions)}
\end{tabbing}

The equivalence of~\eqref{ANAB} and~\eqref{AOB} can also be confirmed
reasoning by cases:
\begin{itemize}
\item[$A$ is \true.]  An expression of the form $(\true \QOR
  \text{anything})$ is equivalent to \true.  Since $A$ is \true\
  both~\eqref{ANAB} and~\eqref{AOB} in this case are of this form, so they
  have the same truth value, namely, \true.

\item[$A$ is \false.]  An expression of the form $(\false \QOR
  \textit{anything})$ will have same truth value as \emph{anything}.
  Since $A$ is \false,~\eqref{AOB} has the same truth value as $B$.

   An expression of the form $(\true \QAND \textit{anything})$ is
   equivalent to \emph{anything}, as is any expression of the form
   $\false \QOR \emph{anything}$.  So in this case $A \QOR (\QNOT(A)
   \QAND B)$ is equivalent to $(\QNOT(A) \QAND B)$, which in turn is
   equivalent to $B$.

   Therefore both~\eqref{ANAB} and~\eqref{AOB} will have the same truth
   value in this case, namely, the value of $B$.
\end{itemize}

Simplifying logical expressions has real practical importance in
computer science.  Expression simplification in programs like the one
above can make a program easier to read and understand. Simplified
programs may also run faster, since they require fewer operations.  In
hardware, simplifying expressions can decrease the number of logic
gates on a chip because digital circuits can be described by
logical formulas (see Problems~\ref{CP_binary_adder_logic}
and~\ref{PS_faster_adder_logic}).  Minimizing the logical formulas
corresponds to reducing the number of gates in the circuit.  The
payoff of gate minimization is potentially enormous: a chip with fewer
gates is smaller, consumes less power, has a lower defect rate, and is
cheaper to manufacture.

\subsection{Cryptic Notation}
Java uses symbols like ``$\&\&$'' and ``$||$'' in place of \QAND\ and
\QOR.  Circuit designers use ``$\cdot$'' and ``$+$,'' and actually refer
to \QAND\ as a product and \QOR\ as a sum.  Mathematicians use still
other symbols, given in the table below.
\begin{center}
\begin{tabular}{ll}
\textbf{English} & \textbf{Symbolic Notation} \\[1ex]
$\QNOT(P)$ & $\neg P$ \quad (alternatively, $\bar{P}$) \\
$P \QAND Q$ & $P \land Q$ \\
$P \QOR Q$ & $P \lor Q$ \\
$P \QIMP Q$ & $P \implies Q$ \\
if $P$ then $Q$ & $P \implies Q$ \\
$P \QIFF Q$ & $P \iff Q$\\
$P \QXOR Q$ & $P \oplus Q$
\end{tabular}
\end{center}
For example, using this notation, ``If $P \QAND \QNOT(Q)$, then $R$''
would be written:
\[
    (P \land \bar{Q}) \implies R.
\]

The mathematical notation is concise but cryptic.  Words such as
``$\QAND$'' and ``$\QOR$'' are easier to remember and won't get
confused with operations on numbers.  We will often use $\bar{P}$ as
an abbreviation for $\QNOT(P)$, but aside from that, we mostly stick
to the words---except when formulas would otherwise run off the page.

\begin{problems}
\classproblems
\pinput{CP_binary_adder_logic}

\homeworkproblems
\pinput{PS_faster_adder_logic}

\examproblems
\pinput{MQ_truth_table_case_reasoning}
\pinput{FP_AND_circuit}
\end{problems}

\section{Equivalence and Validity}\label{equiv_valid_sec}
%\label{sec:logical_equivalence}

\subsection{Implications and Contrapositives}\label{implication_sec}
Do these two sentences say the same thing?
\begin{center}
If I am hungry, then I am grumpy. \\
If I am not grumpy, then I am not hungry.
\end{center}
We can settle the issue by recasting both sentences in terms of
propositional logic.  Let $P$ be the proposition ``I am hungry'' and $Q$
be ``I am grumpy.''  The first sentence says ``$P \QIMPLIES Q$'' and the
second says ``$\QNOT(Q) \QIMP \QNOT(P)$.''  Once more, we can compare
these two statements in a truth table:
\[
\begin{array}{c|c|c|ccc}
   P   &   Q    & (P  \QIMP  Q) & (\QNOT(Q) & \QIMP & \QNOT(P)) \\ \hline
\true  & \true  &     \lgtrue   &  \false   & \lgtrue  &  \false\\
\true  & \false &     \lgfalse  &  \true    & \lgfalse &  \false\\
\false & \true  &     \lgtrue   &  \false   & \lgtrue  &  \true \\
\false & \false &     \lgtrue   &  \true    & \lgtrue  &  \true \\
\end{array}
\]
Sure enough, the highlighted columns showing the truth values of these two
statements are the same.  A statement of the form ``$\QNOT (Q) \QIMPLIES
\QNOT (P)$'' is called the \term{contrapositive} of the implication ``$P
\QIMPLIES Q$.''  The truth table shows that an implication and its
contrapositive are equivalent---they are just different ways of saying
the same thing.

In contrast, the \term{converse} of ``$P \QIMPLIES Q$'' is the statement
``$Q \QIMPLIES P$.''  The converse to our example is:
\begin{center}
If I am grumpy, then I am hungry.
\end{center}
This sounds like a rather different contention, and a truth table
confirms this suspicion:
\[
\begin{array}{c|c|c|c}
P & Q &
    P \QIMPLIES Q &
    Q \QIMPLIES P \\ \hline
\true & \true & \lgtrue & \lgtrue \\
\true & \false & \lgfalse & \lgtrue \\
\false & \true & \lgtrue & \lgfalse \\
\false & \false & \lgtrue & \lgtrue
\end{array}
\]
Now the highlighted columns differ in the second and third row, confirming
that an implication is generally \textit{not} equivalent to its converse.

One final relationship: an implication and its converse together are
equivalent to an iff statement, specifically, to these two statements
together.  For example,
\begin{center}
If I am grumpy then I am hungry, and if I am hungry then I am grumpy.
\end{center}
are equivalent to the single statement:
\begin{center}
I am grumpy iff I am hungry.
\end{center}
Once again, we can verify this with a truth table.  

\iffalse
We begin with a table with just the truth values of $P$ and $Q$:
\[
\begin{array}{c|c|ccc|c}
P & Q & (P \QIMP Q) &\QAND & (Q  \QIMP  P) & P \QIFF Q \\
\hline
\true  &  \true  &&&&\\
\true  &  \false &&&&\\
\false &  \true  &&&&\\
\false &  \false &&&&
\end{array}
\]
These truth values are enough to fill in three more columns:
\[
\begin{array}{c|c|ccc|c}
P & Q & (P \QIMP Q) &\QAND & (Q  \QIMP  P) & P \QIFF Q \\
\hline
\true  &  \true  &\true  &&\true & \lgtrue \\
\true  &  \false &\false &&\true & \lgfalse\\
\false &  \true  &\true  &&\false& \lgfalse\\
\false &  \false &\true  &&\true & \lgtrue 
\end{array}
\]
Finally, now using the first two of the filled in columns, we can fill in
the fourth column:
\fi

\[
\begin{array}{c|c|ccc|c}
P & Q & (P \QIMP Q) &\QAND & (Q  \QIMP  P) & P \QIFF Q \\
\hline
\true  &  \true  &\true  &\lgtrue &\true & \lgtrue \\
\true  &  \false &\false &\lgfalse&\true & \lgfalse\\
\false &  \true  &\true  &\lgfalse&\false& \lgfalse\\
\false &  \false &\true  &\lgtrue &\true & \lgtrue
\end{array}
\]
The fourth column giving the truth values of 
\[
(P \QIMP Q) \QAND (Q \QIMP P)
\]
is the same as the sixth column giving the truth values of $P \QIFF
Q$, which confirms that the \QAND\ of the implications is equivalent
to the \QIFF\ statement.

\subsection{Validity and Satisfiability}
A \emph{valid} \index{validity (logic)|textbf} 
formula is one which is \emph{always} true, no matter
what truth values its variables may have.  The simplest example is
\[
P \QOR \QNOT(P).
\]

You can think about valid formulas as capturing fundamental logical
truths.  For example, a property of implication that we take for
granted is that if one statement implies a second one, and the second
one implies a third, then the first implies the third.  The following
valid formula confirms the truth of this property of implication.
\[
[(P \QIMP Q) \QAND (Q \QIMP R)] \QIMP (P \QIMP R).
\]

Equivalence of formulas is really a special case of validity.  Namely,
statements $F$ and $G$ are equivalent precisely when the statement $(F
\QIFF G)$ is valid.  For example, the equivalence of the
expressions~\eqref{AOB} and~\eqref{ANAB} means that
\[
(A \QOR B) \QIFF (A \QOR (\QNOT(A) \QAND B))
\]
is valid.  Of course, validity can also be viewed as an aspect of
equivalence.  Namely, a formula is valid iff it is equivalent
to \true.

A \emph{satisfiable}
\index{satisfiability|textbf} 
formula is one which can \emph{sometimes} be
true---that is, there is some assignment of truth values to its
variables that makes it true.  One way satisfiability comes up is when
there are a collection of system specifications.  The job of the
system designer is to come up with a system that follows all the
specs.  This means that the \QAND\ of all the specs must be
satisfiable or the designer's job will be impossible (see
Problem~\ref{CP_file_system_functioning_normally}).

There is also a close relationship between validity and
satisfiability: a statement $P$ is satisfiable iff its
negation $\QNOT(P)$ is \emph{not} valid.

\begin{problems}
\practiceproblems
\pinput{TP_not_xor}
\pinput{TP_valid_sat_multiple_choice}
\pinput{MQ_XOR_truthtable}
\pinput{TP_PorQorR_equiv}
\pinput{TP_truth_table_for_distributive_law}

\examproblems
\pinput{FP_validity_by_cases}

\classproblems
\pinput{CP_valid_vs_satisfiable}
\pinput{CP_file_system_functioning_normally}
\end{problems}


\section{The Algebra of Propositions}\label{prop_algebra_sec}

\subsection{Propositions in Normal Form}\label{normal_form_sec}
Every propositional formula is equivalent to a ``sum-of-products'' or
\term{disjunctive normal form} (DNF). \index{DNF|see{disjunctive normal form}}

More precisely, a propositional variable $A$ or its negation $bar(A)$
is called a \term{literal}, and an $\QAND$ of literals involving
\emph{distinct} variables is called an \term{\QAND-clause} or
\term{\QAND-of-literals}.  For example,
\[
A \QAND \bar{B} \QAND \bar{C}
\]
is an \QAND-clause, but $A \QAND B \QAND \bar{B} \QAND C$ is not
because $B$ appears twice.  Finally, a DNF is an \QOR\ of \QAND-clauses such as
\begin{equation}\label{ANBOANC}
(A \QAND B) \QOR (A \QAND C).
\end{equation}

You can read a DNF for any propositional formula directly
from its truth table.  For example, the formula
\begin{equation}\label{ANBRC}
A \QAND (B \QOR C)
\end{equation}
has truth table:
\[\begin{array}{c|c|c|ccc}
A      & B      & C       & A & \QAND & (B \QOR C)\label{ANBRCTT}\\
\hline \true  & \true  & \true   &   &  \true\\
\true  & \true  & \false  &   &  \true\\
\true  & \false & \true   &   &  \true\\
\true  & \false & \false  &   &  \false\\
\false & \true  & \true   &   &  \false\\
\false & \true  & \false  &   &  \false\\
\false & \false & \true   &   &  \false\\
\false & \false & \false  &   &  \false
\end{array}\]
The formula~\eqref{ANBRC} is true in the first row when $A$, $B$ and
$C$ are all true, that is, where $A \QAND B \QAND C$ is true.  It is
also true in the second row where $A \QAND B \QAND \bar{C}$ is true,
and in the third row when $A \QAND \bar{B} \QAND C$ is true, and
that's all.  So~\eqref{ANBRC} is true exactly when
\begin{equation}\label{ABCDNF}
(A \QAND B \QAND C) \QOR (A \QAND B \QAND \bar{C}) \QOR
  (A \QAND \bar{B} \QAND C)
\end{equation}
is true.

The expression~\eqref{ABCDNF} is a DNF where each \QAND-clause
actually includes a literal for \emph{every one} of the variables in
the whole formula.  We'll call such a formula a \term{full DNF}. \index{full
  DNF|see{disjunctive normal form}}
\index{full disjunctive normal form|see{disjunctive normal form}}

A DNF formula can often be simplified into a smaller DNF For example,
the DNF~\eqref{ABCDNF} further simplifies to the equivalent
DNF~\eqref{ANBOANC} above.

Applying the same reasoning to the \false\ entries of a truth table
yields a \term{conjunctive normal form} (CNF)
\index{CNF|see{conjunctive normal form}} for any formula---an
\QAND\ of \term{\QOR-clauses}, where an \QOR-clause is an
\term{$\QOR$-of-literals} from different variables.

For example, formula~\eqref{ANBRC} is false in the fourth row of its
truth table~\eqref{ANBRCTT} where $A$ is \true, $B$ is \false\ and $C$
is \false.  But this is exactly the one row where the \QOR-clause
$(\bar{A} \QOR B \QOR C)$ is \false!  Likewise,~\eqref{ANBRC} is
false in the fifth row, which is exactly where $(A \QOR \bar{B} \QOR
\bar{C})$ is \false.  This means that~\eqref{ANBRC} will be
\false\ whenever the \QAND\ of these two \QOR-clauses is false.
\iffalse $(\bar{A} \QOR B \QOR C) \QAND (A \QOR \bar{B} \QOR \bar{C})$
is \false.  \fi Continuing in this way with the \QOR-clauses
corresponding to the remaining three rows where~\eqref{ANBRC} is
false, we get a CNF that is equivalent to~\eqref{ANBRC}, namely,
\[\begin{array}{l}
(\bar{A} \QOR B \QOR C)  \QAND (A \QOR \bar{B} \QOR \bar{C}) 
        \QAND (A \QOR \bar{B} \QOR C)  \QAND \\
(A \QOR B \QOR \bar{C}) \QAND (A \QOR B \QOR C)
\end{array}\]
Again, each \QOR-clause includes a literal for every one of the
variables, that is, it is a \emph{full} CNF
\index{full conjunctive normal form!conjunctive normal form|textbf}
\index{full CNF!conjunctive normal form|textbf}

The methods above can be applied to any truth table, which implies
\begin{theorem}
Every propositional formula is equivalent to both a full disjunctive
normal form and a full conjunctive normal form.
\end{theorem}

\subsection{Proving Equivalences}\label{propositional_equivalences_sec}
A check of equivalence
\index{equivalence (logic)} 
or validity by truth table runs out of steam
pretty quickly: a proposition with $n$ variables has a truth table
with $2^n$ lines, so the effort required to check a proposition
grows exponentially with the number of variables.  For a
proposition with just 30 variables, that's already over a billion
lines to check!

An alternative approach that \emph{sometimes} helps is to use algebra
to prove equivalence.  A lot of different operators may appear in a
propositional formula, so a useful first step is to get rid of all but
three: \QAND, \QOR\ and \QNOT.  This is easy because each of the
operators is equivalent to a simple formula using only these three.
For example, $A \QIMPLIES B$ is equivalent to $\QNOT(A) \QOR B$.
Formulas defining the remaining operators using only $\QAND, \QOR$ and
$\QNOT$ are left to Problem~\ref{TP_only_and_or_not}.

We list below a bunch of equivalence axioms
\index{axiom!equivalence axioms|see{equivalence (logic)}}
\index{equivalence (logic)!axioms} 
with the symbol ``$\corresp$'' between equivalent formulas.  These axioms are
important because they are all that's needed to prove every possible
equivalence.  We'll start with some equivalences for \QAND's that look
like the familiar ones for multiplication of numbers:
\begin{align}
A \QAND B           &\corresp B \QAND A
         & \text{(commutativity of \QAND)}\label{commutqand}\\
(A \QAND B)\QAND C  & \corresp A \QAND (B \QAND C)
         & \text{(associativity of \QAND)}\label{assocqand}\\
\true \QAND A           &\corresp A
         & \text{(identity for \QAND)}\notag\\
\false \QAND A          &\corresp \false
         & \text{(zero for \QAND)}\notag\\
A \QAND (B \QOR C) & \corresp (A \QAND B) \QOR (A \QAND C)
         & \text{(distributivity of \QAND\ over \QOR)}\label{distand}
\end{align}

Associativity~\eqref{assocqand} justifies writing $A \QAND B \QAND C$
without specifying whether it is parenthesized as $A \QAND (B \QAND
C)$ or $(A \QAND B) \QAND C$.  Both ways of inserting parentheses
yield equivalent formulas.

\iffalse

\begin{theorem}\label{thm:distribute-and-or}[Distributive Law of $\QAND$ over $\QOR$]
\index{Distributive Law!$\QAND$ over $\QOR$|textbf}
\[
A \QAND (B \QOR C)\text{ is equivalent to } (A \QAND B) \QOR (A \QAND C).
\]
\end{theorem}
Theorem~\ref{thm:distribute-and-or} is called a \emph{distributive law}
because of its resemblance to the distributivity of products
over sums in arithmetic.
\fi

Unlike arithmetic rules for numbers, there is also a distributivity
law for ``sums'' over ``products:''
\begin{align}
A \QOR (B \QAND C) & \corresp (A \QOR B) \QAND (A \QOR C)
             & \text{(distributivity of \QOR\ over \QAND)}\label{distor}
\end{align}

\iffalse
Similarly, we have (Problem~\ref{TP_truth_table_for_distributive_law}):
\begin{theorem}\label{thm:distribute-or-and}[Distributive Law of $\QOR$ over $\QAND$]
\[
A \QOR (B \QAND C)\text{ is equivalent to } (A \QOR B) \QAND (A \QOR
C).
\]
\end{theorem}
\index{Distributive Law!$\QOR$ over $\QAND$|textbf}
Note the contrast between Theorem~\ref{thm:distribute-or-and} and
arithmetic, where sums do not distribute over products.
\fi

Three more axioms that don't directly correspond to number properties
are
\begin{align}
A \QAND A       &\corresp A
         & \text{(idempotence for \QAND)}\notag\\
A \QAND \bar{A} & \corresp \false
         & \text{(contradiction for \QAND)}\label{QAND-contra}\\
\QNOT(\bar{A})  & \corresp A
         & \text{(double negation)}\label{double-neg}
\end{align}

There are a corresponding set of equivalences for $\QOR$ which we
won't bother to list, except for validity rule~\eqref{QOR-true} for
$\QOR$:
\begin{align}
A \QOR \bar{A} & \corresp \true  & \text{(validity for \QOR)}\label{QOR-true}
\end{align}

\iffalse
There is also a familiar rule connecting \QAND\ and \QOR:
\begin{align}
\lefteqn{A \QAND (B \QOR C)}\notag\\
 & \corresp (A \QAND B) \QOR (A \QAND C) &\qquad \text{(distributivity
  of \QAND\ over \QOR)}\label{qand-distributivity}
\end{align}
\fi

Finally, there are \term{De Morgan's Laws}\index{De Morgan's
  Laws|seealso{equivalence (logic)}} which explain how to distribute
\QNOT's over \QAND's and \QOR's:
\begin{align}
\QNOT(A \QAND B) &\corresp \bar{A} \QOR \bar{B} & \text{(De Morgan for \QAND)} \label{DeMQAND}\\
\QNOT(A \QOR B) &\corresp \bar{A} \QAND \bar{B} & \text{(De Morgan for \QOR)}
\label{DeMQOR} 
\end{align}
All of these axioms can be verified easily with truth tables.

These axioms are all that's needed to convert any formula to a full
DNF.  We can illustrate how they work by applying them to turn the
negation of formula~\eqref{ANBRC},
\begin{equation}\label{NANBRC}
\QNOT((A \QAND B) \QOR (A \QAND C)).
\end{equation}
into a full DNF.

We start by applying De Morgan's Law for \QOR~\eqref{DeMQOR}
to~\eqref{NANBRC} in order to move the \QNOT\ deeper into the formula.
This gives
\[
\QNOT(A \QAND B) \QAND \QNOT(A \QAND C).
\]
Now applying De Morgan's Law for \QAND~\eqref{DeMQAND} to the two
innermost \QAND-terms, gives
\begin{equation}\label{NAONBANAONC}
(\bar{A} \QOR \bar{B}) \QAND (\bar{A} \QOR \bar{C}).
\end{equation}
At this point \QNOT\ only applies to variables, and we won't need
De Morgan's Laws any further.

Now we will repeatedly apply~\eqref{distand}, distributivity of
\QAND\ over \QOR, to turn~\eqref{NAONBANAONC} into a DNF.
To start, we'll distribute $(\bar{A} \QOR \bar{B})$ over \QAND\ to get
\[
((\bar{A} \QOR \bar{B}) \QAND \bar{A}) \QOR ((\bar{A} \QOR \bar{B}) \QAND \bar{C}).
\]
Using distributivity over both \QAND's we get
\[
((\bar{A} \QAND \bar{A}) \QOR (\bar{B} \QAND \bar{A})) \QOR 
((\bar{A} \QAND \bar{C}) \QOR (\bar{B} \QAND \bar{C})).
\]
By the way, we've implicitly used commutativity~\eqref{commutqand}
here to justify distributing over an \QAND\ from the right.  Now
applying idempotence to remove the duplicate occurrence of $\bar{A}$ we
get
\[
(\bar{A} \QOR (\bar{B} \QAND \bar{A})) \QOR 
((\bar{A} \QAND \bar{C}) \QOR (\bar{B} \QAND \bar{C})).
\]
Associativity of $QOR$ now allows dropping the parentheses grouping
the \QAND-clauses to yield the following DNF
for~\eqref{NANBRC}:
\begin{equation}\label{DFNAONBANAONC}
\bar{A} \QOR
(\bar{B} \QAND \bar{A}) \QOR 
(\bar{A} \QAND \bar{C}) \QOR
(\bar{B} \QAND \bar{C}).
\end{equation}

The penultimate step is to turn this DNF into a full DNF.  This can be
done separately for ech \QAND-clause.  We'll illustrate how using the
second \QAND-clause $(\bar{B} \QAND \bar{A})$.  This clause needs to
mention $C$ to be in full form.  To introduce $C$, we use validity for
$\QOR$ and identity for $\QAND$ to conclude that
\[
(\bar{B} \QAND \bar{A}) \corresp (\bar{B} \QAND \bar{A}) \QAND (C \QOR \bar{C}).
\]

\iffalse
$\bar{B} \QAND \bar{A}$
is equivalent to
\[
(\bar{B} \QAND \bar{A}) \QAND (C \QOR \bar{C}).
\]
\fi

Now distributing $(\bar{B} \QAND \bar{A})$ over the $\QOR$ in $(C \QOR
\bar{C})$ yields the full DNF
\[
(\bar{B} \QAND \bar{A} \QAND C) \QOR
(\bar{B} \QAND \bar{A} \QAND \bar{C}).
\]
Doing the same thing to the other \QAND-clauses in~\eqref{DFNAONBANAONC}
finally gives a full DNF for~\eqref{ANBRC}:
\[\begin{array}{l}
(\bar{A} \QAND B \QAND C) \QOR (\bar{A} \QAND B \QAND \bar{C})\ \QOR\\
(\bar{A} \QAND \bar{B} \QAND C) \QOR  (\bar{A} \QAND \bar{B} \QAND \bar{C})\ \QOR\\
(\bar{B} \QAND \bar{A} \QAND C) \QOR  (\bar{B} \QAND \bar{A} \QAND \bar{C})\ \QOR\\
(\bar{A} \QAND \bar{C} \QAND B) \QOR  (\bar{A} \QAND \bar{C} \QAND \bar{B})\ \QOR\\
(\bar{B} \QAND \bar{C} \QAND A) \QOR  (\bar{B} \QAND \bar{C} \QAND \bar{A}).
\end{array}\]

The final step is to use commutativity to sort the variables within
the \QAND-clauses and then sort the \QAND-clauses themselves, followed
by applying \QOR-idempotence as needed to remove duplicate
\QAND-clauses.  This finally yields a sorted full DNF
without duplicates which is called a \emph{canonical DNF}
\index{canonical DNF!disjunctive normal form|textbf}:
\[\begin{array}{l}
(A \QAND \bar{B} \QAND \bar{C})\ \QOR\\
(\bar{A} \QAND B \QAND C)\ \QOR\\
(\bar{A} \QAND B \QAND \bar{C})\ \QOR\\
(\bar{A} \QAND \bar{B} \QAND C)\ \QOR\\
(\bar{A} \QAND \bar{B} \QAND \bar{C}).
\end{array}\]

This example illustrates the general strategy for applying the axioms
above to any given propositional formala to derive an equivalent
canonical DNF.  This proves:
\begin{theorem}\label{completeDNF}
Using the equivalences listed above, any propositional formula can be
proved equivalent to a canonical form.
\end{theorem}

What has this got to do with equivalence?  That's easy: to prove that
two formulas are equivalent, convert them both to canonical forms over
the set of variables that appear in at least one of the
formulas---call these the \term{combined variables}.  Now if two
formulas are equivalent to the same canonical form then the formula
are certainly equivalent.  Conversely, the way we read off a full
disjunctive normal form from a truth table actually yields a canonical
form.  So if two formulas are equivalent, they will have the same
truth table over the combined variables, and therefore they will have
the same canonical form.  This proves
\begin{theorem}[Completeness of the propositional equivalence axioms]
\label{complete_equivalence}
Two propositional formula are equivalent iff they can be proved
equivalent using the equivalence axioms listed above.
\end{theorem}

Notice that the same approach could be taken used CNF instead of DNF
canonical forms.

The benefit of the axioms is that they allow some ingenious proofs of
equivalence that may involve much less effort than the truth table
method.  Moreover, Theorem~\ref{complete_equivalence} reassures us
that the axioms are guaranteed to provide a proof of every
equivalence, which is a great punchline for this section.

But we don't want to mislead you: the guaranteed proof involves
deriving canonical forms, and canonical forms are essentially copies
of truth tables.  There is no reason to expect algebraic proofs of
equivalence to be any easier in general than conversion to canonical
form, which means algebraic proofs will generally be no easier than
using truth tables.

\begin{problems}
\practiceproblems
\pinput{TP_only_and_or_not}
\pinput{TP_CNF_and_DNF}

\classproblems
\pinput{CP_NOR_operator}
\pinput{CP_CNF_from_DNF}
\pinput{CP_read_off_CNF}

\homeworkproblems
\pinput{PS_find_dnf}

\examproblems
%\pinput{FP_truth_table}  show after S18 final

\end{problems}

\section{The SAT Problem}\label{SAT_sec}
Determining whether or not a more complicated proposition is
satisfiable is not so easy.  How about this one?
\[
(P \QOR Q \QOR R) \QAND (\bar P \QOR \bar Q)
                  \QAND (\bar P \QOR \bar R)
                  \QAND (\bar R \QOR \bar Q)
\]

The general problem of deciding whether a proposition is
satisfiable is called
\index{satisfiability!SAT|textbf} 
\emph{SAT}.  One approach to SAT is to construct a truth table 
and check whether or not a $\true$ ever
appears, but as with testing validity, this approach quickly bogs down
for formulas with many variables because truth tables grow
exponentially with the number of variables.

Is there a more efficient solution to SAT?  In particular, is
there some brilliant procedure that determines SAT in a number of steps
that grows \emph{polynomially}---like
$n^2$ or $n^{14}$---instead of \emph{\index{exponential
    growth}{exponentially}}---$2^n$---whether any given proposition of size
$n$ is satisfiable or not?  No one knows.  And an awful lot hangs on
the answer.

The general definition of an ``efficient'' procedure is one that runs
in \term{polynomial time}, that is, that runs in a number of basic
steps bounded by a polynomial in $s$, where $s$ is the size of an
input.  It turns out that an efficient solution to SAT would
immediately imply efficient solutions to many other important problems
involving scheduling, routing, resource allocation, and circuit
verification across multiple disciplines including programming,
algebra, finance, and political theory.  This would be wonderful, but
there would also be worldwide chaos because decrypting coded messages
would become an easy task, so online financial transactions would be
insecure and secret communications could be read by everyone.  Why
this would happen is explained in Section~\ref{SAT_RSA_sec}.

Of course, the situation is the same for validity \index{validity
  (logic)} checking, since you can check for validity by checking for
satisfiability of a negated formula.  This also explains why the
simplification of formulas mentioned in
Section~\ref{propositions_in_programs_sec} would be hard---validity
testing is a special case of determining if a formula simplifies to
\true.

Recently there has been exciting progress on \emph{SAT-solvers}
\index{satisfiability!SAT!SAT-solver} for practical applications like
digital circuit verification.  These programs find satisfying
assignments with amazing efficiency even for formulas with millions of
variables.  Unfortunately, it's hard to predict which kind of formulas
are amenable to SAT-solver methods, and for formulas that are
\emph{un}satisfiable, SAT-solvers are generally much less effective.

So no one has a good idea how to solve SAT in polynomial time, or how
to prove that it can't be done---researchers are completely stuck.
The problem of determining whether or not SAT has a polynomial time
solution is known as the ``\textbf{P} vs.\ \textbf{NP}''
\index{P vs NP@\textbf{P} vs. \textbf{NP}}
problem.\footnote{\textbf{P} stands for problems whose instances can
  be solved in time that grows polynomially with the size of the
  instance.  \textbf{NP} stands for \emph{\textbf{n}ondeterministtic \textbf{p}olynomial time},
  but we'll leave an explanation of what that is to texts on the
  theory of computational complexity.}  It is the outstanding
unanswered question in theoretical computer science.  It is also one
of the seven \href{http://www.claymath.org/millennium/}{Millenium
  Problems}: the Clay Institute will award you \$1,000,000 if you
solve the \textbf{P} vs.\ \textbf{NP} problem.

\begin{problems}

%\classproblems

\homeworkproblems
\pinput{CP_sat_formulas_vs_circuits}
\pinput{PS_equisatisfiable_3CNF}
%\pinput{PS_sat_formulas_vs_circuits}

\end{problems}

\section{Predicate Formulas}\label{predicate_sec}
%\label{logic_chap}

\subsection{Quantifiers}\label{quantifier_sec}
The ``for all'' notation $\forall$ has already made an early
appearance in Section~\ref{prop_sec}.  For example, the predicate
\[
\text{``$x^2 \geq 0$''}
\]
is always true when $x$ is a real number.  That is,
\[
\forall x \in \reals.\, x^2 \geq 0
\]
is a true statement.  On the other hand, the predicate
\[
\text{``$5x^2 - 7 = 0$''}
\]
is only sometimes true; specifically, when $x = \pm \sqrt{7/5}$.
There is a ``there exists'' notation $\exists$ to indicate that a
predicate is true for at least one, but not necessarily all objects.
So 
\[
\exists x \in \reals.\, 5x^2 - 7 = 0
\]
is true, while
\[
\forall x \in \reals.\, 5x^2 - 7 = 0
\]
is not true.

There are several ways to express the notions of ``always true'' and
``sometimes true'' in English.  The table below gives some general
formats on the left and specific examples using those formats on the
right.  You can expect to see such phrases hundreds of times in
mathematical writing!
\begin{center}
\begin{tabular}{ll}
\multicolumn{2}{c}{\textbf{Always True}} \\[1ex]
For all $x \in D$, $P(x)$ is true. & For all $x \in \reals$, $x^2 \geq 0$. \\
$P(x)$ is true for every $x$ in the set $D$. & $x^2 \geq 0$ for every $x \in \reals$. \\[2ex]
\multicolumn{2}{c}{\textbf{Sometimes True}} \\[1ex]
There is an $x \in D$ such that $P(x)$ is true. & There is an $x \in \reals$ such that $5x^2 - 7 = 0$.\\
$P(x)$ is true for some $x$ in the set $D$. & $5x^2 - 7 = 0$ for some $x \in \reals$.\\
$P(x)$ is true for at least one $x \in D$. & $5x^2-7=0$ for at least one $x \in \reals$.
\end{tabular}
\end{center}

All these sentences ``quantify'' how often the predicate is true.
Specifically, an assertion that a predicate is always true is called a
\term{universal quantification}, and an assertion that a predicate is
sometimes true is an \term{existential quantification}.  Sometimes the
English sentences are unclear with respect to quantification:
\begin{align}
  \text{If you can solve any problem we come up with,}\notag\\
  \text{then you get an \emph{A} for the course.}\label{solvegetA}
\end{align}
The phrase ``you can solve any problem we can come up with'' could
reasonably be interpreted as either a universal or existential
quantification:
\begin{equation}\label{solve_every}
\text{you can solve \emph{every} problem we come up with,}
\end{equation}
or maybe
\begin{equation}\label{solve_atleastone}
\text{you can solve \emph{at least one} problem we come up with.}
\end{equation}

\iffalse
In any case, notice that this quantified phrase appears inside a
larger if-then statement.  This is quite normal; quantified statements
are themselves propositions and can be combined
with \QAND, \QOR, \QIMPLIES, etc., just like any other proposition.
\fi
To be precise, let $\probs$ be the set of problems we come up with,
$\solves(x)$ be the predicate ``You can solve problem $x$,'' and $G$
be the proposition, ``You get an \emph{A} for the course.''  Then the
two different interpretations of~\eqref{solvegetA}
can be written as follows:
\begin{align*}
(\forall x \in \probs.\, & \solves(x)) \QIMP G,
     & \text{for~\eqref{solve_every}},\\
(\exists x \in \probs.\, & \solves(x)) \QIMP G.
     & \text{for~\eqref{solve_atleastone}}.
\end{align*}

\subsection{Mixing Quantifiers}

Many mathematical statements involve several quantifiers.  For
example, we already described
\begin{quote}
 \idx{Goldbach's Conjecture}~\ref{Goldbach}: Every even integer
 greater than 2 is the sum of two primes.
\end{quote}
Let's write this out in more detail to be precise about the
quantification:
\begin{quote}
For every even integer $n$ greater than 2,
there exist primes $p$ and $q$ such that $n = p + q$.
\end{quote}
Let $\even$ be the set of even integers greater than 2, and let $\primes$ be the
set of primes.  Then we can write Goldbach's Conjecture in logic
notation as follows:
\[
\underbrace{\forall n \in \even}_{\substack
    {\text{for every even} \\
     \text{integer $n > 2$}}}\
\underbrace{\exists p \in \primes\ \exists q \in \primes.}_{\substack
    {\text{there exist primes} \\
     \text{$p$ and $q$ such that}}}
\ n = p + q.
\]

\subsection{Order of Quantifiers}

Swapping the order of different kinds of quantifiers (existential or
universal) usually changes the meaning of a proposition.  For example,
let's return to one of our initial, confusing statements:
\begin{center}
``Every American has a dream.''
\end{center}

This sentence is ambiguous because the order of quantifiers is
unclear.  Let $A$ be the set of Americans, let $D$ be the set of
dreams, and define the predicate $H(a, d)$ to be ``American $a$ has
dream $d$.''  Now the sentence could mean there is a single dream
that every American shares---such as the dream of owning their own
home:
\[
\exists\, d \in D\, \forall a \in A.\, H(a, d)
\]

Or it could mean that every American has a personal dream:
\[
\forall a \in A\, \exists\, d \in D.\, H(a, d)
\]
For example, some Americans may dream of a peaceful retirement, while
others dream of continuing practicing their profession as long as they
live, and still others may dream of being so rich they needn't think
about work at all.

Swapping quantifiers in Goldbach's Conjecture creates a patently false
statement that every even number $\geq 2$ is the sum of \emph{the same}
two primes:
\[
\underbrace{\exists\, p \in \primes\ \exists\, q \in \primes.}_{\substack
    {\text{there exist primes} \\
     \text{$p$ and $q$ such that}}}
\
\underbrace{\forall n \in \even}_{\substack
    {\text{for every even} \\
     \text{integer $n > 2$}}}
\ n = p + q.
\]

\subsection{Variables Over One Domain}
When all the variables in a formula are understood to take values from the
same nonempty set $D$ it's conventional to omit mention of $D$.  For
example, instead of $\forall x \in D\, \exists y \in D.\, Q(x,y)$ we'd
write $\forall x \exists y.\, Q(x,y)$.  The unnamed nonempty set that $x$
and $y$ range over is called the \term{domain of discourse}, or just plain
\emph{domain}, of the formula.

It's easy to arrange for all the variables to range over one domain.  For
example, Goldbach's Conjecture could be expressed with all variables
ranging over the domain $\nngint$ as
\[
\forall n.\, n \in \even \QIMP (\exists\, p\, \exists\, q.\, p \in \primes \QAND
q \in \primes \QAND n = p + q).
\]

\subsection{Negating Quantifiers}\label{negquant_sec}

There is a simple relationship between the two kinds of quantifiers.  The
following two sentences mean the same thing:
\begin{quote}
Not everyone likes ice cream.

There is someone who does not like ice cream.
\end{quote}
The equivalence of these sentences is an instance of a general
equivalence that holds between predicate formulas:
%
\begin{equation}\label{notforall}
\QNOT(\forall x.\, P(x))
\quad \text{is equivalent to} \quad
\exists x.\, \QNOT(P(x)).
\end{equation}
%
Similarly, these sentences mean the same thing:
%
\begin{quote}
There is no one who likes being mocked.

Everyone dislikes being mocked.
\end{quote}
The corresponding predicate formula equivalence is
\begin{equation}\label{notexists}
\QNOT(\exists x.\, P(x))
\quad \text{is equivalent to} \quad
\forall x.\, \QNOT(P(x)).
\end{equation}
Note that the equivalence~\eqref{notexists} follows directly by
negating both sides the equivalence~\eqref{notforall}.

The general principle is that \emph{moving a \QNOT\ to the other side
  of an ``$\exists$'' changes it into ``$\forall$,'' and \emph{vice
    versa}.}

These equivalences are called \term{De Morgan's Laws for Quantifiers}
because they can be understood as applying De Morgan's Laws for
propositional formulas to an infinite sequence of \QAND's and \QOR's.
For example, we can explain~\eqref{notforall} by supposing the domain
of discourse is $\set{d_0,d_1,\dots,d_n,\dots}$.  Then $\exists x.\,
\QNOT(P(x))$ means the same thing as the infinite \QOR:
\begin{equation}\label{notPinfOR}
\QNOT(P(d_0))\ \QOR\ \QNOT(P(d_1))\ \QOR\ \cdots\ \QOR\ \QNOT(P(d_n))\ \QOR \dots.
\end{equation}
Applying De Morgan's rule to this infinite \QOR\ yields the equivalent formula
\begin{equation}\label{notPinfAND}
\QNOT[P(d_0)\ \QAND\ P(d_1)\ \QAND \cdots\ \QAND\ P(d_n)\ \QAND \dots].
\end{equation}
But~\eqref{notPinfAND} means the same thing as
\[
\QNOT[\forall x.\, P(x)].
\]
This explains why $\exists x.\, \QNOT(P(x))$ means the same thing as
$\QNOT[\forall x.\, P(x)]$, which confirms\eqref{notforall}.

\iffalse
Logicians have worked very hard to define strict rules for the
use of logic notation so that ideas can be expressed with absolute rigor.
It's all quite charming and clever.  However, the sad irony is that
applied mathematicans usually use their beloved notation as a crude
shorthand, breaking the rules and abusing the notation willy-nilly---sort
of like pounding nails with fine china.
\fi

\subsection{Validity for Predicate Formulas}

\iffalse
A propositional formula is called \emph{valid} when it evaluates to \true\
no matter what truth values are assigned to the individual propositional
variables.  For example, the propositional version of the \idx{Distributive Law}
is that $P \QAND (Q \QOR R)$ is equivalent to $(P \QAND Q) \QOR (P \QAND
R)$.  This is the same as saying that
\[
[P \QAND (Q \QOR R)] \QIFF [(P \QAND Q) \QOR (P \QAND R)]
\]
is valid.
\fi

The idea of validity \index{validity (logic)} extends to predicate
formulas, but to be valid, a formula now must evaluate to true no
matter what the domain of discourse may be, no matter what values its
variables may take over the domain, and no matter what interpretations
its predicate variables may be given.  For example, the
equivalence~\eqref{notforall} that gives the rule for negating a
universal quantifier means that the following formula is valid:
\begin{equation}\label{notforall-equiv}
\QNOT(\forall x.\, P(x)) 
\QIFF  
\exists x.\, \QNOT(P(x)).
\end{equation}

Another useful example of a valid assertion is
\begin{equation}\label{eaimpliesae}
\exists x \forall y.\, P(x,y) \QIMP \forall y \exists x.\, P(x,y).
\end{equation}

Here's an explanation why this is valid:

\begin{quote}
Let $D$ be the domain for the variables and $P_0$ be some
binary predicate\footnote{That is, a predicate that depends on two variables.}
on $D$.  We need to show that if
\begin{equation}\label{exayp0}
\exists x \in D.\, \forall y \in D.\, P_0(x,y)
\end{equation}
holds under this interpretation, then so does
\begin{equation}\label{ayexp0}
\forall y \in D\, \exists x \in D.\, P_0(x,y).
\end{equation}
So suppose~\eqref{exayp0} is true.  Then by definition of $\exists$, this
means that some element $d_0 \in D$ has the property that
\[
\forall y \in D.\, P_0(d_0, y).
\]
By definition of $\forall$, this means that
\[
P_0(d_0,d)
\]
is true for all $d \in D$.  So given any $d \in D$, there is an element in
$D$, namely $d_0$, such that $P_0(d_0,d)$ is true.  But that's exactly
what~\eqref{ayexp0} means, so we've proved that~\eqref{ayexp0} holds under
this interpretation, as required.
\end{quote}

We hope this is helpful as an explanation, but we don't really want to
call it a ``proof.''  The problem is that with something as basic
as~\eqref{eaimpliesae}, it's hard to see what more elementary axioms
are ok to use in proving it.  What the explanation above did was
translate the logical formula~\eqref{eaimpliesae} into English and
then appeal to the meaning, in English, of ``for all'' and ``there
exists'' as justification.

\iffalse So this wasn't a proof, just an explanation intended to make
clear what~\eqref{eaimpliesae} means.\fi

In contrast to~\eqref{eaimpliesae}, the formula
\begin{equation}\label{aenotimplyea}
\forall y \exists x.\, P(x,y)\ \QIMP\ \exists x \forall y.\, P(x,y).
\end{equation}
is \emph{not} valid.  We can prove this just by describing an
interpretation where the hypothesis $\forall y \exists x.\, P(x,y)$ is
true but the conclusion $\exists x \forall y.\, P(x,y)$ is not true.
For example, let the domain be the integers and $P(x,y)$ mean $x > y$.
Then the hypothesis would be true because, given a value $n$ for $y$ we
could choose the value of $x$ to be $n+1$, for example.  But under this
interpretation the conclusion asserts that there is an integer that is
bigger than all integers, which is certainly false.  An interpretation
like this that falsifies an assertion is called a \term{counter-model} to
that assertion.

\begin{problems}

\practiceproblems
\pinput{TP_Propositions_with_Quantifiers}
\pinput{TP_Quantifiers}  %replace with TP_Quantifiers2 and show after S18 final
\pinput{TP_quantifier_practice}
\pinput{CP_counter_model}
\pinput{TP_counter_model_EimpA}
\pinput{TP_counter_model_EEandE}
\pinput{TP_Predicate_Logic}
\pinput{TP_bogus_by_converse}

\classproblems
\pinput{CP_logic_news_network}
%\pinput{CP_logical_set_theory}
\pinput{CP_domain_of_discourse}
\pinput{CP_assertions_about_binary_strings}
\pinput{CP_express_goldbach_twinprime_in_logic}


\homeworkproblems
\pinput{PS_predicate_calculus_power_of_prime}
\pinput{PS_emailed_at_most_2_others}
%\pinput{PS_emailed_exactly_2_others}
\pinput{PS_emailed_at_most_n_others}
\pinput{CP_variable_convention}
\pinput{CP_prenex}
%\pinput{CP_pred_calc_model_N}
\pinput{CP_pred_calc_model_N_arrows}

\examproblems
\pinput{MQ_domain_of_discourse}
\pinput{MQ_swapping_quantifiers_morning}
%\pinput{MQ_pair_predicate} moved to set_theory
\pinput{FP_line_up_quantifiers}
\pinput{FP_logic_of_leq}
\pinput{MQ_equality_logic}
\pinput{FP_infinitely_often_quantifiers}
\pinput{FP_limit_quantifiers}
\pinput{MQ_limit_quantifiers}
\end{problems}

\begin{editingnotes}
\section{Predicate Logic}\label{pred_logic_sec}

The rules of Propositional Logic apply to Predicate Logic formulas,
but Predicate Logic is much richer and much more complicated.  With
Predicate Logic, we can start to handle real mathematics and
engineering problems.

We've seen in the previous section how Goldbach's Conjecture can be
expressed with predicate formulas with the domain of discourse being
the nonnegative integers.  It's an easy exercise to express in this
way all the famous open problems of number theory that we've already
mentioned (Problem~\ref{CP_express_goldbach_twinprime_in_logic}).  So
this is the first clue that predicate calculus has got to be hard to
manage.  If we had some method for determining the truth of predicate
formulas about numbers that was anything like truth tables for
propositional logic, then we could set computers to work on all these
open problems in number theory and expect them to be solved in due
course.  There have been few great successes in this direction: the four color problem

CP\_pred\_calc\_model\_N
MQ\_counter\_*3
CP\_3\_counters

cite Boolos, Machtey-Young, others re counter-machines, completeness,
incompleteness.

\end{editingnotes}

\section{References}

\cite{GareyJohnson70}

\endinput
