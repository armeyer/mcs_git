\subsection{Covariance}
Covariance is a generalization of variance that provides a measure of
correlation between two random variables.

\begin{definition*}
The \emph{covariance} of two random variables $R$ and $S$ is
\begin{equation}\label{covardef}
\covar{R,S} \eqdef \expect{(R - \mu_R)(S - \mu_S)}.
\end{equation}
where $\mu_R \eqdef \expect{R}$ and likewise for $\mu_S$.
\end{definition*}

Notice that
\[
\variance{R} = \covar{R,R}.
\]

The following equivalent formulation of covariance follows immediately by
applying linearity of expectation to~(\ref{covardef}).
\begin{lemma*}
\begin{equation}\label{covar:alt}
\covar{R,S} = \expect{R\cdot S} - \expect{R} \cdot \expect{S}.
\end{equation}
\end{lemma*}

Another simple consequence of linearity of expectation is:
\begin{lemma*}For $a,b,c,d \in \reals$,
\begin{equation}\label{covaraR}
\covar{aR+b,cS+d} = ac\covar{R,S}.
\end{equation}
\end{lemma*}

\begin{definition*}
Two variables are said to be \emph{uncorrelated} iff their covariance is
zero.
\end{definition*}

By~(\ref{covar:alt}), we can put it another way:
\begin{lemma*}
$R$ and $S$ are uncorrelated iff
\begin{equation}\label{ERS}
\expect{R\cdot S} = \expect{R} \cdot \expect{S}.
\end{equation}
\end{lemma*}

We know that if $R$ and $S$ are independent, then equation~(\ref{ERS})
holds, and hence $R$ and $S$ are uncorrelated.  But the converse is not
true: there are uncorrelated variables that are not independent.

\begin{example}
Let $R$ be a random variable taking values $-1, 0, 1$ with a uniform
distribution.  So $\expect{R} =0$.  Now let $S$ be the indicator variable
for the event $[R = 0]$.  So $S=0$ iff $R \neq 0$, and hence $RS = 0$.  So
\[
\expect{RS} = \expect{0} = 0 = 0 \cdot \frac{2}{3} = \expect{R}\expect{S},
\]
confirming that $R$ and $S$ are uncorrelated.

Also, $R$ and $S$ are obviously not independent, since
\[
\prcond{R=0}{S=0} = 0 \neq \frac{1}{3} = \pr{R=0}.
\]

There are also examples of uncorrelated but dependent variables whose
expectations are nonzero.  For example, by equation~(\ref{covaraR}),
$\covar{R+2,S+1} = \covar{R,S}$, so $R+2$ and $S+1$ are also uncorrelated
and only take positive values.
\end{example}

Correlation, more specifically lack of it, is important because it is
sufficient for variances to add.  A set of variables $R_1, R_2, \ldots,
R_n$ is said to be \emph{pairwise uncorrelated} iff $R_i$ and $R_j$ are
uncorrelated for all $i \neq j$.

\begin{theorem}
If $R_1, R_2, \ldots, R_n$ are pairwise \emph{uncorrelated} random
variables, then
\begin{equation}\label{th2:varsum}
\variance{R_1 + R_2 + \cdots + R_n} = \variance{R_1} + \variance{R_2} +
  \cdots + \variance{R_n}.
\end{equation}
\end{theorem}

The proof of this Theorem is the same as the proof of
Theorem~\ref{th:varsum} that variances of pairwise \emph{independent}
variables add, because the only property of pairwise independence used in
the proof of Theorem~\ref{th:varsum} was~(\ref{ERS}).  Similarly,
equation~(\ref{th2:varsum}) is the only property required for the Pairwise
Independent Sampling Theorem~\ref{devmean} to hold, so we also have:
\begin{corollary}
\label{uncorsample}[Pairwise Uncorrelated Sampling]
Let
\[
S_n \eqdef \sum_{i=1}^n G_i
\]
where $G_1, \dots, G_n$ are pairwise \emph{uncorrelated} variables with
the same mean $\mu$ and deviation $\sigma$.  Then
\begin{equation}\label{Snx}
\pr{\abs{\frac{S_n}{n} - \mu} \geq x} \leq \frac{1}{n}
\left(\frac{\sigma}{x}\right)^2.
\end{equation}
\end{corollary}

For correlated variables, variances do not add, but covariance
provides just the right corrective term.

\begin{theorem}[General Variance Additivity}
For any random variables $R,S$, 
\begin{equation}\label{vr+scov}
\end{equation}
\variance{R + S} = \variance{R} + \variancef{S} + 2\covariance{R,S}.
\end{theorem}

