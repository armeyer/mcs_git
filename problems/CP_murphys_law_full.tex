\documentclass[problem]{mcs}

\begin{pcomments}
  \pcomment{CP_murphys_law_full}
  \pcomment{CP_murphys_law plus extra last parts}
  \pcomment{F02.cp14f}
\end{pcomments}

\pkeywords{
  murphy
  mutually_independent
  independent
  indicator
  sum
}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% Problem starts here
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\begin{problem}
In this problem you will check a proof of Murphy's Law\inbook{, Theorem~\bref{th:murphy}}:
\begin{theorem*}[Murphy's Law]
Let $A_1, A_2, \dots A_n$ be mutually independent events, and let $T$ be
the number of these events that occur.  The probability that none of the
events occur is at most $e^{-\expect{T}}$.
\end{theorem*}

To prove Murphy's Law, note that
\begin{equation}\label{Tsum}
T = T_1 + T_2 + \dots + T_n,
\end{equation}
where $T_i$ is the indicator variable for the event $A_i$.  Also, remember
that
\begin{equation} \label{1+xeleq}
1 + x \leq e^x
\end{equation}
for all $x$\iffalse
and
\begin{equation}
1 + x \approx e^x \label{1+xesim},
\end{equation}
for $0 \leq x \leq 1$.  Both~\eqref{1+xeleq} and~\eqref{1+xesim} follow
from the Taylor's expansion of $e^x$\fi.

\bparts

\ppart
Justify each line in the following derivation (without looking it up
in the text):

\instatements{\begin{proof}
\begin{align*}
\pr{T = 0}
  & = \prob{\bar{A_1 \union A_2 \union \cdots \union A_n}}\\
  & =  \pr{\bar{A_1} \cap \bar{A_2} \cap \dots \cap \bar{A_n}}\\
  & =  \prod_{i=1}^n \pr{\bar{A_i}}\\
  & =  \prod_{i=1}^n 1 - \pr{A_i}\\
  & \leq  \prod_{i=1}^n e^{-\pr{A_i}}\\
  & =  e^{-\sum_{i=1}^n \pr{A_i}}\\
  & =  e^{-\sum_{i=1}^n \expect{T_i}}\\
  & =  e^{-\expect{T}}.
\end{align*}
\end{proof}
}

\begin{solution}

\begin{proof}
\begin{align*}
\pr{T = 0}
  & = \prob{\bar{A_1 \union A_2 \union \cdots \union A_n}}
           & \text{($T=0$ iff no $A_i$ occurs)}\\
  & =  \pr{\bar{A_1} \cap \bar{A_2} \cap \dots \cap \bar{A_n}}
           & \text{(De Morgan's law)}\\
  & =  \prod_{i=1}^n \pr{\bar{A_i}} & \text{(mutual independence of $A_i$'s)}\\
  & =  \prod_{i=1}^n 1 - \pr{A_i} & \text{(complement rule)}\\
  & \leq  \prod_{i=1}^n e^{-\pr{A_i}} & \text{(by~\eqref{1+xeleq})}\\
  & =  e^{-\sum_{i=1}^n \pr{A_i}} & \text{(exponent algebra)}\\
  & =  e^{-\sum_{i=1}^n \expect{T_i}} & \text{(expectation of indicator variable)}\\
  & =  e^{-\expect{T}}. & \text{((\ref{Tsum}) \& linearity of expectation)}
\end{align*}
\end{proof}

\end{solution}

\eparts

Two special cases of Murphy's Law are worth singling out because they
come up all the time.
\begin{corollary}\label{1/mtrials}
Suppose an event has probability $1/m$.  Then the probability that the
event will occur at least once in $m$ independent trials is approximately
$1- 1/e \approx 63\%$.  There is a 50\% chance the event will occur in $n
= (\ln 2) m \approx 0.69m$ trials.
\end{corollary}

\bparts

\ppart
Prove Corollary~\ref{1/mtrials}.

\begin{solution}
In this case, $\pr{A_i}=1/m$ for $1\leq i \leq n$ and
\[
\expect{\text{\# occurrences}} = n \frac{1}{m} = \frac{n}{m}.
\]
So by Theorem~\bref{th:murphy},
\[
\pr{\text{no occurrence}} \leq e^{-(n/m)},
\]
and therefore
\begin{equation}\label{1-enm}
\pr{\text{at least one occurrence}} \geq 1 - e^{-(n/m)}.
\end{equation}
In fact, \iffalse it follows from by~(\ref{1+xesim}), that \fi the
$\geq$ in~\eqref{1-enm} is a tight estimate.  So if the number, $n$ of
trials is $m$, we have
\[
\pr{\text{at least one occurrence}} \approx 1- e^{-(m/m)} = 1-\frac{1}{e}.
\]

If we want
\[
1 - e^{-(n/m)} \approx \pr{\text{at least one occurrence}} \approx
\frac{1}{2},
\]
then we need
\[
e^{-(n/m)} \approx \frac{1}{2},
\]
so taking log's we conclude
\[
n \approx m\ln 2.
\]

\end{solution}
\eparts

\end{problem}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% Problem ends here
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\endinput
