\chapter{Patterns of Proof}\label{templates_chap}

\section{The Axiomatic Method}

The standard procedure for establishing truth in mathematics was
invented by \idx{Euclid}, a mathematician working in Alexandria, Egypt
around 300 BC.  His idea was to begin with five \emph{assumptions}
about geometry, which seemed undeniable based on direct experience.
For example, one of the assumptions was ``There is a straight line
segment between every pair of points.''  Propositions like these that
are simply accepted as true are called \term{axioms}.

Starting from these axioms, Euclid established the truth of many
additional propositions by providing ``proofs''.  A \term{proof} is a
sequence of logical deductions from axioms and previously-proved
statements that concludes with the proposition in question.  You
probably wrote many proofs in high school geometry class, and you'll
see a lot more in this course.

There are several common terms for a proposition that has been proved.
The different terms hint at the role of the proposition within a
larger body of work.
%
\begin{itemize}
\item Important propositions are called \term{theorems}.
\item A \term{lemma} is a preliminary proposition useful for proving
later propositions.
\item A \term{corollary} is a proposition that follows
in just a few logical steps from a lemma or a theorem.
\end{itemize}
%
The definitions are not precise.  In fact, sometimes a good lemma
turns out to be far more important than the theorem it was originally
used to prove.

Euclid's axiom-and-proof approach, now called the \term{axiomatic
  method}, is the foundation for mathematics today.  In fact, just a
handful of axioms, collectively called \idx{Zermelo-Frankel} Set
Theory with Choice (\term{ZFC}), together with a few logical deduction
rules, appear to be sufficient to derive essentially all of
mathematics.

\subsection{Our Axioms}

The ZFC axioms are important in studying and justifying the foundations of
mathematics, but for practical purposes, they are much too primitive.
Proving theorems in ZFC is a little like writing programs in byte code
instead of a full-fledged programming language---by one reckoning, a
formal proof in ZFC that $2 + 2 = 4$ requires more than 20,000 steps!  So
instead of starting with ZFC, we're going to take a \emph{huge} set of
axioms as our foundation: we'll accept all familiar facts from high school
math!

This will give us a quick launch, but you may find this imprecise
specification of the axioms troubling at times.  For example, in the midst
of a proof, you may find yourself wondering, ``Must I prove this little
fact or can I take it as an axiom?''  Feel free to ask for guidance, but
really there is no absolute answer.  Just be up front about what you're
assuming, and don't try to evade homework and exam problems by declaring
everything an axiom!

\subsection{Logical Deductions}\label{sec:logical_deduction}

Logical deductions or \term{inference rules} are used to prove new
propositions using previously proved ones.

A fundamental inference rule is \term{modus ponens}.  This rule says that
a proof of $P$ together with a proof that $P \QIMPLIES Q$ is a proof of
$Q$.

Inference rules are sometimes written in a funny notation.  For example,
\emph{modus ponens} is written:
\begin{rul}\label{rule:modus_ponens}
\Rule{P, \quad P \QIMPLIES Q}{Q}
\end{rul}

When the statements above the line, called the \term{antecedents}, are
proved, then we can consider the statement below the line, called the
\term{conclusion} or \term{consequent}, to also be proved.

A key requirement of an inference rule is that it must be \term{sound}: any
assignment of truth values that makes all the antecedents true must also
make the consequent true.  So if we start off with true axioms and apply
sound inference rules, everything we prove will also be true.

You can see why modus ponens is a sound inference rule by checking the
truth table of $P\QIMPLIES Q$.  There is only one case where $P$ and
$P \QIMPLIES Q$ are both true, and in that case $Q$ is also true.
\[
\begin{array}{cc|c}
P & Q & P \implies Q \\ \hline
\false & \false & \true \\
\false & \true  & \true \\
\true  & \false & \false \\
\true  & \true  & \true \\
\end{array}
\]

There are many other natural, sound inference rules, for example:
\begin{rul}\label{rule:transitivity}
  \Rule{P \QIMPLIES Q, \quad Q \QIMPLIES R}{P \QIMPLIES R}
\end{rul}

\begin{rul}
\Rule{P \QIMPLIES Q, \quad \QNOT(Q)}{\QNOT(P)}
\end{rul}

\begin{rul}
  \Rule{\QNOT(P) \QIMPLIES \QNOT(Q)}{Q \QIMPLIES P}
\end{rul}

On the other hand,
\begin{nonrul*}
\Rule{\QNOT(P) \QIMPLIES \QNOT(Q)}{P \QIMPLIES Q}
\end{nonrul*}
\noindent is \emph{not} sound: if $P$ is assigned $\true$ and $Q$ is assigned
$\false$, then the antecedent is true and the consequent is not.

Note that a propositional inference rule is sound precisely when the conjunction
(AND) of all its antecedents implies its consequent.

As with axioms, we will not be too formal about the set of legal inference
rules.  Each step in a proof should be clear and ``logical''; in
particular, you should state what previously proved facts are used to
derive each new conclusion.

\subsection{Proof Templates}

In principle, a proof can be \emph{any} sequence of logical
deductions from axioms and previously proved statements that concludes
with the proposition in question.  This freedom in constructing a
proof can seem overwhelming at first.  How do you even \emph{start}
a proof?

Here's the good news: many proofs follow one of a handful of standard
templates.  Each proof has it own details, of course, but these
templates at least provide you with an outline to fill in.  In the
remainder of this chapter, we'll go
through several of these standard patterns, pointing out the basic
idea and common pitfalls and giving some examples.  Many of these
templates fit together; one may give you a top-level outline while
others help you at the next level of detail.  And we'll show you
other, more sophisticated proof techniques in
Chapter~\ref{induction_chap}.

The recipes that follow are very specific at times, telling you exactly
which words to write down on your piece of paper.  You're certainly
free to say things your own way instead; we're just giving you
something you \emph{could} say so that you're never at a complete
loss.

\section{Proof by Cases}

Breaking a complicated proof into cases and proving each case
separately is a useful and common proof strategy.  In fact, we have
already implicitly used this strategy when we used truth tables to
show that certain propositions were true or valid.  For example, in
section~\ref{sec:logical_equivalence}, we showed that an implication
$P \QIMPLIES Q$ is equivalent to its contrapositive $\QNOT(Q)
\QIMPLIES \QNOT(P)$ by considering all 4 possible assignments
of~$\true$ or~$\false$ to~$P$ and~$Q$.  In each of the four cases, we
showed that $P \QIMPLIES Q$ is true if and only if $\QNOT(Q) \QIMPLIES
\QNOT(P)$ is true.  For example, if $P = \true$ and $Q = \false$, then
both $P \QIMPLIES Q$ and $\QNOT(Q) \QIMPLIES \QNOT(P)$ are false,
thereby establishing that $(P \QIMPLIES Q) \QIFF (\QNOT(Q) \QIMPLIES
\QNOT(P))$ is true for this case.  If a proposition is true in every
possible case, then it is true.

Proof by cases works in much more general environments than
propositions involving Boolean variables.  In what follows, we will
use this approach to prove a simple fact about acquaintances.  As
background, we will assume that for any pair of people, either they
have met or not.  If every pair of people in a group has met, we'll
call the group a \term*{club}.  If every pair of people in a group has
not met, we'll call it a group of \term*{strangers}.

\begin{theorem*}
Every collection of 6 people includes a club of 3 people or a group of 3
strangers.
\end{theorem*}

\begin{proof}
The proof is by case analysis\footnote{Describing your approach at the
outset helps orient the reader.  Try to remember to always do this.}.
Let $x$ denote one of the six people.  There are two cases:

\begin{enumerate}
\item\label{3met} Among the other 5 people besides $x$, at least 3 have met
  $x$.

\item \label{3notmet} Among the other 5 people, at least 3 have not met
  $x$.
\end{enumerate}

Now we have to be sure that at least one of these two cases must
hold,\footnote{Part of a case analysis argument is showing that you've
  covered all the cases.  Often this is obvious, because the two cases
  are of the form ``$P$'' and ``not $P$''.  However, the situation
  above is not stated quite so simply.} but that's easy: we've split
the 5 people into two groups, those who have shaken hands with $x$ and
those who have not, so one of the groups must have at least half the
people.

\textbf{Case 1:}  Suppose that at least 3 people have met~$x$.

This case splits into two subcases:
\begin{quote}

\textbf{Case 1.1:} Among the people who have met~$x$, none have
met each other.  Then the people who have met~$x$ are a group of
at least 3 strangers.  So the Theorem holds in this subcase.

\textbf{Case 1.2:} Among the people who have met~$x$, some pair
have met each other.  Then that pair, together with $x$, form a club
of 3 people.  So the Theorem holds in this subcase.

\end{quote}
This implies that the Theorem holds in Case 1.

\textbf{Case 2:} Suppose that at least 3 people have not met~$x$.

This case also splits into two subcases:
\begin{quote}

\textbf{Case 2.1}: Among the people who have not met~$x$, every pair
has met each other.  Then the people who have not met~$x$ are a club of
at least 3 people.  So the Theorem holds in this subcase.

\textbf{Case 2.2:} Among the people who have not met~$x$, some pair
have not met each other.  Then that pair, together with $x$, form a
group of at least 3 strangers.  So the Theorem holds in this subcase.

\end{quote}
This implies that the Theorem also holds in Case 2, and therefore holds in
all cases.
\end{proof}

\section{Proving an Implication}\label{sec:prove_implies}

Propositions of the form ``If $P$, then $Q$'' are called
\term{implications}.  This implication is often rephrased as ``$P
\QIMPLIES Q$'' or ``$P \implies Q$''.

Here are some examples of implications:
%
\begin{itemize}

\item (Quadratic Formula) If $a x^2 + b x + c = 0$ and $a \neq 0$,
then
\[
x = \frac{- b \pm \sqrt{b^2 - 4 a c}}{2a}.
\]

\item (Goldbach's Conjecture) If $n$ is an even integer greater than
$2$, then $n$ is a sum of two primes.

\item If $0 \leq x \leq 2$, then $-x^3 + 4x + 1 > 0$.

\end{itemize}
%
There are a couple of standard methods for proving an implication.

\subsection{Method \#1: Assume $P$ is true}

When proving $P \QIMPLIES Q$, there are two cases to
consider: $P$ is true and $P$ is false.  The case when $P$ is false is
easy since, by definition, $\false \QIMPLIES Q$ is true no matter what
$Q$ is.  This case is so easy that we usually just forget about it and
start right off by assuming that $P$ is true when proving an
implication, since this is the only case that is interesting.  Hence,
in order to prove that $P \QIMPLIES Q$:
%
\begin{enumerate}
\item Write, ``Assume $P$.''
\item Show that $Q$ logically follows.
\end{enumerate}

For example, we will use this method to prove
\begin{theorem}\label{th:-x3+4x+1}
If $0 \leq x \leq 2$, then $-x^3 + 4x + 1 > 0$.
\end{theorem}

Before we write a proof of this theorem, we have to do some
scratchwork to figure out why it is true.

The inequality certainly holds for $x = 0$; then the left side is
equal to 1 and $1 > 0$.  As $x$ grows, the $4x$ term (which is
positive) initially seems to have greater magnitude than $-x^3$ (which
is negative).  For example, when $x = 1$, we have $4x = 4$, but $-x^3
= -1$.  In fact, it looks like $-x^3$ doesn't begin to dominate~$4x$
until $x > 2$.  So it seems the $-x^3 + 4x$ part should be nonnegative
for all $x$ between 0 and 2, which would imply that $-x^3 + 4x + 1$ is
positive.

So far, so good.  But we still have to replace all those ``seems
like'' phrases with solid, logical arguments.  We can get a better
handle on the critical $-x^3 + 4x$ part by factoring it, which is not
too hard:
%
\[
-x^3 + 4x = x (2 - x)(2 + x)
\]
%
Aha!  For $x$ between 0 and 2, all of the terms on the right side are
nonnegative.  And a product of nonnegative terms is also nonnegative.
Let's organize this blizzard of observations into a clean proof.

\begin{proof}
Assume $0 \leq x \leq 2$.  Then $x$, $2 - x$, and $2 + x$ are all
nonnegative.  Therefore, the product of these terms is also
nonnegative.  Adding 1 to this product gives a positive number, so:
%
\[
x (2 - x)(2 + x) + 1 > 0
\]
%
Multiplying out on the left side proves that
%
\[
-x^3 + 4x + 1 > 0
\]
%
as claimed.
\end{proof}


There are a couple points here that apply to all proofs:
%
\begin{itemize}

\item You'll often need to do some scratchwork while you're trying to
figure out the logical steps of a proof.  Your scratchwork can be as
disorganized as you like---full of dead-ends, strange diagrams,
obscene words, whatever.  But keep your scratchwork separate from your
final proof, which should be clear and concise.

\item Proofs typically begin with the word ``Proof'' and end with some
  sort of doohickey like $\Box$ or $\blacksquare$ or ``q.e.d''.  The
  only purpose for these conventions is to clarify where proofs begin
  and end.

\end{itemize}

\subsubsection{Potential Pitfall}

For the purpose of proving an implication $P \QIMP Q$, it's OK, and
typical, to begin by assuming $P$.  But when the proof is over, it's
no longer OK to assume that $P$ holds!  For example,
Theorem~\ref{th:-x3+4x+1} has the form ``if $P$, then $Q$'' with $P$
being ``$0 \le x \le 2$'' and $Q$ being ``$-x^3 + 4x + 1 > 0$,'' and
its proof began by assuming that $0 \le x \le 2$.  But of course this
assumption does not always hold.  Indeed, if you were going to prove
another result using the variable~$x$, it could be disastrous to have
a step where you assume that $0 \le x \le 2$ just because you assumed
it as part of the proof of Theorem~\ref{th:-x3+4x+1}.

\subsection{Method \#2: Prove the Contrapositive}\label{sec:contrapositive}

We have already seen that an implication ``$P \QIMPLIES Q$'' is
logically equivalent to its \term{contrapositive}
\[
\QNOT(Q) \QIMPLIES \QNOT(P).
\]
Proving one is as good as proving the other, and proving the
contrapositive is sometimes easier than proving the original statement.
Hence, you can proceed as follows:
%
\begin{enumerate}
\item Write, ``We prove the contrapositive:'' and then state the
contrapositive.
\item Proceed as in Method \#1.
\end{enumerate}

For example, we can use this approach to prove
\begin{theorem}
If $r$ is irrational, then $\sqrt{r}$ is also irrational.
\end{theorem}

Recall that rational numbers are equal to a ratio of integers and
irrational numbers are not.  So we must show that if $r$ is \emph{not} a
ratio of integers, then $\sqrt{r}$ is also \emph{not} a ratio of
integers.  That's pretty convoluted!  We can eliminate both \emph{not}'s
and make the proof straightforward by considering the contrapositive
instead.

\begin{proof}
We prove the contrapositive: if $\sqrt{r}$ is rational, then $r$ is
rational.

Assume that $\sqrt{r}$ is rational.  Then there exist integers $a$ and $b$
such that:
%
\[
\sqrt{r} = \frac{a}{b}
\]
%
Squaring both sides gives:
%
\[
r  = \frac{a^2}{b^2}
\]
%
Since $a^2$ and $b^2$ are integers, $r$ is also rational.
\end{proof}

\section{Proving an ``If and Only If''}\label{sec:prove_iff}

Many mathematical theorems assert that two statements are logically
equivalent; that is, one holds if and only if the other does.  Here is an
example that has been known for several thousand years:
\begin{quote}
Two triangles have the same side lengths if and only if two
side lengths and the angle between those sides are the same in each
triangle.
\end{quote}

The phrase ``if and only if'' comes up so often that it is often
abbreviated ``iff''.

\subsection{Method \#1:  Prove Each Statement Implies the Other}

The statement ``$P \QIFF Q$'' is equivalent to the two statements ``$P
\QIMPLIES Q$'' and ``$Q \QIMPLIES P$''.  So you can prove an ``iff'' by
proving \emph{two} implications:
%
\begin{enumerate}
\item Write, ``We prove $P$ implies $Q$ and vice-versa.''
\item Write, ``First, we show $P$ implies $Q$.'' Do this by one
of the methods in Section~\ref{sec:prove_implies}.
\item Write, ``Now, we show $Q$ implies $P$.''  Again, do this by
one of the methods in Section~\ref{sec:prove_implies}.
\end{enumerate}

\subsection{Method \#2: Construct a Chain of~$\QIFF$s}

In order to prove that $P$ is true iff $Q$ is true:
%
\begin{enumerate}
\item Write, ``We construct a chain of if-and-only-if implications.''
\item Prove $P$ is equivalent to a second statement which is
equivalent to a third statement and so forth until you reach $Q$.
\end{enumerate}
%
This method sometimes requires more ingenuity than the first, but the
result can be a short, elegant proof, as we see in the following
example.

\begin{theorem}\label{th:sd}
The standard deviation of a sequence of values $x_1, \dots, x_n$ is
zero iff all the values are equal to the mean.
\end{theorem}

\begin{definition*}
The \emph{standard deviation} of a sequence of values $x_1, x_2,
\dots, x_n$ is defined to be:
%
\begin{equation}\label{sd}
\sqrt{\frac{(x_1 - \mu)^2 + (x_2 - \mu)^2 + \cdots + (x_n - \mu)^2}{n}}
\end{equation}
%
where $\mu$ is the \term{mean} of the values:
%
\[
\mu \eqdef \frac{x_1 + x_2 + \cdots + x_n}{n}
\]
\end{definition*}

As an example, Theorem~\ref{th:sd} says that the standard deviation of
test scores is zero if and only if everyone scored exactly the class
average.  (We will talk a lot more about means and standard deviations
in Part~IV of the book.)

\begin{proof}
We construct a \idx{chain of ``iff''} implications, starting with the
statement that the standard deviation~\eqref{sd} is zero:
%
\begin{equation}\label{sqrtis0}
\sqrt{\frac{(x_1 - \mu)^2 + (x_2 - \mu)^2 + \cdots + (x_n - \mu)^2}{n}} = 0.
\end{equation}
%
Since zero is the only number whose square root is zero,
equation~\eqref{sqrtis0} holds iff
\begin{equation}\label{is0}
(x_1 - \mu)^2 + (x_2 - \mu)^2 + \cdots + (x_n - \mu)^2 = 0.
\end{equation}
Squares of real numbers are always nonnegative, and so every term on the
left hand side of equation~\eqref{is0} is nonnegative.  This means
that~\eqref{is0} holds iff
\begin{equation}\label{every}
\text{Every term on the left hand side of~\eqref{is0} is zero.}
\end{equation}
But a term $(x_i - \mu)^2$ is zero iff $x_i=\mu$, so~\eqref{every} is true
iff
\[
\text{Every $x_i$ equals the mean.}
\]

\end{proof}


\section{Proof by Contradiction}\label{contradiction}

In a \term{proof by contradiction} or \term{indirect proof}, you show that
if a proposition were false, then some false fact would be true.  Since a
false fact can't be true, the proposition had better not be false.  That
is, the proposition really must be true.

Proof by contradiction is \emph{always} a viable approach.  However, as
the name suggests, indirect proofs can be a little convoluted.  So direct
proofs are generally preferable as a matter of clarity.

\textbf{Method}: In order to prove a proposition $P$ by contradiction:

\begin{enumerate}

\item Write, ``We use proof by contradiction.''

\item Write, ``Suppose $P$ is false.''

\item Deduce something known to be false (a logical contradiction).

\item Write, ``This is a contradiction.  Therefore, $P$ must be
true.''

\end{enumerate}


As an example, we will use proof by contradiction to prove that
$\sqrt{2}$ is irrational.  Recall that a number is \term{rational} if
it is equal to a ratio
of integers.  For example, $3.5 = 7/2$ and $0.1111\dots = 1/9$ are
rational numbers.

\begin{theorem}\label{thm:sqrt2irr_by_contra}
$\sqrt{2}$ is irrational.
\end{theorem}

\begin{proof}

We use proof by contradiction.  Suppose the claim is false; that is,
$\sqrt{2}$ is rational.  Then we can write $\sqrt{2}$ as a fraction
$n/d$ where $n$ and~$d$ are positive integers.  Furthermore, let's
take $n$ and~$d$ so that $n/d$ is in \emph{lowest terms} (\ie so that
there is no number greater than~1 that divides both $n$ and~$d$).

Squaring both sides gives $2 = n^2/d^2$ and so $2 d^2 = n^2$.  This
implies that $n$ is a multiple of~2.  Therefore $n^2$ must be a
multiple of~4.  But since $2 d^2 = n^2$, we know $2d^2$ is a multiple
of~4 and so $d^2$ is a multiple of~2.  This implies that $d$ is a
multiple of~2.

So the numerator and denominator have 2 as a common factor, which
contradicts the fact that $n/d$ is in lowest terms.  So $\sqrt{2}$
must be irrational.
\end{proof}

\subsubsection{Potential Pitfall}

A proof of a proposition~$P$ by contradiction is really the same as
proving the implication $\true \QIMPLIES P$ by contrapositive.
Indeed, the contrapositive of $T \QIMPLIES P$ is $\QNOT(P) \QIMPLIES
\false$.  As we saw in Section~\ref{sec:contrapositive},
such a proof would be begin by assuming $\QNOT(P)$ in an effort to
derive a falsehood, just as you do in a proof by contradiction.

No matter how you think about it, it is important to remember that
when you start by assuming $\QNOT(P)$, you will derive conclusions
along the way that are not necessarily true.  (Indeed, the whole point
of the method is to derive a falsehood.)  This means that you cannot
rely on intermediate results after a proof by contradiction is
completed (\eg that $n$ is even after the proof of
Theorem~\ref{thm:sqrt2irr_by_contra}).  There was not much risk of
that happening in the proof of Theorem~\ref{thm:sqrt2irr_by_contra},
but when you are doing more complicated proofs that build up from
several lemmas, some of which utilize a proof by contradiction, it
will be important to keep track of which propositions only follow from
a (false) assumption in a proof by contradiction.

\section{Proofs about Sets}

Sets are simple, flexible, and everywhere.  You will find some set
mentioned in nearly every section of this text.  In fact, we have
already talked about a lot of sets: the set of integers, the set of
real numbers, and the set of positive even numbers, to name a few.

In this section, we'll see how to prove basic facts about sets.  We'll
start with some definitions just to make sure that you know the
terminology and that you are comfortable working with sets.

\subsection{Definitions}

Informally, a \term{set} is a bunch of objects, which are called the
\term{elements} of the set.  The elements of a set can be just about
anything: numbers, points in space, or even other sets.  The conventional
way to write down a set is to list the elements inside curly-braces.  For
example, here are some sets:

\[
\begin{array}{rcll}
%\naturals & = & \set{0, 1, 2, 3, \ldots} & \text{the} \text{nonnegative integers} \\
A & = & \set{\text{Alex}, \text{Tippy}, \text{Shells}, \text{Shadow}} & \text{dead pets} \\
B & = & \set{\text{red}, \text{blue}, \text{yellow}} & \text{primary colors} \\
C & = & \set{ \set{a, b}, \set{a, c}, \set{b, c}} & \text{a set of sets}
\end{array}
\]
This works fine for small finite sets.  Other sets might be defined by
indicating how to generate a list of them:
\begin{align*}
D & =  \set{1,2,4,8,16,\dots} & \text{the powers of 2}
\end{align*}

The order of elements is not significant, so $\set{x, y}$ and $\set{y, x}$
are the same set written two different ways.  Also, any object is, or is
not, an element of a given set---there is no notion of an element
appearing more than once in a set.\footnote{It's not hard to develop a
notion of \term{multisets} in which elements can occur more than once, but
multisets are not ordinary sets.}  So writing $\set{x,x}$ is just
indicating the same thing twice, namely, that $x$ is in the set.  In
particular, $\set{x,x} = \set{x}$.

The expression $e \in S$ asserts that $e$ is an element of set $S$.  For
example, $32 \in D$ and $\text{blue} \in B$, but $\text{Tailspin}
\not\in A$---yet.

\subsubsection{Some Popular Sets}

Mathematicians have devised special symbols to represent some common
sets.

\begin{center}
\begin{tabular}{lll}
\textbf{symbol} & \textbf{set} & \textbf{elements} \\
\term{$\emptyset$} & the empty set & \text{none}\\
\term{$\naturals$} & nonnegative integers & $\set{0, 1, 2, 3, \ldots}$ \\
\term{$\integers$} & integers & $\set{\ldots, -3, -2, -1, 0, 1, 2, 3, \ldots}$ \\
\term{$\rationals$} & rational numbers & $\frac{1}{2},\ -\frac{5}{3},\ 16,\ \text{etc.}$ \\
\term{$\reals$} & real numbers & $\pi,\ e,\ -9,\ \sqrt{2},\ \text{etc.}$ \\
\term{$\complexes$} & complex numbers & $i,\ \frac{19}{2},\ \sqrt{2} - 2i,\ \text{etc.}$
\end{tabular}
\end{center}
A superscript ``$^+$'' restricts a set to its positive elements; for
example, \term{$\reals^+$} denotes the set of positive real numbers.  Similarly,
\term{$\reals^-$} denotes the set of negative reals.

\subsubsection{Comparing and Combining Sets}

The expression $S \subseteq T$ indicates that set $S$ is a \term{subset}
of set $T$, which means that every element of $S$ is also an element of
$T$ (it could be that $S=T$).  For example, $\naturals \subseteq
\mathbb{Z}$ and $\mathbb{Q} \subseteq
\reals$ (every rational number is a real number), but $\complexes
\not\subseteq \mathbb{Z}$ (not every complex number is an integer).

As a memory trick, notice that the \term{$\subseteq$} points to the
smaller set, just like a $\leq$ sign points to the smaller number.
Actually, this connection goes a little further: there is a symbol
\term{$\subset$} analogous to $<$.  Thus, $S \subset T$ means that $S$
is a subset of $T$, but the two are \emph{not} equal.  So $A \subseteq
A$, but $A \not\subset A$, for every set $A$.

There are several ways to combine sets.  Let's define a couple of sets for
use in examples:
\begin{align*}
X & \eqdef \set{1, 2, 3} \\
Y & \eqdef \set{2, 3, 4}
\end{align*}

\begin{itemize}

\item The \term{union} of sets $X$ and $Y$ (denoted $X$ \term{$\union$} $Y$)
contains all elements appearing in $X$ or $Y$ or both.  Thus, $X \union
Y = \set{1, 2, 3, 4}$.

\item The \term{intersection} of $X$ and $Y$ (denoted $X$
  \term{$\intersect$} $Y$) consists of all elements that appear in
  \emph{both} $X$ and $Y$.  So $X \intersect Y = \set{2, 3}$.

\item The \term{set difference} of $X$ and $Y$ (denoted $X$ \index{$-$,
    set difference}$-$ $Y$) consists of all elements that are in $X$, but not in $Y$.
  Therefore, $X - Y = \set{1}$ and $Y - X = \set{4}$.

\end{itemize}

\subsubsection{The Complement of a Set}

Sometimes we are focused on a particular domain, $D$.  Then for any
subset, $A$, of $D$, we define \term{$\overline{A}$} to be the set of all
elements of $D$ \emph{not} in $A$.  That is, $\overline{A} \eqdef D-A$.
The set $\overline{A}$ is called the \term{complement} of $A$.

For example, when the domain we're working with is the real numbers,
the complement of the positive real numbers is the set of negative real
numbers together with zero.  That is,
\[
\overline{\reals^+} = \reals^- \union \set{0}.
\]

It can be helpful to rephrase properties of sets using complements.  For
example, two sets, $A$ and $B$, are said to be \term{disjoint} iff they
have no elements in common, that is, $A \intersect B = \emptyset$.  This
is the same as saying that $A$ is a subset of the complement of $B$, that
is, $A \subseteq \overline{B}$.

\subsubsection{Cardinality}

The \emph{cardinality} of a set~$A$ is the number of elements in~$A$
and is denoted by~$|A|$.  For example,
\begin{align*}
    & |\emptyset| = 0, \\
    & |\{ 1, 2, 4 \}| = 3\text{, and} \\
    & |\naturals| \text{ is infinite.}
\end{align*}

\subsubsection{The Power Set}

The set of all the subsets of a set, $A$, is called the \term{power
  set}, \term{$\power(A)$}, of $A$.  So $B \in \power(A)$ iff $B
\subseteq A$.  For example, the elements of $\power( \set{1, 2})$ are
$\emptyset, \set{1}, \set{2}$ and $\set{1, 2}$.

More generally, if $A$ has $n$ elements, then there are $2^n$ sets in
$\power(A)$.  In other words, if $A$ is finite, then $|\power(A)| =
  2^{|A|}$.  For this reason, some authors use the notation $2^A$
  instead of $\power(A)$ to denote the power set of~$A$.

\subsubsection{Sequences}

Sets provide one way to group a collection of objects.  Another way is
in a \term{sequence}, which is a list of objects called \term{terms}
or \term{components}.  Short sequences are commonly described by
listing the elements between parentheses; for example, $(a, b, c)$ is
a sequence with three terms.

While both sets and sequences perform a gathering role, there are
several differences.
\begin{itemize}

\item The elements of a set are required to be distinct, but terms in a
sequence can be the same.  Thus, $(a, b, a)$ is a valid sequence of length
three, but $\set{a, b, a}$ is a set with two elements---not three.

\item The terms in a sequence have a specified order, but the elements
of a set do not.  For example, $(a, b, c)$ and $(a, c, b)$ are
different sequences, but $\set{a, b, c}$ and $\set{a, c, b}$ are the
same set.

\item Texts differ on notation for the \term{empty sequence}; we use
  \term{$\lambda$} for the empty sequence and $\emptyset$ for the
  empty set.
\end{itemize}

\subsubsection{Cross Products}

The product operation is one link between sets and sequences.  A
\term{product of sets}, $S_1 \times S_2 \times \cdots \times S_n$, is a
new set consisting of all sequences where the first component is drawn
from $S_1$, the second from $S_2$, and so forth.  For example, $\naturals
\times \set{a,b}$ is the set of all pairs whose first element is a
nonnegative integer and whose second element is an $a$ or a $b$:
\[
\naturals \times \set{a,b}
    = \set{(0,a), (0,b), (1,a), (1,b), (2,a), (2, b), \dots}
\]
A product of $n$ copies of a set $S$ is denoted $S^n$.  For example,
$\set{0, 1}^3$ is the set of all $3$-bit sequences:
\[
\set{0, 1}^3 = \set{ (0,0,0), (0,0,1), (0,1,0), (0,1,1),
                     (1,0,0), (1,0,1), (1,1,0), (1,1,1) }
\]

\subsection{Set Builder Notation}\label{sec:set_builder_notation}

An important use of predicates is in \term*{set builder notation}.  We'll
often want to talk about sets that cannot be described very well by
listing the elements explicitly or by taking unions, intersections,
etc., of easily-described sets.  Set builder notation often comes to the
rescue.  The idea is to define a \emph{set} using a \emph{predicate};
in particular, the set consists of all values that make the predicate
true.  Here are some examples of set builder notation:
\begin{align*}
A & \eqdef \set{n \in \naturals \suchthat \text{$n$ is a prime and $n =
    4k+1$ for some integer $k$}} \\
B & \eqdef \set{x \in \reals \suchthat x^3 - 3 x + 1 > 0} \\
C & \eqdef \set{a + b i \in \complexes \suchthat a^2 + 2 b^2 \leq 1}
\end{align*}

The set $A$ consists of all nonnegative integers $n$ for which the
predicate
\begin{center}
``$n$ is a prime and $n = 4k+1$ for some integer $k$''
\end{center}
is true.  Thus, the smallest elements of $A$ are:
\[
5, 13, 17, 29, 37, 41, 53, 57, 61, 73, \ldots.
\]
Trying to indicate the set $A$ by listing these first few elements
wouldn't work very well; even after ten terms, the pattern is not
obvious!  Similarly, the set $B$ consists of all real numbers $x$ for
which the predicate
\[
x^3 - 3x + 1 > 0
\]
is true.  In this case, an explicit description of the set $B$ in
terms of intervals would require solving a cubic equation.  Finally,
set $C$ consists of all complex numbers $a + b i$ such that:
\[
a^2 + 2 b^2 \leq 1
\]
This is an oval-shaped region around the origin in the complex plane.

\subsection{Proving Set Equalities}

Two sets are defined to be equal if they contain the same elements.  That
is, $X = Y$ means that $z \in X$ if and only if $z \in Y$, for all
elements, $z$.  (This is actually the first of the ZFC axioms.)  So set
equalities can often be formulated and proved as ``iff'' theorems.  For
example:
\begin{theorem}[\term{Distributive Law} for Sets]
Let $A$, $B$, and $C$ be sets.  Then:
\begin{equation}\label{set-distrib}
A \intersect (B \union C) = (A \intersect B) \union (A \intersect C)
\end{equation}
\end{theorem}

\begin{proof}
The equality~\eqref{set-distrib} is equivalent to the assertion that
\begin{equation}\label{set-distrib-z}
  z \in A \intersect (B \union C) \qiff z \in (A \intersect B)
  \union (A \intersect C)
\end{equation}
for all $z$.  This assertion looks very similar to the Distributive Law
for~$\QAND$ and $\QOR$ that we proved in Section~\ref{sec:validity}
(equation~\ref{eq:distributive_law}).  Namely, if $P$, $Q$, and~$R$
are propositions, then
\begin{equation}\label{eq:distributive_lawII}
[P \QAND (Q \QOR R)] \QIFF [(P \QAND Q) \QOR (P \QAND R)]
\end{equation}
Using this fact, we can now prove~\eqref{set-distrib-z} by a chain of
iff's:
\begin{align*}
\lefteqn{z \in A \intersect (B \union C)}\\
& \qiff (z \in A) \QAND (z \in B \union C) & \text{(def of $\intersect$)}\\
& \qiff (z \in A) \QAND (z \in B \QOR z \in C)
                & \text{(def of $\union$)}\\
& \qiff (z \in A \QAND z \in B) \QOR (z \in A \QAND z \in C)
                & \text{(equation~\ref{eq:distributive_lawII})}\\
& \qiff (z \in A \intersect B) \QOR (z \in A \intersect C)
                & \text{(def of $\intersect$)}\\
& \qiff z \in (A \intersect B) \union (A \intersect C)
                & \text{(def of $\union$)} & \qedhere
\end{align*}
\end{proof}

Many other set equalities can be derived from other valid propositions
and proved in an analogous manner.  In particular, propositions such
as $P$, $Q$ and~$R$ are replaced with sets such as $A$, $B$, and~$C$,
$\QAND$ ($\land$) is replaced with intersection ($\intersect$), $\QOR$
($\lor$) is replaced with union ($\union$), $\QNOT$ is replaced with
complement (\eg $\bar{P}$ would become~$\bar{A}$), and $\QIFF$ becomes
set equality~($=$).  Of course, you should always check that any
alleged set equality derived in this manner is indeed true.

\begin{problems}
\homeworkproblems
\pinput{PS_set_union}
\end{problems}

\subsection{\idx{Russell's Paradox} and the Logic of Sets}

Reasoning naively about sets can sometimes be tricky.  In fact, one of the
earliest attempts to come up with precise axioms for sets by a late
nineteenth century logician named Gotlob \term{Frege} was shot down by a three
line argument known as \emph{Russell's Paradox}:\footnote{Bertrand \term{Russell}
  was a mathematician/logician at Cambridge University at the turn of the
  Twentieth Century.  He reported that when he felt too old to do
  mathematics, he began to study and write about philosophy, and when he
  was no longer smart enough to do philosophy, he began writing about
  politics.  He was jailed as a conscientious objector during World War I.
  For his extensive philosophical and political writing, he won a Nobel
  Prize for Literature.}  This was an astonishing blow to efforts to
provide an axiomatic foundation for mathematics.

\textbox{
\textboxheader{Russell's Paradox}
Let $S$ be a variable ranging over all sets, and define
\[W \eqdef \set{S \suchthat S \not\in S}.\]
So by definition, for any set~$S$,
\[S \in W  \mbox{  iff  } S \not\in S.\]
In particular, we can let $S$ be $W$, and obtain
the contradictory result that
\[W \in W  \mbox{  iff  } W \not\in W.\]
}

A way out of the paradox was clear to Russell and others at the time:
\emph{it's unjustified to assume that $W$ is a set}.  So the step in the
proof where we let $S$ be $W$ has no justification, because $S$ ranges
over sets, and $W$ may not be a set.  In fact, the paradox implies that
$W$ had better not be a set!

But denying that $W$ is a set means we must \emph{reject} the very natural
axiom that every mathematically well-defined collection of elements is
actually a set.  So the problem faced by Frege, Russell and their
colleagues was how to specify \emph{which} well-defined collections are
sets.  Russell and his fellow Cambridge University colleague Whitehead
immediately went to work on this problem.  They spent a dozen years
developing a huge new axiom system in an even huger monograph called
\emph{Principia Mathematica}.

Over time, more efficient axiom systems were developed and today, it is
generally agreed that, using some simple logical deduction rules,
essentially all of mathematics can be derived from
the Axioms of \idx{Zermelo-Frankel Set Theory} with Choice (\idx{ZFC}).
We are \emph{not} going to be working with these axioms in this course,
but just in case you are interested, we have included them as a
sidebar below.

The ZFC axioms avoid Russell's Paradox because they imply that no set
is ever a member of itself.  Unfortunately, this does not necessarily
mean that there are not other paradoxes lurking around out there, just
waiting to be uncovered by future mathematicians.

\bigskip

\begin{inlinesidebar}[ZFC Axioms]

\begin{description}

\item[\term{Extensionality}.] Two sets are equal if they have the same members.
In formal logical notation, this would be stated as:
\[
(\forall z.\; (z \in x \QIFF z \in y)) \QIMPLIES x = y.
\]

\item[\term{Pairing}.] For any two sets $x$ and $y$, there is a set,
     $\set{x,y}$, with $x$ and $y$ as its only elements:
\[
\forall x,y.\; \exists u.\; \forall z.\;
[z \in u \QIFF (z = x \QOR z = y)]
\]

\item[\index{Union axiom}Union.] The union, $u$, of a collection, $z$, of sets is also a set:
\[
\forall z.\, \exists u \forall x.\; (\exists y.\; x \in y \QAND y \in z) \QIFF x \in u.
\]

\item[\index{Infinity axiom}Infinity.]  There is an infinite set.
  Specifically, there is a nonempty set, $x$, such that for any set $y \in
  x$, the set $\set{y}$ is also a member of $x$.

\item[Subset.] Given any set, $x$, and any proposition $P(y)$, there is a
  set containing precisely those elements $y \in x$ for which $P(y)$ holds.

\item[\index{Power Set axiom}Power Set.]  All the subsets of a set form another set:
\[
\forall x.\; \exists p.\; \forall u.\> u \subseteq x \QIFF u \in p.
\]

\item[\index{Replacement axiom}Replacement.]  Suppose a formula, $\phi$,
  of set theory defines the graph of a function, that is,
\[
\forall x, y, z.\, [\phi(x,y) \QAND \phi(x,z)] \QIMPLIES y = z.
\]
Then the image of any set, $s$, under that function is also a set, $t$.  Namely,
\[
\forall s\, \exists t\, \forall y.\, [\exists x.\, \phi(x,y) \QIFF y \in t].
\]


\item[\term{Foundation}.]
There cannot be an infinite sequence
\[
\cdots \in x_n \in \cdots \in x_1 \in x_0
\]
of sets each of which is a member of the previous one.  This is equivalent
to saying every nonempty set has a ``member-minimal'' element.  Namely, define
\[
\text{member-minimal}(m, x) \eqdef [m \in x \QAND \forall y \in x.\, y \notin m].
\]
Then the Foundation axiom is
\[
\forall x.\ x \neq \emptyset\ \QIMPLIES\ \exists m.\, \text{member-minimal}(m, x).
\]

\item[\index{Choice axiom}Choice.]  Given a set, $s$, whose members are nonempty sets no two
  of which have any element in common, then there is a set, $c$,
  consisting of exactly one element from each set in $s$.


\[\begin{array}{rlll}
\exists y \forall z \forall w & ( (z \in w \QAND w \in x) \QIMPLIES\\
                              &\quad \exists v \exists u (\exists t
                                           ((u \in w \QAND & w \in t)
                                                              & \QAND (u \in t \QAND t \in y))\\
                                                            &&& \QIFF u = v))
\end{array}\]

\end{description}

\end{inlinesidebar}

\section{\emph{Good} Proofs in Practice}

One purpose of a proof is to establish the truth of an assertion with
absolute certainty.  Mechanically checkable proofs of enormous length or
complexity can accomplish this.  But humanly intelligible proofs are the
only ones that help someone understand the subject.  Mathematicians
generally agree that important mathematical results can't be fully
understood until their proofs are understood.  That is why proofs are an
important part of the curriculum.

To be understandable and helpful, more is required of a proof than just
logical correctness: a good proof must also be clear.  Correctness and
clarity usually go together; a well-written proof is more likely to be a
correct proof, since mistakes are harder to hide.

In practice, the notion of proof is a moving target.  Proofs in a
professional research journal are generally unintelligible to all but
a few experts who know all the terminology and prior results used in
the proof.  Conversely, proofs in the first weeks of an introductory
course like \emph{Mathematics
for Computer Science} would be regarded as tediously
long-winded by a professional mathematician.  In fact, what we accept
as a good proof later in the term will be different than what we
consider to be a good proof in the first couple of weeks of this
course. But even so, we can offer some
general tips on writing good proofs:

\begin{description}

\item[State your game plan.]  A good proof begins by explaining the
  general line of reasoning. For example, ``We use case analysis'' or ``We
  argue by contradiction.''

\item[Keep a linear flow.]  Sometimes proofs are written like mathematical
  mosaics, with juicy tidbits of independent reasoning sprinkled
  throughout.  This is not good.  The steps of an argument should follow
  one another in an intelligible order.

\item[A proof is an essay, not a calculation.]  Many students initially
  write proofs the way they compute integrals.  The result is a long
  sequence of expressions without explanation, making it very hard to
  follow.  This is bad.  A good proof usually looks like an essay with
  some equations thrown in.  Use complete sentences.

\item[Avoid excessive symbolism.]  Your reader is probably good at
understanding words, but much less skilled at reading arcane
mathematical symbols.  So use words where you reasonably can.

\item[Revise and simplify.]  Your readers will be grateful.

\item[Introduce notation thoughtfully.]  Sometimes an argument can be
greatly simplified by introducing a variable, devising a special
notation, or defining a new term.  But do this sparingly since you're
requiring the reader to remember all that new stuff.  And remember to
actually \emph{define} the meanings of new variables, terms, or
notations; don't just start using them!

\item[Structure long proofs.]  Long programs are usually broken into a
hierarchy of smaller procedures.  Long proofs are much the same.
Facts needed in your proof that are easily stated, but not readily
proved are best pulled out and proved in preliminary lemmas.  Also, if
you are repeating essentially the same argument over and over, try to
capture that argument in a general lemma, which you can cite
repeatedly instead.

\item[Be wary of the ``obvious''.]  When familiar or truly obvious facts
  are needed in a proof, it's OK to label them as such and to not prove
  them.  But remember that what's obvious to you, may not be---and
  typically is not---obvious to your reader.

  Most especially, don't use phrases like ``clearly'' or ``obviously'' in
  an attempt to bully the reader into accepting something you're having
  trouble proving.  Also, go on the alert whenever you see one of these
  phrases in someone else's proof.

\item[Finish.]  At some point in a proof, you'll have established all the
essential facts you need.  Resist the temptation to quit and leave the
reader to draw the ``obvious'' conclusion.  Instead, tie everything
together yourself and explain why the original claim follows.

\end{description}

The analogy between good proofs and good programs extends beyond
structure.  The same rigorous thinking needed for proofs is essential in
the design of critical computer systems.  When algorithms and protocols
only ``mostly work'' due to reliance on hand-waving arguments, the results
can range from problematic to catastrophic.  An early example was the
Therac~25, a machine that provided radiation therapy to cancer victims,
but occasionally killed them with massive overdoses due to a software race
condition.  A more recent (August 2004) example involved a single faulty
command to a computer system used by United and American Airlines that
grounded the entire fleet of both companies---and all their passengers!

It is a certainty that we'll all one day be at the mercy of critical
computer systems designed by you and your classmates.  So we really
hope that you'll develop the ability to formulate rock-solid logical
arguments that a system actually does what you think it does!

\problemsection

\begin{problems}
\classproblems
\pinput{CP_irrational_raised_to_an_irrational}
\pinput{CP_buggy_highschool_proofs}
\pinput{CP_false_arithmetic_mean_proof}
\pinput{CP_generalize_root_2_proof}
\pinput{CP_AMM_root_2_proof}
\pinput{CP_roots_of_polynomials}

\homeworkproblems
\pinput{PS_log7_not_in_QZ}
\pinput{PS_prime_polynomial_41}
\pinput{PS_log2_of_3_irrational}
\end{problems}

\endinput
